<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Theory | ESL &amp; ISLR Working Examples</title>
  <meta name="description" content="Theory | ESL &amp; ISLR Working Examples" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Theory | ESL &amp; ISLR Working Examples" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://my-cabbages.github.io/ESL_ISLR/" />
  <meta property="og:image" content="https://my-cabbages.github.io/ESL_ISLR//./_bookdown_files/multivariate_QDA_files/figure-html" />
  
  <meta name="github-repo" content="my-cabbages/ESL_ISLR/" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Theory | ESL &amp; ISLR Working Examples" />
  
  
  <meta name="twitter:image" content="https://my-cabbages.github.io/ESL_ISLR//./_bookdown_files/multivariate_QDA_files/figure-html" />

<meta name="author" content="Isaac Baumann" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ridge-regression.html"/>
<link rel="next" href="case-1-linearly-dependent-variables.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/clipboard-2.0.6/clipboard.min.js"></script>
<link href="libs/xaringanExtra-clipboard-0.2.6/xaringanExtra-clipboard.css" rel="stylesheet" />
<script src="libs/xaringanExtra-clipboard-0.2.6/xaringanExtra-clipboard.js"></script>
<script>window.xaringanExtraClipboard(null, {"button":"<i class=\"fa fa-clipboard\"><\/i>","success":"<i class=\"fa fa-check\" style=\"color: #90BE6D\"><\/i>","error":"Press Ctrl+C to Copy"})</script>
<link href="libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  background-color: #f8f8f8; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">ESL & ISLR Working Examples</a></li>

<li class="divider"></li>
<li><a href="index.html#section"></a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a>
<ul>
<li class="chapter" data-level="" data-path="packages.html"><a href="packages.html"><i class="fa fa-check"></i>Packages</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i>Logistic Regression</a>
<ul>
<li class="chapter" data-level="" data-path="data-generation.html"><a href="data-generation.html"><i class="fa fa-check"></i>Data Generation</a></li>
<li class="chapter" data-level="" data-path="the-log-likelihood-surface.html"><a href="the-log-likelihood-surface.html"><i class="fa fa-check"></i>The Log-Likelihood Surface</a></li>
<li class="chapter" data-level="" data-path="implementation.html"><a href="implementation.html"><i class="fa fa-check"></i>Implementation</a>
<ul>
<li class="chapter" data-level="" data-path="implementation.html"><a href="implementation.html#method-1-newton-raphsonirls"><i class="fa fa-check"></i>Method 1: Newton-Raphson/IRLS</a></li>
<li class="chapter" data-level="" data-path="implementation.html"><a href="implementation.html#method-2-modified-irls"><i class="fa fa-check"></i>Method 2: Modified IRLS</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="testing.html"><a href="testing.html"><i class="fa fa-check"></i>Testing</a>
<ul>
<li><a href="testing.html#comparing-to-glm">Comparing to <code>glm</code></a></li>
<li class="chapter" data-level="" data-path="testing.html"><a href="testing.html#the-log-likelihood-surface-1"><i class="fa fa-check"></i>The Log-Likelihood Surface</a></li>
<li class="chapter" data-level="" data-path="testing.html"><a href="testing.html#plotting-the-logistic-fit"><i class="fa fa-check"></i>Plotting the Logistic Fit</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="linear-discriminant-analysis.html"><a href="linear-discriminant-analysis.html"><i class="fa fa-check"></i>Linear Discriminant Analysis</a>
<ul>
<li class="chapter" data-level="" data-path="univariate-lda.html"><a href="univariate-lda.html"><i class="fa fa-check"></i>Univariate LDA</a>
<ul>
<li class="chapter" data-level="" data-path="univariate-lda.html"><a href="univariate-lda.html#theory"><i class="fa fa-check"></i>Theory</a></li>
<li class="chapter" data-level="" data-path="univariate-lda.html"><a href="univariate-lda.html#setup"><i class="fa fa-check"></i>Setup</a></li>
<li class="chapter" data-level="" data-path="univariate-lda.html"><a href="univariate-lda.html#data-generation-1"><i class="fa fa-check"></i>Data Generation</a></li>
<li class="chapter" data-level="" data-path="univariate-lda.html"><a href="univariate-lda.html#implementation-1"><i class="fa fa-check"></i>Implementation</a></li>
<li class="chapter" data-level="" data-path="univariate-lda.html"><a href="univariate-lda.html#testing-1"><i class="fa fa-check"></i>Testing</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="multivariate-lda.html"><a href="multivariate-lda.html"><i class="fa fa-check"></i>Multivariate LDA</a>
<ul>
<li class="chapter" data-level="" data-path="multivariate-lda.html"><a href="multivariate-lda.html#theory-1"><i class="fa fa-check"></i>Theory</a></li>
<li class="chapter" data-level="" data-path="multivariate-lda.html"><a href="multivariate-lda.html#setup-1"><i class="fa fa-check"></i>Setup</a></li>
<li class="chapter" data-level="" data-path="multivariate-lda.html"><a href="multivariate-lda.html#data-generation-2"><i class="fa fa-check"></i>Data Generation</a></li>
<li class="chapter" data-level="" data-path="multivariate-lda.html"><a href="multivariate-lda.html#implementation-2"><i class="fa fa-check"></i>Implementation</a></li>
<li class="chapter" data-level="" data-path="multivariate-lda.html"><a href="multivariate-lda.html#testing-2"><i class="fa fa-check"></i>Testing</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="quadratic-discriminant-analysis.html"><a href="quadratic-discriminant-analysis.html"><i class="fa fa-check"></i>Quadratic Discriminant Analysis</a>
<ul>
<li class="chapter" data-level="" data-path="quadratic-discriminant-analysis.html"><a href="quadratic-discriminant-analysis.html#theory-2"><i class="fa fa-check"></i>Theory</a></li>
<li class="chapter" data-level="" data-path="data-generation-3.html"><a href="data-generation-3.html"><i class="fa fa-check"></i>Data Generation</a></li>
<li class="chapter" data-level="" data-path="implementation-3.html"><a href="implementation-3.html"><i class="fa fa-check"></i>Implementation</a>
<ul>
<li class="chapter" data-level="" data-path="implementation-3.html"><a href="implementation-3.html#the-bayes-classifier-and-decision-boundaries-1"><i class="fa fa-check"></i>The Bayes Classifier and Decision Boundaries</a></li>
<li class="chapter" data-level="" data-path="implementation-3.html"><a href="implementation-3.html#the-qda-classifier-and-decision-boundaries"><i class="fa fa-check"></i>The QDA Classifier and Decision Boundaries</a></li>
<li class="chapter" data-level="" data-path="implementation-3.html"><a href="implementation-3.html#visualizing-the-decision-boundaries"><i class="fa fa-check"></i>Visualizing the Decision Boundaries</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="testing-3.html"><a href="testing-3.html"><i class="fa fa-check"></i>Testing</a>
<ul>
<li class="chapter" data-level="" data-path="testing-3.html"><a href="testing-3.html#visualization"><i class="fa fa-check"></i>Visualization</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="ridge-regression.html"><a href="ridge-regression.html"><i class="fa fa-check"></i>Ridge Regression</a>
<ul>
<li class="chapter" data-level="" data-path="theory-3.html"><a href="theory-3.html"><i class="fa fa-check"></i>Theory</a>
<ul>
<li class="chapter" data-level="" data-path="theory-3.html"><a href="theory-3.html#minimizing-rss-ols"><i class="fa fa-check"></i>Minimizing RSS: OLS</a></li>
<li class="chapter" data-level="" data-path="theory-3.html"><a href="theory-3.html#minimizing-rss-ridge"><i class="fa fa-check"></i>Minimizing RSS: Ridge</a></li>
<li class="chapter" data-level="" data-path="theory-3.html"><a href="theory-3.html#important-features"><i class="fa fa-check"></i>Important Features</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="case-1-linearly-dependent-variables.html"><a href="case-1-linearly-dependent-variables.html"><i class="fa fa-check"></i>Case 1: Linearly Dependent Variables</a>
<ul>
<li class="chapter" data-level="" data-path="case-1-linearly-dependent-variables.html"><a href="case-1-linearly-dependent-variables.html#data-generation-4"><i class="fa fa-check"></i>Data Generation</a></li>
<li class="chapter" data-level="" data-path="case-1-linearly-dependent-variables.html"><a href="case-1-linearly-dependent-variables.html#the-problem-ols"><i class="fa fa-check"></i>The Problem: OLS</a></li>
<li class="chapter" data-level="" data-path="case-1-linearly-dependent-variables.html"><a href="case-1-linearly-dependent-variables.html#a-solution-ridge-regression"><i class="fa fa-check"></i>A Solution: Ridge Regression</a></li>
<li><a href="case-1-linearly-dependent-variables.html#solving-for-hatbetaridge">Solving for <span class="math inline">\(\hat{\beta}^{Ridge}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="" data-path="case-2-correlated-independent-variables.html"><a href="case-2-correlated-independent-variables.html"><i class="fa fa-check"></i>Case 2: Correlated Independent Variables</a>
<ul>
<li class="chapter" data-level="" data-path="case-2-correlated-independent-variables.html"><a href="case-2-correlated-independent-variables.html#data"><i class="fa fa-check"></i>Data</a></li>
<li class="chapter" data-level="" data-path="case-2-correlated-independent-variables.html"><a href="case-2-correlated-independent-variables.html#exploratory-analysis"><i class="fa fa-check"></i>Exploratory Analysis</a></li>
<li class="chapter" data-level="" data-path="case-2-correlated-independent-variables.html"><a href="case-2-correlated-independent-variables.html#implementation-4"><i class="fa fa-check"></i>Implementation</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="standardization.html"><a href="standardization.html"><i class="fa fa-check"></i>Standardization</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">ESL &amp; ISLR Working Examples</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="theory-3" class="section level2">
<h2>Theory</h2>
<p>Just like in OLS, the intent is to minimize <a href="https://en.wikipedia.org/wiki/Residual_sum_of_squares" target="_blank">residual sum of squares</a>.</p>
<center>
<span class="math display">\[RSS = \sum_{i=1}^N (y_i - \hat{y}_i)^2\]</span>
</center>
<div id="minimizing-rss-ols" class="section level3">
<h3>Minimizing RSS: OLS</h3>
<p>Before jumping into ridge regression, let’s review deriving the OLS solution via minimizing RSS. Where <span class="math inline">\(X\)</span> is our <a href="https://en.wikipedia.org/wiki/Design_matrix" target="_blank"><strong>design matrix</strong></a>:</p>
<center>
<p><span class="math display">\[\vec{\beta} = \{ \beta_0, \beta_1, \ldots, \beta_k \}\]</span></p>
<span class="math display">\[\begin{align*}
RSS(\vec{\beta})_{OLS} &amp;= (\vec{y} - X \vec{\beta})^T(\vec{y} - X \vec{\beta}) \\
&amp;= \vec{y}^T \vec{y} - \underbrace{\vec{y}^T X \vec{\beta}}_{\vec{\beta}^T X^T \vec{y}} - \vec{\beta}^T X^T \vec{y} + \vec{\beta}^T X^T X \vec{\beta} \\
&amp;= \vec{y}^T \vec{y} - 2 \vec{\beta}^T X^T \vec{y} + \vec{\beta}^T X^T X \vec{\beta}
\end{align*}\]</span>
</center>
<p>Then minimizing<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a> <span class="math inline">\(RSS(\vec{\beta})\)</span> with respect to <span class="math inline">\(\vec{\beta}\)</span></p>
<center>
<span class="math display">\[\begin{align*}
\frac{\partial RSS(\vec{\beta})_{OLS}}{\partial \vec{\beta}} = -2 X^T \vec{y} + 2 X^T X \vec{\beta} &amp;= 0 \\
-2 (X^T \vec{y} - X^T X \vec{\beta}) &amp;= 0 \\
X^T \vec{y} - X^T X \vec{\beta} &amp;= 0 \\
X^T X \vec{\beta} &amp;= X^T \vec{y} \\
\hat{\vec{\beta}}^{OLS} &amp;= (X^T X)^{-1} X^T \vec{y}
\end{align*}\]</span>
</center>
<p>we get the <a href="https://mathworld.wolfram.com/NormalEquation.html" target="_blank">normal equation</a> along the way and arrive at the usual solution.</p>
</div>
<div id="minimizing-rss-ridge" class="section level3">
<h3>Minimizing RSS: Ridge</h3>
<p>The ridge minimization of RSS works the same way, except we include the penalty term (<span class="math inline">\(\lambda\)</span>) on the squared values of our estimates.</p>
<p>However, <strong>the penalty does not apply to the intercept term</strong>, so we will use a matrix of just our variables without a leading one-vector for the intercept term (which I will call the <strong>input matrix</strong>). It’s easy to conceptualize how the ridge regression solutions for the non-intercept terms would depend on the intercept if we also penalized it. If the column vectors of <span class="math inline">\(X\)</span> are mean-centered then <span class="math inline">\(\beta_0 = \bar{y}\)</span>. Then we minimize RSS with the penalty term as if there were no intercept.</p>
<center>
<p><span class="math display">\[\vec{\beta} = \{ \beta_1, \beta_2 \ldots, \beta_k \}\]</span></p>
<span class="math display">\[\begin{align*}
RSS(\vec{\beta})_{Ridge} &amp;= (\vec{y} - X \vec{\beta})^T(\vec{y} - X \vec{\beta}) + \lambda \vec{\beta}^T \vec{\beta} \\
&amp;= \vec{y}^T \vec{y} - \underbrace{\vec{y}^T X \vec{\beta}}_{\vec{\beta}^T X^T \vec{y}} - \vec{\beta}^T X^T \vec{y} + \vec{\beta}^T X^T X \vec{\beta} + \lambda \vec{\beta}^T \vec{\beta} \\
&amp;= \vec{y}^T \vec{y} - 2 \vec{\beta}^T X^T \vec{y} + \vec{\beta}^T X^T X \vec{\beta} + \lambda \vec{\beta}^T \vec{\beta}
\end{align*}\]</span>
</center>
<p>Then minimize with respect to <span class="math inline">\(\vec{\beta}\)</span>:</p>
<center>
<span class="math display">\[\begin{align*}
\frac{\partial RSS(\vec{\beta})_{Ridge}}{\partial \vec{\beta}} = -2 X^T \vec{y} + 2 X^T X \vec{\beta} + 2 \lambda \vec{\beta} &amp;= 0 \\
-2 (X^T \vec{y} - X^T X \vec{\beta} - \lambda \vec{\beta}) &amp;= 0 \\
X^T \vec{y} - X^T X \vec{\beta} - \lambda \vec{\beta} &amp;= 0 \\
X^T \vec{y} &amp;= X^T X \beta + \lambda \vec{\beta} \\
X^T \vec{y} &amp;= \vec{\beta} (X^T X + \lambda I) \\
\hat{\vec{\beta}}^{Ridge} &amp;= (X^T X + \lambda I)^{-1} X^T \vec{y}
\end{align*}\]</span>
</center>
<p>Notice that for <span class="math inline">\(\lambda = 0\)</span> our minimization is the same as with OLS, so a ridge regression fit with <span class="math inline">\(\lambda = 0\)</span> is just an OLS fit. In calculating MSE we will need to add <span class="math inline">\(\beta_0 = \bar{y}\)</span> in so long as we’re concerned about being off by a constant.</p>
</div>
<div id="important-features" class="section level3">
<h3>Important Features</h3>
<p>This section is not strictly necessary and may be skipped, but know that</p>
<ol style="list-style-type: decimal">
<li>the ridge regression problem will never not have a solution because of <a href="https://en.wikipedia.org/wiki/Rank_(linear_algebra)#Main_definitions" target="_blank">rank-deficiency</a> or <a href="https://en.wikipedia.org/wiki/Linear_independence" target="_blank">linearly dependent</a> variables and</li>
<li>ridge regression performs a type of variable selection by shrinking variables that contribute less to the variance of fitted values more.<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a></li>
</ol>
<div id="solvability" class="section level4">
<h4>Solvability</h4>
<p>Because we add a positive constant, <span class="math inline">\(\lambda I\)</span>, to the diagonal of <span class="math inline">\(X^T X\)</span> prior to inverting that matrix the resulting adjusted matrix will always be <a href="https://en.wikipedia.org/wiki/Invertible_matrix" target="_blank">nonsingular</a>. So there will always be a solution, even if <span class="math inline">\(X^T X\)</span> is <a href="https://en.wikipedia.org/wiki/Rank_(linear_algebra)#Main_definitions" target="_blank">rank-deficient</a>. This is not the case with OLS: ridge regression can provide solutions to least squares-like problems where OLS cannot.<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a> This may be useful if two or more variables are <a href="https://en.wikipedia.org/wiki/Linear_independence" target="_blank">linearly dependent</a> or are very highly correlated or if there are more variables than observations.</p>
</div>
<div id="sort-of-variable-selection" class="section level4">
<h4>(Sort Of) Variable Selection</h4>
<p>It’s also important to note that <strong>ridge regression shrinks all estimates toward zero, but it does not shrink them equally.</strong><a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a> Ridge regression shrinks the variables that contribute more variation to the fitted (or predicted) values less than those variables that don’t: in this way, it performs a kind of variable selection.</p>
<p>To demonstrate this, we can take the <a href="https://en.wikipedia.org/wiki/Singular_value_decomposition" target="_blank">singular value decomposition</a> of the mean-centered input matrix.<a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a></p>
<center>
<span class="math display">\[X = U D V^T \]</span>
</center>
<p>Like with all SVDs, the <span class="math inline">\(D\)</span> matrix is diagonal and contains the singular values (which are associated with <a href="https://en.wikipedia.org/wiki/Principal_component_analysis" target="_blank">principal components</a>) of <span class="math inline">\(X\)</span> along its diagonal in descending order.</p>
<p>We can take this decomposition of <span class="math inline">\(X\)</span> and plug it into our <a href="https://en.wikipedia.org/wiki/Projection_matrix" target="_blank">hat matrix</a> formula</p>
<center>
<span class="math display">\[\begin{align}
X \hat{\beta}^{Ridge} &amp;= X (X^T X + \lambda I)^{-1} X^T \vec{y} \\
&amp;= U D V^T (V D^T U^T U D V^T + \lambda I)^{-1} V D^T U^T \vec{y} \\
&amp;= U D V^T (V D^T D V^T + \lambda I)^{-1} V D^T U^T \vec{y} &amp;&amp; U^T U = I \\ 
&amp;= UD (D^T D + \lambda I)^{-1}) D^T U^T \vec{y} &amp;&amp; V^T V^{-1} (V^T)^{-1} V = I \\
&amp;= UD (D^2 + \lambda I)^{-1} D U^T \vec{y} &amp;&amp; D^T D = D^2 \text{, } D^T = D
\end{align}\]</span>
</center>
<p>where <span class="math inline">\(U^T U = I\)</span> because <span class="math inline">\(U\)</span> is symmetric, <span class="math inline">\(V^T V^{-1} (V^T)^{-1} V = I\)</span> because <span class="math inline">\(V\)</span> is <a href="https://en.wikipedia.org/wiki/Unitary_matrix" target="_blank">unitary</a>, and <span class="math inline">\(D^T = D\)</span> because <span class="math inline">\(D\)</span> is diagonal and therefore also symmetric.</p>
<p>If we focus on the above expression in terms of individual vectors of <span class="math inline">\(U\)</span> and individual diagonal elements of <span class="math inline">\(D\)</span> it’s easy to see that</p>
<center>
<span class="math display">\[X \hat{\beta}^{Ridge} = \sum_{k = 1}^K \vec{u}_k \frac{\sigma_k^2}{\sigma_k^2 + \lambda} \vec{u}_k^T \vec{y}\]</span>
</center>
<p>where <span class="math inline">\(K\)</span> is the number of variables, <span class="math inline">\(u_k\)</span> is the <span class="math inline">\(k\)</span>th column of <span class="math inline">\(U\)</span> (or the <span class="math inline">\(k\)</span>th left <a href="https://en.wikipedia.org/wiki/Singular_value_decomposition#Singular_values,_singular_vectors,_and_their_relation_to_the_SVD" target="_blank">singular vector</a>), and <span class="math inline">\(\sigma_k\)</span> is the <span class="math inline">\(k\)</span>th singular value. A hat matrix is simply a projection of <span class="math inline">\(X\)</span> onto the column space of <span class="math inline">\(\vec{y}\)</span>, so it is clear to see here that the fraction is the penalty in that projection. It’s also clear to see that, because the singular values are decreasing along the diagonal of <span class="math inline">\(D\)</span> (i.e., <span class="math inline">\(\sigma_1 &gt; \sigma_2, \ldots, &gt; \sigma_K\)</span>), this penalty grows larger for basis vectors associated with smaller singular values: i.e., ridge regression more heavily shrinks the estimated “effect” of variables with smaller sample variance.</p>
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="4">
<li id="fn4"><p>Recall that <span class="math inline">\(\frac{\partial a^T b a}{\partial a} = 2 a x\)</span> and <span class="math inline">\(\frac{\partial a^T b}{\partial a} = a\)</span>.<a href="theory-3.html#fnref4" class="footnote-back">↩︎</a></p></li>
<li id="fn5"><p>It can be easy to think that this means the “less important” variables which don’t “explain” our outcome are being shrunk more, but the reason why these variables are shrunk actually has nothing to do with the outcome. Remember that we’re going for prediction here, not inference. So while it might not be true that the variables with higher variance “explain” our outcome, we don’t really care since the goal isn’t causally explaining the outcome.
Elements of Statistical Learning explains this well:
“The implicit assumption is that the response will tend to vary most in the directions of high variance of the inputs. This is often a reasonable assumption, since predictors are often chosen for study because they vary with the response variable, but need not hold in general.”<a href="theory-3.html#fnref5" class="footnote-back">↩︎</a></p></li>
<li id="fn6"><p>But, unlike OLS, the estimates will not be <a href="https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem" target="_blank">BLUE</a>.<a href="theory-3.html#fnref6" class="footnote-back">↩︎</a></p></li>
<li id="fn7"><p>It would be pretty useless if it did shrink all variables equally!<a href="theory-3.html#fnref7" class="footnote-back">↩︎</a></p></li>
<li id="fn8"><p>Remember, this is just our matrix of variable values, not including the one-vector that would be in the design matrix for the intercept.<a href="theory-3.html#fnref8" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ridge-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="case-1-linearly-dependent-variables.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"],
"google": false,
"instapper": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
},
"df_print": "kable"
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
