<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Multivariate Quadratic Discriminant Analysis</title>

<script src="site_libs/header-attrs-2.9/header-attrs.js"></script>
<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<script src="site_libs/clipboard-2.0.6/clipboard.min.js"></script>
<link href="site_libs/xaringanExtra-clipboard-0.2.6/xaringanExtra-clipboard.css" rel="stylesheet" />
<script src="site_libs/xaringanExtra-clipboard-0.2.6/xaringanExtra-clipboard.js"></script>
<script>window.xaringanExtraClipboard(null, {"button":"<i class=\"fa fa-clipboard\"><\/i>","success":"<i class=\"fa fa-check\" style=\"color: #90BE6D\"><\/i>","error":"Press Ctrl+C to Copy"})</script>
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>








<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.tab('show');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">ESL/ISLR Examples</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/my-cabbages/ESL_ISLR">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Multivariate Quadratic Discriminant Analysis</h1>

</div>


<p>This example walks through using quadratic discriminant analysis to classify observations in a three-class multivariate setting with (idealized) generated data. I’d suggest reading the <a href="multivariate_LDA.html" target="_blank">multivariate LDA</a> example first.</p>
<p>Unlike linear discriminant analysis,</p>
<ol style="list-style-type: decimal">
<li>QDA allows for non-linear decision boundaries, and</li>
<li>QDA does not assume that the class distribution covariance matrices are identical.</li>
</ol>
<p>Though not shown here, this example uses the <a href="https://cran.r-project.org/web/packages/tidyverse/index.html" target="_blank"><code>tidyverse</code></a> and <a href="https://cran.r-project.org/web/packages/MASS/index.html" target="_blank"><code>MASS</code></a> packages.</p>
<div id="data-generation" class="section level2">
<h2>Data Generation</h2>
<p>We’ll generate data with three possible outcome classes (coded as 0, 1, and 2) with two independent variables/predictors.</p>
<p>We’ll use <code>MASS</code>’s <a href="https://www.rdocumentation.org/packages/rockchalk/versions/1.8.144/topics/mvrnorm" target="_blank"><code>mvnorm</code></a> function to sample from a multivariate normal distribution. Throughout this example I’ll refer to the first variable as X1 and the second variable as X2. For each class we’ll construct a vector of means for X1 and X2, randomly sampled integers between 1 and 10:</p>
<pre class="r"><code>X1_means &lt;- sample.int(10, 3, replace = FALSE)
X2_means &lt;- sample.int(10, 3, replace = FALSE)

pop_mean_c0_X1 &lt;- X1_means[1]
pop_mean_c1_X1 &lt;- X1_means[2]
pop_mean_c2_X1 &lt;- X1_means[3]

pop_mean_c0_X2 &lt;- X2_means[1]
pop_mean_c1_X2 &lt;- X2_means[2]
pop_mean_c2_X2 &lt;- X2_means[3]

mu_c0 &lt;- c(pop_mean_c0_X1, pop_mean_c0_X2)
mu_c1 &lt;- c(pop_mean_c1_X1, pop_mean_c1_X2)
mu_c2 &lt;- c(pop_mean_c2_X1, pop_mean_c2_X2)</code></pre>
<p>As noted above, <strong>QDA does not assume that the class covariance matrices are identical</strong> so instead of creating one randomly generated covariance matrix we will create one for each outcome class. For this example, we’ll randomly choose class-level correlations and class-level variances (between 1 and 10).</p>
<pre class="r"><code>pop_c0_corr &lt;- runif(1, 0, 1)
pop_c1_corr &lt;- runif(1, 0, 1)
pop_c2_corr &lt;- runif(1, 0, 1)

pop_var_c0 &lt;- runif(1, 0, 10)
pop_var_c1 &lt;- runif(1, 0, 10)
pop_var_c2 &lt;- runif(1, 0, 10)</code></pre>
<p>Just like in the <a href="multivariate_LDA.html" target="_blank">multivariate LDA</a> example we’ll generate 300 sample observations for each class, once for a training set and once for a test set. We could use the same process as in the multivariate LDA example, but we can be more efficient. Using a loop with the <a href="https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/get" target="_blank"><code>get</code></a> and <a href="https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/assign" target="_blank"><code>assign</code></a> functions, we can variably refer to our class-level correlations and variances as well as variably generate our class covariance matrices and class sample data sets.</p>
<pre class="r"><code>n &lt;- 300

for (i in 0:2) {
  
  class &lt;- paste(&quot;c&quot;, i, sep = &quot;&quot;)
  
  class_var &lt;- paste(&quot;pop_var&quot;, class, sep = &quot;_&quot;)
  class_corr &lt;- paste(&quot;pop&quot;, class, &quot;corr&quot;, sep = &quot;_&quot;)
  mu = paste(&quot;mu&quot;, class, sep = &quot;_&quot;)
  
  temp_sigma &lt;- matrix(c(get(class_var), get(class_corr),
                        get(class_corr), get(class_var)), 2, 2)
  
  temp_train_data &lt;- mvrnorm(n = n,
                             mu = get(mu),
                             Sigma = temp_sigma)
  
  temp_test_data &lt;- mvrnorm(n = n,
                            mu = get(mu),
                            Sigma = temp_sigma)
  
  assign(paste(class, &quot;sigma&quot;, sep = &quot;_&quot;), temp_sigma)
  assign(paste(class, &quot;train&quot;, sep = &quot;_&quot;), temp_train_data)
  assign(paste(class, &quot;test&quot;, sep = &quot;_&quot;), temp_test_data)
  
}</code></pre>
<p>Now all that’s left is to bind the class training and test data together.</p>
<pre class="r"><code>train_sample_df &lt;- bind_rows(
  tibble(y = 0,
         X1 = c0_train[,1],
         X2 = c0_train[,2]),
  tibble(y = 1,
         X1 = c1_train[,1],
         X2 = c1_train[,2]),
  tibble(y = 2,
         X1 = c2_train[,1],
         X2 = c2_train[,2])
)

test_sample_df &lt;- bind_rows(
  tibble(y = 0,
         X1 = c0_test[,1],
         X2 = c0_test[,2]),
  tibble(y = 1,
         X1 = c1_test[,1],
         X2 = c1_test[,2]),
  tibble(y = 2,
         X1 = c2_test[,1],
         X2 = c2_test[,2])
)</code></pre>
<div id="initial-visualizations" class="section level3">
<h3>Initial Visualizations</h3>
<p>Generally, our data here will look a lot like the data in the multivariate LDA example: three clouds of data points, one for each class, except that in this case the clouds may not be similarly-shaped since each comes from distributions with distinct covariance matrices. We can see this with a simple scatterplot. We’ll focus on the training data.</p>
<pre class="r"><code>train_sample_scatter &lt;- ggplot(train_sample_df) + 
  geom_point(aes(x = X1, y = X2, color = as.factor(y))) +
  scale_color_manual(values = c(&quot;red&quot;, &quot;blue&quot;, &quot;green&quot;),
                     name = &quot;Class&quot;) +
  labs(title = &quot;Bivariate Training Sample of Three Classes: Scatterplot&quot;)</code></pre>
<p><img src="multivariate_QDA_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
</div>
</div>
<div id="the-bayes-classifier-and-decision-boundaries" class="section level2">
<h2>The Bayes Classifier and Decision Boundaries</h2>
<p>Just like in the <a href="univariate_LDA.html" target="_blank">univariate LDA</a> and <a href="multivariate_LDA.html" target="_blank">multivariate LDA</a> examples, we will use the Bayes classifier and its decision boundaries to compare our model against since discriminant analysis is the same as applying the Bayes classifier but with sample estimates.</p>
<div id="theory" class="section level3">
<h3>Theory</h3>
<p>Deriving the pairwise discriminant functions in the QDA case where we don’t assume that the class covariance matrices are identical is the exact same process as in the multivariate LDA case: we simply end up with slightly uglier discriminant functions since we don’t assume identical covariance matrices. In fact, the derivation is even easier than in multivariate LDA and is really just an exercise in remembering <a href="https://mathinsight.org/logarithm_basics" target="_blank">log rules</a>.</p>
<p>Just like in the multivariate LDA example, we start by taking <a href="https://en.wikipedia.org/wiki/Bayes%27_theorem#For_continuous_random_variables" target="_blank">Bayes’ theorem for continuous variables</a> and plugging in the <a href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution" target="_blank">multivariate normal PDF</a> and then taking the log ratio of conditional probabilities.</p>
<center>
<span class="math display">\[\begin{align}
\log\frac{\Pr(Y = k|X = x)}{\Pr(Y = l|X = x)} &amp;= \log \frac{\pi_k f_k(x)}{\pi_l f_l(x)} \\
&amp;= \log{\frac{\pi_k}{\pi_l}} + \log{\frac{f_k(x)}{f_l(x)}} \\
&amp;= \log{\frac{\pi_k}{\pi_l}} + \log{\frac{\frac{1}{{(2\color{red}{\pi}})^{\frac{p}{2}} \det(\Sigma_k)^\frac{1}{2}} e^{\frac{-1}{2} (x - \mu_k)^T \Sigma_k^{-1} (x - \mu_k)}}{\frac{1}{{(2\color{red}{\pi}})^{\frac{p}{2}} \det(\Sigma_l)^\frac{1}{2}} e^{\frac{-1}{2} (x - \mu_l)^T \Sigma_l^{-1} (x - \mu_l)}}}
\end{align}\]</span>
</center>
<p>Just as in the univariate and multivariate LDA examples, <span class="math inline">\(\color{red}{\pi}\)</span> in the PDF is the literal value pi, not a prior probability. <span class="math inline">\(p\)</span> is the number of variables.</p>
<p>Remembering our log rules, we can break this down:</p>
<center>
<span class="math display">\[\begin{align}
\log\frac{\Pr(Y = k|X = x)}{\Pr(Y = l|X = x)} &amp;= \log{\frac{\pi_k}{\pi_l}} + \log{\frac{\frac{1}{{(2\color{red}{\pi}})^{\frac{p}{2}} \det(\Sigma_k)^\frac{1}{2}} e^{\frac{-1}{2} (x - \mu_k)^T \Sigma_k^{-1} (x - \mu_k)}}{\frac{1}{{(2\color{red}{\pi}})^{\frac{p}{2}} \det(\Sigma_l)^\frac{1}{2}} e^{\frac{-1}{2} (x - \mu_l)^T \Sigma_l^{-1} (x - \mu_l)}}} \\
&amp;= \log{\frac{\pi_k}{\pi_l}} + \log{\frac{\frac{1}{{(2\color{red}{\pi}})^{\frac{p}{2}} \det(\Sigma_k)^\frac{1}{2}}}{\frac{1}{{(2\color{red}{\pi}})^{\frac{p}{2}} \det(\Sigma_l)^\frac{1}{2}}}} + \log{\frac{e^{\frac{-1}{2} (x - \mu_k)^T \Sigma_k^{-1} (x - \mu_k)}}{e^{\frac{-1}{2} (x - \mu_l)^T \Sigma_l^{-1} (x - \mu_l)}}} \\
&amp;= \log{\frac{\pi_k}{\pi_l}} + \log{1} - \log{2\color{red}{\pi}^\frac{p}{2} \det(\Sigma_k)^\frac{1}{2}} - \log{1} + \log{2\color{red}{\pi}^\frac{p}{2} \det(\Sigma_l)^\frac{1}{2}}
+ \log{e^{\frac{-1}{2} (x - \mu_k)^T \Sigma_l^{-1} (x - \mu_k)}} - \log{e^{\frac{-1}{2} (x - \mu_l)^T \Sigma_l^{-1} (x - \mu_l)}} \\
&amp;= \log{\frac{\pi_k}{\pi_l}} - \log{2\color{red}{\pi}^\frac{p}{2}} + \log{\det(\Sigma_k)^\frac{1}{2}} + \log{2\color{red}{\pi}^\frac{p}{2}} + \log{\det(\Sigma_l)^\frac{1}{2}}
- \frac{1}{2} (x - \mu_k)^T \Sigma_l^{-1} (x - \mu_k) + \frac{1}{2} (x - \mu_l)^T \Sigma_l^{-1} (x - \mu_l) \\
&amp;= \log{\pi_k} - \log{\pi_l} - \frac{1}{2}\log{\det(\Sigma_k)} + \frac{1}{2}\log{\det(\Sigma_l)}
- \frac{1}{2} (x - \mu_k)^T \Sigma_l^{-1} (x - \mu_k) + \frac{1}{2} (x - \mu_l)^T \Sigma_l^{-1} (x - \mu_l) \\
\end{align}\]</span>
</center>
<p>Then, like in the multivariate LDA case, all that’s left to do is to evaluate this expression at the decision boundary (where <span class="math inline">\(\frac{\Pr(Y = k|X = x)}{\Pr(Y = l|X = x)} = 1\)</span> and so <span class="math inline">\(\log\frac{\Pr(Y = k|X = x)}{\Pr(Y = l|X = x)} = 0\)</span>) and then reorganize the expression to find our discriminant functions.</p>
<center>
<span class="math display">\[\begin{align}
0 &amp;= \log{\pi_k} - \log{\pi_l} - \frac{1}{2}\log{\det(\Sigma_k)} + \frac{1}{2}\log{\det(\Sigma_l)}
- \frac{1}{2} (x - \mu_k)^T \Sigma_l^{-1} (x - \mu_k) + \frac{1}{2} (x - \mu_l)^T \Sigma_l^{-1} (x - \mu_l) \\
\underbrace{\log{\pi_l} - \frac{1}{2}\log{\det(\Sigma_l)} - \frac{1}{2} (x - \mu_l)^T \Sigma_l^{-1} (x - \mu_l)}_{\delta_l(x)} &amp;= \underbrace{\log{\pi_k} - \frac{1}{2}\log{\det(\Sigma_k)} - \frac{1}{2} (x - \mu_k)^T \Sigma_k^{-1} (x - \mu_k)}_{\delta_k(x)} \\
\end{align}\]</span>
</center>
<p>Just like in the univariate and multivariate LDA cases, for a given observation the Bayes classifier assigns that observation to a class based on which class’s discriminant function evaluates largest.</p>
</div>
<div id="implementation" class="section level3">
<h3>Implementation</h3>
<p>Creating the discriminant functions, classifying with the Bayes classifier, and classifying observations in our training set using the population parameters we know is straightforward in R. This will mirror what we did in the multivariate LDA example but with our new discriminant functions.</p>
<p>First, we’ll construct vectors of our population means by class as well as the class-wise prior probabilities. In this example we know that the class-wise prior probabilities are equal, but we’ll calculate them each separately for thoroughness.</p>
<pre class="r"><code>pop_c0_mean_vec &lt;- c(pop_mean_c0_X1, pop_mean_c0_X2)
pop_c1_mean_vec &lt;- c(pop_mean_c1_X1, pop_mean_c1_X2)
pop_c2_mean_vec &lt;- c(pop_mean_c2_X1, pop_mean_c2_X2)

c0_prior &lt;- n / (n * 3)
c1_prior &lt;- n / (n * 3)
c2_prior &lt;- n / (n * 3)</code></pre>
<p>Then we’ll build our three discriminant functions and <a href="https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/apply" target="_blank"><code>apply</code></a> them inside the decision rule for classification described above.</p>
<pre class="r"><code>d0_bayes &lt;- function(x_vec){
  -.5 * log(norm(c0_sigma, type = &quot;2&quot;)) - 
    .5 * t(x_vec - mu_c0) %*% solve(c0_sigma) %*% (x_vec - mu_c0) + 
    log(c0_prior)
}

d1_bayes &lt;- function(x_vec){
  -.5 * log(norm(c1_sigma, type = &quot;2&quot;)) - 
    .5 * t(x_vec - mu_c1) %*% solve(c1_sigma) %*% (x_vec - mu_c1) + 
    log(c1_prior)
}

d2_bayes &lt;- function(x_vec){
  -.5 * log(norm(c2_sigma, type = &quot;2&quot;)) - 
    .5 * t(x_vec - mu_c2) %*% solve(c2_sigma) %*% (x_vec - mu_c2) + 
    log(c2_prior)
}

bayes_classifier &lt;- function(x_vec){
  score_c0 &lt;- d0_bayes(x_vec)
  score_c1 &lt;- d1_bayes(x_vec)
  score_c2 &lt;- d2_bayes(x_vec)
  
  if (score_c0 &gt; score_c1 &amp; score_c0 &gt; score_c2) {
    0
  } else if (score_c1 &gt; score_c0 &amp; score_c1 &gt; score_c2) {
    1
  } else {
    2
  }
}

train_sample_df$bayes_predicted_y &lt;- apply(train_sample_df[, c(&quot;X1&quot;, &quot;X2&quot;)], 
                                           1, bayes_classifier)</code></pre>
</div>
</div>
<div id="the-qda-classifier-and-decision-boundaries" class="section level2">
<h2>The QDA Classifier and Decision Boundaries</h2>
<p>Just like in the multivariate LDA case, the only difference between the Bayes classifier above and the QDA classifier is that we almost surely can’t know the population covariance matrices and means. To move forward, we need to estimate them.</p>
<p>There really isn’t any theory here: for the means we simply calculate the classwise means for X1 and X2 from our sample and for the covariance matrices we can simply use R’s <a href="https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/cor" target="_blank">`cov’</a> function. Why don’t we need to do the whole estimated weighted covariance matrix process like we did in the multivariate LDA example? Because we’re not assuming a common covariance matrix.</p>
<div id="implementation-1" class="section level3">
<h3>Implementation</h3>
<p>First we’ll estimate our means and covariance matrices by class.</p>
<pre class="r"><code>sample_c0_mean_vec &lt;- c(mean(train_sample_df$X1[train_sample_df$y == 0]), 
                        mean(train_sample_df$X2[train_sample_df$y == 0]))
sample_c1_mean_vec &lt;- c(mean(train_sample_df$X1[train_sample_df$y == 1]), 
                        mean(train_sample_df$X2[train_sample_df$y == 1]))
sample_c2_mean_vec &lt;- c(mean(train_sample_df$X1[train_sample_df$y == 2]), 
                        mean(train_sample_df$X2[train_sample_df$y == 2]))

sample_cov_mat_c0 &lt;- cov(train_sample_df %&gt;% 
                           filter(y == 0) %&gt;% 
                           dplyr::select(X1, X2))

sample_cov_mat_c1 &lt;- cov(train_sample_df %&gt;% 
                           filter(y == 1) %&gt;% 
                           dplyr::select(X1, X2))

sample_cov_mat_c2 &lt;- cov(train_sample_df %&gt;% 
                           filter(y == 2) %&gt;% 
                           dplyr::select(X1, X2))</code></pre>
<p>The only things left to calculate are the classwise prior probabilities. Again, we know that these are equal because of our generated samples so we don’t really need to calculate them separately, but we will anyway for thoroughness.</p>
<pre class="r"><code>c0_prior &lt;- nrow(train_sample_df[train_sample_df$y == 0,]) /
  nrow(train_sample_df)

c1_prior &lt;- nrow(train_sample_df[train_sample_df$y == 1,]) /
  nrow(train_sample_df)

c2_prior &lt;- nrow(train_sample_df[train_sample_df$y == 2,]) /
  nrow(train_sample_df)</code></pre>
<p>Then, just like with the Bayes classifier, we create our discriminant functions and <code>apply</code> them to the data via a classifier function.</p>
<pre class="r"><code>d0_QDA &lt;- function(x_vec){
  -.5 * log(det(sample_cov_mat_c0)) -
    .5 * t(x_vec - sample_c0_mean_vec) %*% solve(sample_cov_mat_c0) %*%
    (x_vec - sample_c0_mean_vec) +
    log(c0_prior)
}

d1_QDA &lt;- function(x_vec){
  -.5 * log(det(sample_cov_mat_c1)) -
    .5 * t(x_vec - sample_c1_mean_vec) %*% solve(sample_cov_mat_c1) %*%
    (x_vec - sample_c1_mean_vec) +
    log(c1_prior)
}

d2_QDA &lt;- function(x_vec){
  -.5 * log(det(sample_cov_mat_c2)) -
    .5 * t(x_vec - sample_c2_mean_vec) %*% solve(sample_cov_mat_c2) %*%
    (x_vec - sample_c2_mean_vec) +
    log(c2_prior)
}

QDA_classifier &lt;- function(x_vec){
  score_c0 &lt;- d0_QDA(x_vec)
  score_c1 &lt;- d1_QDA(x_vec)
  score_c2 &lt;- d2_QDA(x_vec)
  
  if (score_c0 &gt; score_c1 &amp; score_c0 &gt; score_c2) {
    0
  } else if (score_c1 &gt; score_c0 &amp; score_c1 &gt; score_c2) {
    1
  } else {
    2
  }
}

train_sample_df$QDA_predicted_y &lt;- apply(train_sample_df[, c(&quot;X1&quot;, &quot;X2&quot;)], 
                                         1, QDA_classifier)</code></pre>
</div>
<div id="visualization" class="section level3">
<h3>Visualization</h3>
<p>The nice non-linear decision boundaries shown in <a href="https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12_toc.pdf" target="_blank">Elements of Statistical Learning</a> were created through an extensive contouring process, which is way outside the scope of this working example.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> While we won’t plot the decision boundaries directly, we can pretty easily plot the decision areas using an approach similar to that we use for plotting a maximum likelihood surface in the <a href="univariate_LDA.html" target="_blank">univariate logistic regression</a> example. This is a hacky approach in this example in a way that it’s not in the logistic regression example, but if it works it should do the trick well enough.</p>
<p>The basic idea is that we should be able to apply the QDA classifier to a set of cartesian coordinates for our X1 and X2 variables and then show the approximate areas in which the QDA classifier would choose one class or another. This obviously gives us the approximate decision boundaries between classes as well.</p>
<p>First, we can create our “surface” of coordinates by creating a <a href="https://tibble.tidyverse.org/" target="_blank">tibble</a> of every combination of X1 and X2 values within some space. We’ll restrict our space to the smallest and largest train sample values for X1 and X2. We obviously can’t classify <em>every</em> point in this space, so we’ll use <a href="https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/seq" target="_blank"><code>seq</code></a> to create a tibble of coordinates from approximately the smallest and largest values of our two variables in some increment (in this case we’ll use .05).</p>
<pre class="r"><code>X1_vec &lt;- seq(floor(min(train_sample_df$X1)), ceiling(max(train_sample_df$X1)), .05)
X2_vec &lt;- seq(floor(min(train_sample_df$X2)), ceiling(max(train_sample_df$X2)), .05)

surface &lt;- expand_grid(X1_vec, X2_vec)
colnames(surface) &lt;- c(&quot;X1&quot;, &quot;X2&quot;)</code></pre>
<p>We’ve already created our LDA classifier, so we can simply <code>apply</code> it to our surface tibble of coordinates.</p>
<pre class="r"><code>surface$class &lt;- apply(surface[, c(&quot;X1&quot;, &quot;X2&quot;)], 1, QDA_classifier)</code></pre>
<p>Rather than <a href="https://ggplot2.tidyverse.org/reference/geom_contour.html" target="_blank"><code>geom_contour_filled</code></a> like in the logistic example, we use <a href="https://ggplot2.tidyverse.org/reference/geom_tile.html" target="_blank"><code>geom_tile</code></a> because our decision areas aren’t parts of a 3D surface and the classifications aren’t part of a continuous scale. ggplot2 layers plots based on what order they are expressed in the <code>ggplot</code> expression, so instead of adding the tile layer to our existing scatterplot (which would layer the tiles <em>on top of</em> the scatter points) we will rewrite the <code>geom_point</code> expression that provides the scatterplot after the <code>geom_tile</code> expression (which layers the scatterplot on top of the background of the classification areas).</p>
<pre class="r"><code>QDA_decision_areas &lt;- ggplot() +
  geom_tile(data = surface, aes(x = X1, y = X2, fill = as.factor(class)), alpha = .25) +
  scale_fill_manual(values = c(&quot;red&quot;, &quot;blue&quot;, &quot;green&quot;),
                    name = &quot;QDA Classification Area&quot;) +
  geom_point(data = train_sample_df, aes(x = X1, y = X2, color = as.factor(y))) +
  scale_color_manual(values = c(&quot;red&quot;, &quot;blue&quot;, &quot;green&quot;),
                     name = &quot;Class&quot;) +
  labs(title = &quot;QDA: Train Scatterplot with Classification Areas&quot;)</code></pre>
<p><img src="multivariate_QDA_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
</div>
</div>
<div id="testing" class="section level2">
<h2>Testing</h2>
<p>Using the Bayes and QDA classifiers we’ve built, we can easily apply both to the test data just like we did with the training data.</p>
<pre class="r"><code>test_sample_df$bayes_predicted_y &lt;- apply(test_sample_df[, c(&quot;X1&quot;, &quot;X2&quot;)], 
                                           1, bayes_classifier)

test_sample_df$QDA_predicted_y &lt;- apply(test_sample_df[, c(&quot;X1&quot;, &quot;X2&quot;)], 
                                         1, QDA_classifier)</code></pre>
<p>What does the misclassification rate for QDA look like?</p>
<pre class="r"><code>QDA_misclass_rate &lt;- nrow(test_sample_df[test_sample_df$y != 
                                           test_sample_df$QDA_predicted_y,]) /
  nrow(test_sample_df)

QDA_misclass_rate * 100</code></pre>
<pre><code>## [1] 6.888889</code></pre>
<p>And what does the misclassification rate for our (optimal) Bayes classifier look like?</p>
<pre class="r"><code>bayes_misclass_rate &lt;- nrow(test_sample_df[test_sample_df$y != 
                                           test_sample_df$bayes_predicted_y,]) /
  nrow(test_sample_df)

bayes_misclass_rate * 100</code></pre>
<pre><code>## [1] 6.222222</code></pre>
<div id="visualization-1" class="section level3">
<h3>Visualization</h3>
<p>We can easily visualize how our trained QDA decision decision areas map to our test data. Using the tile-and-scatterplot we created above, all we need to do is simply change our train data to our test data for the scatterplot layer.</p>
<pre class="r"><code>QDA_decision_areas_test &lt;- ggplot() +
  geom_tile(data = surface, aes(x = X1, y = X2, fill = as.factor(class)), alpha = .25) +
  scale_fill_manual(values = c(&quot;red&quot;, &quot;blue&quot;, &quot;green&quot;),
                    name = &quot;QDA Classification Area&quot;) +
  geom_point(data = test_sample_df, aes(x = X1, y = X2, color = as.factor(y))) +
  scale_color_manual(values = c(&quot;red&quot;, &quot;blue&quot;, &quot;green&quot;),
                     name = &quot;Class&quot;) +
  labs(title = &quot;QDA: Test Scatterplot with Classification Areas&quot;)</code></pre>
<p><img src="multivariate_QDA_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>This is clearly a longer way of saying “I don’t know how to do it and don’t want to learn it for this.”<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
</ol>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
