[["index.html", "ESL &amp; ISLR Working Examples ", " ESL &amp; ISLR Working Examples Isaac Baumann Updated 2021-10-07 "],["introduction.html", "Introduction", " Introduction This book is intended to be a collection of working examples of common statistical learning models, built as I make my way through Hastie, Tibshirani, and Friedman’s Elements of Statistical Learning and Hastie, Tibshirani, James, and Witten’s Introduction to Statistical Learning With Applications in R. All code is written simply for fun in my free time while going through the above texts and are meant only for educational/fun purposes. In most cases the processes/algorithms used are not the most stable or efficient in finite precision. Please open an issue if you notice any errors! "],["packages.html", "Packages", " Packages Packages used extensively throughout this book include: MASS patchwork the tidyverse family of packages These packages may not be used in every example, but typically are. Other packages used in specific examples will be noted within those examples. This book also uses xaringanExtra to enable snippet copying. "],["logistic-regression.html", "Logistic Regression", " Logistic Regression This example walks through fitting a univariate logistic regression model to (idealized) generated data. It should be easy to see how this generalizes to multivariate cases. It assumes some prior knowledge of logistic regression and log-odds (or see Elements of Statistical Learning) and will be light on theory. We’ll skip doing a train/test split and comparison in this example and focus solely on implementing logistic regression from scratch. "],["data-generation.html", "Data Generation", " Data Generation This example uses the tidyverse and latex2exp packages. library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ── ## ✓ ggplot2 3.3.5 ✓ purrr 0.3.4 ## ✓ tibble 3.1.5 ✓ dplyr 1.0.7 ## ✓ tidyr 1.1.4 ✓ stringr 1.4.0 ## ✓ readr 1.4.0 ✓ forcats 0.5.1 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() library(latex2exp) We’ll generate data with 1,000 observations from a simple case: a binary (0/1) balanced outcome… y &lt;- append(rep(0, each = 500), rep(1, each = 500)) …and a Gaussian independent variable. To ensure some overlap between the \\(y = 0\\) and \\(y = 1\\) observations we’ll draw from two normal distributions with offset means. To include an intercept term we’ll augment our independent variable vectors with a one-vector. means &lt;- sample.int(6, 2, replace = FALSE) x &lt;- append(rnorm(500, mean = means[1], sd = 1), rnorm(500, mean = means[2], sd = 1)) x &lt;- cbind(1, x) colnames(x) &lt;- c(&quot;Int&quot;, &quot;X1&quot;) "],["the-log-likelihood-surface.html", "The Log-Likelihood Surface", " The Log-Likelihood Surface Like linear regression, logistic regression is simply an optimization problem. In the logistic case, the function we maximize is the log-likelihood function \\[\\ell (\\vec{\\beta}) = \\sum_{i = 1}^N \\left[ \\vec{y}_i \\vec{\\beta}^T \\vec{x}_i - \\log(1 + e^{\\vec{\\beta}^T \\vec{x}_i}) \\right]\\] This function is strictly concave, so it will have one maximum and our \\(\\beta\\)s (\\(\\beta_0\\) for the intercept and \\(\\beta_1\\) for our single independent variable) will (hopefully!) maximize this function. In this case we can easily visualize a log-likelihood surface by evaluating the function over some set of values for our two \\(\\beta\\)s. First we’ll create our set of \\(\\beta\\) values and then use those vectors to create a tibble of coordinates for our plot as well as put our \\(y\\) vector and design matrix together to form a data matrix for easier evaluation. b0_vec &lt;- seq(-20, 20, .5) b1_vec &lt;- seq(-20, 20, .5) surface &lt;- expand_grid(b0_vec, b1_vec) data_matrix &lt;- cbind(y, x) Since we’re evaluating the log-likelihood function over the data for all combinations of \\(\\beta\\) values in our set, we’ll write two functions. The first handles the interior of the sum of the log-likelihood log_like_interior &lt;- function(data, b_0, b_1){ b_vec &lt;- c(b_0, b_1) data[1] %*% t(b_vec) %*% data[2:3] - log(1 + exp(t(b_vec) %*% data[2:3])) } and the second handles the summation, applying over the data matrix. log_like_sum &lt;- function(b_vec){ sum(apply(data_matrix, 1, log_like_interior, b_0 = b_vec[1], b_1 = b_vec[2])) } Then we can apply the summation function over the \\(\\beta\\)-value coordinated in our surface tibble and add the appropriate log-likelihood to that tibble. surface$l &lt;- apply(surface, 1, log_like_sum) Now that we have our \\(\\beta\\) coordinates and their corresponding log-likelihoods for our data, we can plot the log-likelihood surface. likelihood_surface_plot &lt;- ggplot(surface, aes(x = b0_vec, y = b1_vec, z = l, fill = l)) + geom_raster(interpolate = TRUE) + geom_contour(bins = 50, color = &quot;grey&quot;, size = 0.5) + scale_fill_distiller(palette = &quot;YlOrRd&quot;, name = &quot;Log-Likelihood&quot;, direction = 1) + labs(title = &quot;Surface Plot of Log-Likelihood Function&quot;, subtitle = &quot;Evaluated Across Range of Beta Coefficients&quot;, x = TeX(&quot;$\\\\beta_0$&quot;), y = TeX(&quot;$\\\\beta_1$&quot;)) # + # theme(legend.position = &quot;none&quot;) Our \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) coordinates estimated by our logistic regression should be in the bright yellow area, representing our highest-valued bin of log-likelihoods, at the maximum of the log-likelihood surface. "],["implementation.html", "Implementation", " Implementation To fit the logistic regression model to the data and find the optimal \\(\\hat{\\beta}\\) vector we set the derivatives of the log-likelihood function (or score equations ) with respect to \\(\\vec{\\beta}\\) to zero: \\[\\frac{\\partial \\ell(\\vec{\\beta})}{\\partial \\vec{\\beta}} = \\sum_{i = 1}^N x_i (y_i - \\Pr (Y = 1 | X = x_i; \\vec{\\beta})) = 0\\] Method 1: Newton-Raphson/IRLS The method Hastie et al. use in Elements of Statistical Learning is the Newton-Raphson method (which is re-written to be a iteratively re-weighted least squares problem), which iteratively updates \\(\\vec{\\hat{\\beta}}\\) to find the maximum likelihood. We will use a stopping criteria that checks how the score equations for \\(\\vec{\\hat{\\beta}}_0\\) and \\(\\vec{\\hat{\\beta}}_1\\) evaluate and stops iterating when they are within a tolerable distance from zero. This method uses a diagonal matrix \\(W\\) which contains the weights derived from \\(\\Pr (Y = 1 | X = x_i; \\vec{\\hat{\\beta}})\\) where \\(\\vec{\\hat{\\beta}}\\) contains the estimated coefficients from the previous iteration. These weights are then used to solve a weighted least squares problem, which produces a vector \\(z\\) of “adjusted” responses that are used to update the coefficients. In each iteration \\(i\\) we will solve \\[\\vec{\\hat{\\beta}} \\vphantom{1}^{i} = (X^T W X)^{-1} X^T W \\vec{z}\\] where \\[z = X \\vec{\\hat{\\beta}} \\vphantom{1}^{i - 1} + W^{-1} (\\vec{y} - \\vec{p})\\] and \\(\\vec{p}\\) are the fitted probabilities given \\(\\vec{\\hat{\\beta}} \\vphantom{1}^{i - 1}\\). beta_vec_irls &lt;- c(0,0) max_check &lt;- FALSE irls_iterations &lt;- 0 while (max_check != TRUE) { p &lt;- exp(x %*% beta_vec_irls) / (1 + exp(x %*% beta_vec_irls)) W &lt;- diag(as.vector(p * (1 - p))) z &lt;- x %*% beta_vec_irls + solve(W) %*% (y - p) beta_vec_irls &lt;- solve(t(x) %*% W %*% x) %*% t(x) %*% W %*% z # Check that we are actually maximizing the likelihood function by checking # if the score functions are evaluating to zero score &lt;- t(x) %*% (y - p) score &lt;- c(score[1], score[2]) max_check &lt;- all.equal(c(0,0), score, tolerance = .Machine$double.eps ^ 0.5) irls_iterations &lt;- irls_iterations + 1 } Our \\(\\vec{\\hat{\\beta}}\\) for this example from IRLS is: ## [,1] ## Int -6.872162 ## X1 2.696987 Method 2: Modified IRLS Because \\(W\\) is a \\(N \\times N\\) matrix, IRLS process may not always be effcient. We can side-step the large \\(W\\) matrix and matrix operations involving it by instead directly multiplying the rows of the \\(X\\) matrix by the predicted probability of \\(Y = 1\\) for the observation corresponding to that row (given the “old” \\(\\beta\\) from the previous iteration); see pseudo-code. This new matrix will be \\(\\tilde{X}\\). Note that, in R, %*% is a dot product operator. But if two matrices have identical dimensions then the traditional multiplication operator * will perform an element-for-element scalar multiplication. So we will use our vector of probabilities to create an appropriately dimensioned matrix using matrix and ncol to create \\(\\tilde{X}\\). beta_vec_mod &lt;- c(0,0) max_check &lt;- FALSE mod_iterations &lt;- 0 while (max_check != TRUE) { p &lt;- exp(x %*% beta_vec_mod) / (1 + exp(x %*% beta_vec_mod)) p_mat &lt;- matrix(p * (1 - p), length(p * (1 - p)), ncol(x)) # Use `sweep` to multiply each row of x by scalar vector x_tilde &lt;- x * p_mat beta_vec_mod &lt;- beta_vec_mod + solve(t(x) %*% x_tilde) %*% t(x) %*% (y - p) # Check that we are actually maximizing the likelihood function by checking # if the score functions are evaluating to zero score &lt;- t(x) %*% (y - p) score &lt;- c(score[1], score[2]) max_check &lt;- all.equal(c(0,0), score, tolerance = .Machine$double.eps ^ 0.5) mod_iterations &lt;- mod_iterations + 1 } This will sometimes converge a little more quickly. The number of iterations for this particular example were ## [1] &quot;IRLS iterations: 9&quot; ## [1] &quot;Modified IRLS iterations: 9&quot; Our \\(\\vec{\\hat{\\beta}}\\) for this example from the modified algorithm is: ## [,1] ## Int -6.872162 ## X1 2.696987 "],["testing.html", "Testing", " Testing Comparing to glm We can easily check the coefficients we’ve calculated against those produced by R’s glm. By default glm uses IRLS to fit models. glm_model &lt;- glm(y ~ X1, data = as.data.frame(data_matrix), family = &quot;binomial&quot;) beta_vec_irls ## [,1] ## Int -6.872162 ## X1 2.696987 beta_vec_mod ## [,1] ## Int -6.872162 ## X1 2.696987 glm_model$coefficients ## (Intercept) X1 ## -6.872162 2.696987 The Log-Likelihood Surface We can also use the log-likelihood surface plot to (approximately) verify that the coefficients we estimated are correct and that the logistic regression estimates do maximize the log-likelihood function by plotting the point for our estimates on the surface plot: likelihood_surface_plot &lt;- likelihood_surface_plot + geom_point(aes(x = beta_vec_mod[1], y = beta_vec_mod[2])) Plotting the Logistic Fit To get the classic logistic regression plot we can calculate \\[\\Pr (Y = 1 | X = x_i; \\vec{\\beta}) = \\frac{e^{\\vec{x} \\hat{\\vec{\\beta^T}}}}{1 + e^{\\vec{x} \\hat{\\vec{\\beta^T}}}}\\] for all \\(x_i\\) and plot those probabilities as a line against the data points on the \\((x, y)\\) plane. We’ll also color-code the points by how our model fit classifies the points, using the decision rule that an observation is classified to \\(\\hat{Y} = 0\\) if \\(\\Pr (Y = 1 | X = x_i; \\vec{\\beta}) &lt; 0.5\\) and \\(\\hat{Y} = 1\\) otherwise. pred_prob &lt;- exp(data_matrix[, 2:3] %*% beta_vec_mod) / (1 + exp(data_matrix[, 2:3] %*% beta_vec_mod)) data_tibble &lt;- as_tibble(data_matrix) data_tibble$pred_prob &lt;- pred_prob[,1] data_tibble$pred &lt;- if_else(data_tibble$pred_prob &lt; .5, 0, 1) logistic_plot_preds &lt;- ggplot(data_tibble) + geom_point(aes(X1, y, color = as.factor(pred))) + geom_line(aes(X1, pred_prob)) + scale_color_manual(name = &quot;Predicted y&quot;, values = c(&quot;red&quot;, &quot;blue&quot;)) "],["linear-discriminant-analysis.html", "Linear Discriminant Analysis", " Linear Discriminant Analysis Linear discriminant analysis is way of estimating probability of class membership using Bayes’ theorem. We first do a simple univariate example with binary outcomes, then a multivariate case with three outcomes. "],["univariate-lda.html", "Univariate LDA", " Univariate LDA This example walks through using linear discriminant analysis to classify observations in a two-class univariate setting with (idealized) generated data. Theory The LDA classifier uses estimates of mean and variance for each class as well as discriminant functions to determine the probability that an observation is of a particular class. The LDA classifier then assigns to each observation that class for which the estimated probability of membership is highest. LDA assumes equal variance between classes. The theory discussion and example reflect this. Bayes’ Theorem The general LDA model explored here and in Elements of Statistical Learning is based on Bayes’ theorem for continuous variables. Since we have normal data, we use the normal probability density function (for each class, respectively) as our probability function. Taking Bayes’ theorem \\[\\Pr(Y = k|X = x) = \\frac{\\pi_k f_k(x)}{\\sum^K_{j = 1} \\pi_j f_j(x)}\\] where \\(k\\) represents one class and \\(j\\) all others, \\(\\pi_k\\) represents the prior probability of class \\(k\\), and \\(f(\\cdot)\\) is some probability function, we substitute the normal PDF for \\(f(\\cdot)\\) and rewrite \\[\\Pr(Y = k|X = x) = \\frac{\\pi_k \\frac{1}{\\sqrt{2 \\color{red}{\\pi} \\sigma_k}} e^{ \\left( \\frac{-1}{2\\sigma^2_k}(x - \\mu_k)^2 \\right)}}{\\sum^K_{j = 1} \\pi_j \\frac{1}{\\sqrt{2 \\color{red}{\\pi} \\sigma_j}} e^{ \\left( \\frac{-1}{2\\sigma^2_j}(x - \\mu_j)^2 \\right)}}\\] where \\(\\color{red}{\\pi}\\) is literally the value pi (as used in the normal PDF), not a prior probability. In this example, we only have two classes and we will set \\(\\sigma_k = \\sigma_l\\). A Bayes decision boundary is a boundary between classes along which the classifier determines the probability of class membership to be equal, i.e. \\(\\Pr(Y = k|X = x) = \\Pr(Y = l|X = x)\\). Discriminant Functions So where does the “discriminant” in LDA come from? How does LDA divide between classes? In the two-class case like this example, we can use the log of the ratio of the probabilities to get our discriminant functions for each class. Taking the ratio of the probabilities… \\[\\begin{align} \\frac{\\Pr(Y = k|X = x)}{\\Pr(Y = l|X = x)} &amp;= \\frac{\\pi_k f_k(x)}{\\sum^K_{j = 1} \\pi_j f_j(x)} \\frac{\\sum^K_{j = 1} \\pi_j f_j(x)}{\\pi_l f_l(x)} \\\\ &amp;= \\frac{\\pi_k f_k(x)}{\\pi_l f_l(x)} \\end{align}\\] …and taking the log of the ratio… \\[\\begin{align} \\log\\frac{\\Pr(Y = k|X = x)}{\\Pr(Y = l|X = x)} &amp;= \\log \\frac{\\pi_k f_k(x)}{\\pi_l f_l(x)} \\\\ &amp;= \\log{\\frac{\\pi_k}{\\pi_l}} + \\log{\\frac{f_k(x)}{f_l(x)}} \\end{align}\\] …we can build a general formula for finding our discriminant functions. Plugging in the normal PDF for \\(f(\\cdot)\\) in this case allows for quite a lot of simplification. \\[\\begin{align} \\log\\frac{\\Pr(Y = k|X = x)}{\\Pr(Y = l|X = x)} &amp;= \\log{\\frac{\\pi_k}{\\pi_l}} + \\log \\frac{\\frac{1}{\\sqrt{2 \\color{red}{\\pi} \\sigma}} e^{ \\left( \\frac{-1}{2\\sigma^2}(x - \\mu_k)^2 \\right)}}{\\frac{1}{\\sqrt{2 \\color{red}{\\pi} \\sigma}} e^{ \\left( \\frac{-1}{2\\sigma^2}(x - \\mu_l)^2 \\right)}} \\\\ &amp;= \\log{\\frac{\\pi_k}{\\pi_l}} + \\log \\left( \\frac{1}{\\sqrt{2 \\color{red}{\\pi} \\sigma}} \\bigg/ \\frac{1}{\\sqrt{2 \\color{red}{\\pi} \\sigma}} \\right) + \\log \\frac{e^{ \\left( \\frac{-1}{2\\sigma^2}(x - \\mu_k)^2 \\right)}}{e^{ \\left( \\frac{-1}{2\\sigma^2}(x - \\mu_l)^2 \\right)}} \\\\ &amp;= \\log{\\frac{\\pi_k}{\\pi_l}} + \\log e^{ \\left( \\frac{-1}{2\\sigma^2}(x - \\mu_k)^2 \\right)} - \\log e^{ \\left( \\frac{-1}{2\\sigma^2}(x - \\mu_l)^2 \\right)} \\\\ &amp;= \\log{\\frac{\\pi_k}{\\pi_l}} - \\frac{1}{2\\sigma^2}(x - \\mu_k)^2 + \\frac{1}{2\\sigma^2}(x - \\mu_l)^2 \\\\ &amp;= \\log{\\frac{\\pi_k}{\\pi_l}} + \\frac{-x^2 + 2x \\mu_k - \\mu_k^2 + x^2 - 2x \\mu_l + \\mu_l^2}{2 \\sigma^2} \\\\ &amp;= \\log{\\frac{\\pi_k}{\\pi_l}} + x \\frac{\\mu_k - \\mu_l}{\\sigma^2} + \\frac{\\mu_l^2 - \\mu_k^2}{2 \\sigma^2} \\end{align}\\] Remember the Bayes decision boundary? The LDA decision boundary also exists where the probability of an observation being class \\(k\\) is equal to its probability of being class \\(l\\) in the two-class case, but using the sample data. If we think of our log-ratio in this way (in which case the ratio is 1 and so the log of the ratio is 0) and evaluate on the boundary then we can easily use the above expression to find our discriminant functions. \\[\\Pr(Y = k|X = x) = \\Pr(Y = l|X = x) \\rightarrow \\frac{\\Pr(Y = k|X = x)}{\\Pr(Y = l|X = x)} = 1\\] \\[\\begin{align} \\log \\left( 1 \\right) &amp;= \\log{\\frac{\\pi_k}{\\pi_l}} + x \\frac{\\mu_k - \\mu_l}{\\sigma^2} + \\frac{\\mu_l^2 - \\mu_k^2}{2 \\sigma^2} \\\\ 0 &amp;= \\log{\\pi_k} - \\log{\\pi_l} + \\frac{x \\mu_k}{\\sigma^2} - \\frac{x \\mu_l}{\\sigma^2} + \\frac{\\mu_l^2}{2 \\sigma^2} - \\frac{\\mu_k^2}{2 \\sigma^2} \\\\ \\underbrace{\\log \\pi_l + \\frac{x \\mu_l}{\\sigma^2} - \\frac{\\mu_l^2}{2 \\sigma^2}}_{\\delta_l(x)} &amp;= \\underbrace{\\log \\pi_k + \\frac{x \\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2 \\sigma^2}}_{\\delta_k (x)} \\end{align}\\] \\(\\delta_k(x)\\) and \\(\\delta_l(x)\\) are our discriminant functions which we use to classify observations. We use a simple decision rule for classification: if for observation \\(x_i\\) the \\(k\\) discriminant function evaluates greater than the \\(l\\) discriminant function then we assign \\(x_i\\) to class \\(k\\). Setup This example uses the tidyverse and patchwork packages. library(tidyverse) library(patchwork) Data Generation We’ll generate train and test data for two classes (coded 0/1): 1,000 normally-distributed observations for each class with differing means for each class but equal variances. means &lt;- sample.int(6, 2, replace = FALSE) population0_mean &lt;- min(means) population0_sd &lt;- 1 population1_mean &lt;- max(means) population1_sd &lt;- 1 c0_train &lt;- tibble(y = 0, x = rnorm(1000, mean = population0_mean, sd = population0_sd)) c1_train &lt;- tibble(y = 1, x = rnorm(1000, mean = population1_mean, sd = population1_sd)) c0_test &lt;- tibble(y = 0, x = rnorm(1000, mean = population0_mean, sd = population0_sd)) c1_test &lt;- tibble(y = 1, x = rnorm(1000, mean = population1_mean, sd = population1_sd)) train_sample_df &lt;- bind_rows(c0_train, c1_train) test_sample_df &lt;- bind_rows(c0_test, c1_test) We can easily visualize the distributions of our class-wise populations: population_density &lt;- ggplot(data.frame(x = c(population0_mean - 3 * population0_sd, population1_mean + 3 * population1_sd)), aes(x)) + stat_function(fun = dnorm, aes(color = &quot;Class: 0&quot;), size = 1.25, args = list(mean = population0_mean, sd = population0_sd)) + stat_function(fun = dnorm, aes(color = &quot;Class: 1&quot;), size = 1.25, args = list(mean = population1_mean, sd = population1_sd)) + scale_color_brewer(palette = &quot;Dark2&quot;) + labs(title = &quot;Population Densities of Two Classes&quot;) + theme(legend.title = element_blank()) population_density Let’s also quickly visualize the train and test data: train_sample_histogram &lt;- ggplot(train_sample_df) + geom_histogram(aes(x = x, fill = as.factor(y)), alpha = .7, position = &quot;identity&quot;) + scale_color_brewer(palette = &quot;Dark2&quot;, labels = c(&quot;Class: 0&quot;, &quot;Class: 1&quot;)) + labs(title = &quot;Training Sample&quot;) + theme(legend.title = element_blank()) test_sample_histogram &lt;- ggplot(test_sample_df) + geom_histogram(aes(x = x, fill = as.factor(y)), alpha = .7, position = &quot;identity&quot;) + scale_color_brewer(palette = &quot;Dark2&quot;, labels = c(&quot;Class: 0&quot;, &quot;Class: 1&quot;)) + labs(title = &quot;Test Sample&quot;) + theme(legend.title = element_blank()) train_sample_histogram + test_sample_histogram Implementation Bayes Decision Boundary We’ll use the Bayes classifier as a comparison for our LDA. As covered in the theory section the Bayes classifier simply assigns an observation to the class for which an observation has the highest prior probability of belonging. The Bayes decision boundary is the boundary for which the probability of an observation being classified by the Bayes classifier is equal among classes; in this case, we will only have one boundary because we only have two classes. We’ll compute the optimal Bayes decision boundary from the population data to compare our LDA against. In this case, since we only have two classes and one independent variable/predictor, it’s easy: bayes_decision &lt;- (population0_mean ^ 2 - population1_mean ^ 2) / (2 * (population0_mean - population1_mean)) We can easily add this optimal boundary to our population density plot and histogram. Unsurprisingly, the decision boundary lies where the two PDFs meet: population_density + geom_vline(xintercept = bayes_decision, color = &quot;black&quot;) train_sample_histogram + geom_vline(xintercept = bayes_decision, color = &quot;black&quot;) LDA Decision Boundary For our estimates of the mean and variance we use the empirical (i.e., data/sample-derived) class-specific means and standard deviations. Even though the class variances are equivalent in this case we will treat them separately for thoroughness. We also need to calculate the empirical prior probability that an observation belongs to each class. In this case, both will be 0.5 since we have equal samples but we will again calculate these separately for thoroughness. class_0_mean &lt;- mean(train_sample_df$x[train_sample_df$y == 0]) class_0_var &lt;- var(train_sample_df$x[train_sample_df$y == 0]) class_1_mean &lt;- mean(train_sample_df$x[train_sample_df$y == 1]) class_1_var &lt;- var(train_sample_df$x[train_sample_df$y == 1]) class_0_prior &lt;- length(train_sample_df$x[train_sample_df$y == 0]) / length(train_sample_df$x) class_1_prior &lt;- length(train_sample_df$x[train_sample_df$y == 1]) / length(train_sample_df$x) Building our discriminant functions as specified in the previous section is simple. We’ll write these as functions. d0 &lt;- function(x){ x * (class_0_mean / class_0_var) - (class_0_mean ^ 2 / (2 * class_0_var ^ 2)) + log(class_0_prior) } d1 &lt;- function(x){ x * (class_1_mean / class_1_var) - (class_1_mean ^ 2 / (2 * class_1_var ^ 2)) + log(class_1_prior) } Then we can build our simple decision rule function, assigning an observation to class \\(k\\) if \\(\\delta_k(x) &gt; \\delta_l(x)\\) and vice versa: LDA &lt;- function(x){ score_0 &lt;- d0(x) score_1 &lt;- d1(x) ifelse(score_0 &gt; score_1, 0, 1) } Then we can apply our LDA function over our training data to classify the observations: train_sample_df$predicted_y &lt;- apply(train_sample_df[,2], MARGIN = 1, LDA) Because our discriminant functions are linear in \\(x\\), we can also solve for the value of \\(x\\) that acts as our LDA boundary: LDA_decision &lt;- (class_0_mean ^ 2 / (2 * class_0_var) - class_1_mean ^ 2 / (2 * class_0_var) + log(class_1_prior) - log(class_0_prior)) / (class_0_mean / class_0_var - class_1_mean / class_1_var) And we can easily visualize both our (population-based) Bayes decision boundary and our (sample-based) LDA decision boundary: train_sample_histogram + geom_vline(xintercept = bayes_decision, color = &quot;black&quot;) + geom_vline(xintercept = LDA_decision, color = &quot;black&quot;, linetype = &quot;longdash&quot;) + labs(subtitle = &quot;Solid line: Bayes (optimal) decision boundary\\nDashed line: LDA decision boundary&quot;) Testing Using the decision boundary from our LDA model applied to the training data, we can simply and easily classify the test data observations: test_sample_df$predicted_y &lt;- if_else(test_sample_df$x &lt; LDA_decision, 0, 1) What’s LDA’s misclassification rate for the test data in this case? LDA_misclass_rate &lt;- nrow(test_sample_df[test_sample_df$y != test_sample_df$predicted_y,]) / nrow(test_sample_df) LDA_misclass_rate * 100 ## [1] 0.35 It’s certainly not perfect! But how imperfect is it? Since we’re using Bayes’ classifier on the population data as our standard, we can apply its decision rule to the test data and calculate its misclassification rate. test_sample_df$bayes_predicted_y &lt;- if_else(test_sample_df$x &lt; bayes_decision, 0, 1) bayes_misclass_rate &lt;- nrow(test_sample_df[test_sample_df$y != test_sample_df$bayes_predicted_y,]) / nrow(test_sample_df) bayes_misclass_rate * 100 ## [1] 0.35 Of course, it is possible for LDA to “outperform” the optimal Bayes classifier depending on the train and test samples, but the Bayes classifier here in a sense represents the “true” optimal classifier since it is coming from population data. "],["multivariate-lda.html", "Multivariate LDA", " Multivariate LDA This example walks through using linear discriminant analysis to classify observations in a three-class multivariate setting with (idealized) generated data. Theory This section builds on the theory behind univariate LDA example. The theory in the multivariate case is essentially the same, but we will cover it here for thoroughness. Similar to the univariate example, in the multivariate case LDA assumes identical variance-covariance matrices between classes. The theory discussion and example reflect this. Bayes’ Theorem Like in the univariate case, take Bayes’ theorem, \\[\\Pr(Y = k|X = x) = \\frac{\\pi_k f_k(x)}{\\sum^K_{j = 1} \\pi_j f_j(x)}\\] where \\(k\\) is one class and \\(j\\) all others, \\(\\pi_k\\) represents the prior probability of class \\(k\\), \\(p\\) is the number of variables/predictors, and \\(f(\\cdot)\\) is some probability function. In this case we use the multivariate normal PDF \\[\\frac{1}{(2\\color{red}{\\pi})^{\\frac{p}{2}} \\det(\\Sigma)^\\frac{1}{2}} e^{\\frac{-1}{2} (x - \\mu_k)^T \\Sigma^{-1} (x - \\mu_k)}\\] where, in this idealized example, \\(\\Sigma_k, \\Sigma_l, \\ldots, \\Sigma_K = \\Sigma\\) since we are assuming (and in fact generating data from a distribution in which) all classes have the same covariance matrix. As noted in the univariate example, \\(\\color{red}{\\pi}\\) in the PDF is the literal value pi, not a prior probability. Whereas in the univariate case we only had two outcomes/classes, in this example we have three. Instead of pairwise solving for two discriminant functions we need to pairwise solve for three. The process is of course the same for each pair, so we will generically work through solving for the discriminant functions for a single pair of classes. Just as in the univariate case, in order to arrive at our discriminant functions we take the log of the ratio of conditional probabilities and the summation terms in the ratio cancel, leaving us with \\[\\begin{align} \\log\\frac{\\Pr(Y = k|X = x)}{\\Pr(Y = l|X = x)} &amp;= \\log \\frac{\\pi_k f_k(x)}{\\pi_l f_l(x)} \\\\ &amp;= \\log{\\frac{\\pi_k}{\\pi_l}} + \\log{\\frac{f_k(x)}{f_l(x)}} \\end{align}\\] Plugging in the multivariate normal PDF for \\(f(\\cdot)\\), like in the univariate case, allows for a lot of simplification but with some twists: \\[\\small \\begin{align} \\log\\frac{\\Pr(Y = k|X = x)}{\\Pr(Y = l|X = x)} &amp;= \\log{\\frac{\\pi_k}{\\pi_l}} + \\log \\frac{\\frac{1}{(2\\color{red}{\\pi})^{\\frac{p}{2}} \\det(\\Sigma)^\\frac{1}{2}} e^{\\frac{-1}{2} (x - \\mu_k)^T \\Sigma^{-1} (x - \\mu_k)}}{\\frac{1}{(2\\color{red}{\\pi})^{\\frac{p}{2}} \\det(\\Sigma)^\\frac{1}{2}} e^{\\frac{-1}{2} (x - \\mu_l)^T \\Sigma^{-1} (x - \\mu_l)}} \\\\ &amp;= \\log{\\frac{\\pi_k}{\\pi_l}} + \\log \\left( \\frac{1}{(2\\color{red}{\\pi})^{\\frac{p}{2}} \\det(\\Sigma)^\\frac{1}{2}} \\bigg/ \\frac{1}{(2\\color{red}{\\pi})^{\\frac{p}{2}} \\det(\\Sigma)^\\frac{1}{2}} \\right) + \\log{\\frac{e^{\\frac{-1}{2} (x - \\mu_k)^T \\Sigma^{-1} (x - \\mu_k)}}{e^{\\frac{-1}{2} (x - \\mu_l)^T \\Sigma^{-1} (x - \\mu_l)}}} \\\\ &amp;= \\log{\\frac{\\pi_k}{\\pi_l}} - \\frac{1}{2} (x - \\mu_k)^T \\Sigma^{-1} (x - \\mu_k) + \\frac{1}{2} (x - \\mu_l)^T \\Sigma^{-1} (x - \\mu_l) \\\\ \\end{align}\\] Because \\(\\Sigma\\) represents the covariance matrix for all classes, a convenient cancellation occurs after we expand the right two terms in the expression above: \\[\\begin{align} \\log\\frac{\\Pr(Y = k|X = x)}{\\Pr(Y = l|X = x)} &amp;= \\log{\\frac{\\pi_k}{\\pi_l}} - \\frac{1}{2} (x - \\mu_k)^T \\Sigma^{-1} (x - \\mu_k) + \\frac{1}{2} (x - \\mu_l)^T \\Sigma^{-1} (x - \\mu_l) \\\\ &amp;= \\log{\\frac{\\pi_k}{\\pi_l}} \\color{red}{- \\frac{1}{2}x^T \\Sigma^{-1}x} + \\frac{1}{2}x^T \\Sigma^{-1} \\mu_k + \\frac{1}{2}\\mu_k^T \\Sigma^{-1} x - \\frac{1}{2} \\mu_k^T \\Sigma^{-1} \\mu_k \\\\ &amp;\\qquad \\qquad \\color{red}{+ \\frac{1}{2}x^T \\Sigma^{-1}x} - \\frac{1}{2}x^T \\Sigma^{-1} \\mu_l - \\frac{1}{2}\\mu_l^T \\Sigma^{-1} x + \\frac{1}{2} \\mu_l^T \\Sigma^{-1} \\mu_l \\\\ &amp;= \\log{\\frac{\\pi_k}{\\pi_l}} \\color{purple}{+ \\frac{1}{2}x^T \\Sigma^{-1} \\mu_k + \\frac{1}{2}\\mu_k^T \\Sigma^{-1} x} \\color{green}{- \\frac{1}{2} \\mu_k^T \\Sigma^{-1} \\mu_k} \\\\ &amp;\\qquad ~~~~~~~ \\color{purple}{- \\frac{1}{2}x^T \\Sigma^{-1} \\mu_l - \\frac{1}{2}\\mu_l^T \\Sigma^{-1} x} \\color{green}{+ \\frac{1}{2} \\mu_l^T \\Sigma^{-1} \\mu_l} \\\\ \\end{align}\\] This is where the fun starts. Because \\(\\Sigma\\) is symmetric (and so \\(\\Sigma^{-1}\\) is symmetric) \\(a^T \\Sigma^{-1} b = b^T \\Sigma^{-1} a\\). So the purple terms can be rewritten and consolidated: \\[\\begin{align} \\log\\frac{\\Pr(Y = k|X = x)}{\\Pr(Y = l|X = x)} &amp;= \\log{\\frac{\\pi_k}{\\pi_l}} \\color{purple}{+ x^T \\Sigma^{-1} \\mu_k} \\color{purple}{- x^T \\Sigma^{-1} \\mu_l} \\color{green}{- \\frac{1}{2} \\mu_k^T \\Sigma^{-1} \\mu_k} \\color{green}{+ \\frac{1}{2} \\mu_l^T \\Sigma^{-1} \\mu_l} \\\\ &amp;= \\log{\\frac{\\pi_k}{\\pi_l}} \\color{purple}{+ x^T \\Sigma^{-1} (\\mu_k - \\mu_l)} \\color{green}{- \\frac{1}{2} \\mu_k^T \\Sigma^{-1} \\mu_k} \\color{green}{+ \\frac{1}{2} \\mu_l^T \\Sigma^{-1} \\mu_l} \\\\ \\end{align}\\] The green terms can’t be easily simplified unless we creatively add zero to the equation. Using the above property that \\(a^T \\Sigma^{-1} b = b^T \\Sigma^{-1} a\\) we can do exactly that. Let’s add \\(0 = \\frac{1}{2} \\mu_k^T \\Sigma^{-1} \\mu_l - \\frac{1}{2} \\mu_l^T \\Sigma^{-1} \\mu_k\\) and factor and consolidate: \\[\\small \\begin{align} \\log\\frac{\\Pr(Y = k|X = x)}{\\Pr(Y = l|X = x)} &amp;= \\log{\\frac{\\pi_k}{\\pi_l}} \\color{purple}{+ x^T \\Sigma^{-1} (\\mu_k - \\mu_l)} \\color{green}{- \\frac{1}{2} \\mu_k^T \\Sigma^{-1} \\mu_k} \\color{green}{+ \\frac{1}{2} \\mu_l^T \\Sigma^{-1} \\mu_l + \\frac{1}{2} \\mu_k^T \\Sigma^{-1} \\mu_l - \\frac{1}{2} \\mu_l^T \\Sigma^{-1} \\mu_k} \\\\ &amp;= \\log{\\frac{\\pi_k}{\\pi_l}} \\color{purple}{+ x^T \\Sigma^{-1} (\\mu_k - \\mu_l)} \\color{green}{- \\frac{1}{2} \\mu_k^T \\Sigma^{-1} (\\mu_k - \\mu_l) - \\frac{1}{2} \\mu_l^T \\Sigma^{-1} (\\mu_k - \\mu_l)} \\\\ &amp;= \\log{\\frac{\\pi_k}{\\pi_l}} \\color{purple}{+ x^T \\Sigma^{-1} (\\mu_k - \\mu_l)} \\color{green}{- \\frac{1}{2} (\\mu_k + \\mu_l)^T \\Sigma^{-1} (\\mu_k - \\mu_l)} \\\\ \\end{align}\\] Discriminant Functions The above derivation gets us to the nice, neat expression in Elements of Statistical Learning, but for arriving at our discriminant functions it’s easier to stay with the fully-expanded-without-consolidating expression. From this point on we will work from the following expression we intermediately arrived at above: \\[\\log\\frac{\\Pr(Y = k|X = x)}{\\Pr(Y = l|X = x)} = \\log{\\frac{\\pi_k}{\\pi_l}} + x^T \\Sigma^{-1} \\mu_k - x^T \\Sigma^{-1} \\mu_l - \\frac{1}{2} \\mu_k^T \\Sigma^{-1} \\mu_k + \\frac{1}{2} \\mu_l^T \\Sigma^{-1} \\mu_l\\] Just like in the univariate case, all we need to do to get the discriminant functions is evaluate this ratio on the pairwise decision boundary where our probability ratio is 1 and so the log of the ratio is 0. Doing this, we can simply rearrange the expression to get our discriminant functions for classes \\(k\\) and \\(l\\) \\[\\small \\begin{align} 0 &amp;= \\log{\\pi_k} - \\log{\\pi_l} + x^T \\Sigma^{-1} \\mu_k - x^T \\Sigma^{-1} \\mu_l - \\frac{1}{2} \\mu_k^T \\Sigma^{-1} \\mu_k + \\frac{1}{2} \\mu_l^T \\Sigma^{-1} \\mu_l \\\\ \\underbrace{\\log{\\pi_l} + x^T \\Sigma^{-1} \\mu_l - \\frac{1}{2} \\mu_l^T \\Sigma^{-1} \\mu_l}_{\\delta_l (x)} &amp;= \\underbrace{\\log{\\pi_k} + x^T \\Sigma^{-1} \\mu_k - \\frac{1}{2} \\mu_k^T \\Sigma^{-1} \\mu_k}_{\\delta_k (x)} \\\\ \\end{align}\\] Again, just like in the univariate case, we evaluate the discriminant functions for each class for some observation’s vector of values for X1 and X2, then assign the class to that observation for which the discriminant function evaluates largest. Estimating the Variance-Covariance Matrix In reality, we typically don’t know the covariance matrix, prior probabilities, and class-wise population means for our variables. Instead, we need to estimate them from the sample data. Estimating the class-wise population means and prior probabilities is easy: we can simply use the empirical means and proportions as estimates, respectively. Estimating the common covariance matrix is less straightforward. We estimate this matrix by creating a weighted average of covariance matrices within each class with a Bessel-like correction. This is a lot like pooled variance. \\[\\hat{\\Sigma} = \\frac{\\sum_{k = 1}^K \\sum_{i = 1; y = k}^{n_k} (x_i - \\hat{\\mu}_k)(x_i - \\hat{\\mu}_k)^T}{n - K}\\] \\(K\\) here is the total number of classes. Because we are still assuming that all classes have the same covariance matrix (in this case, the estimated matrix \\(\\hat{\\Sigma}\\)) none of the derivations in the Bayes classifier theory section are different: we are simply substituting the “true” means and covariance matrix in the Bayes classifier with our estimates since we usually cannot know the “true” parameters. Setup This example uses the tidyverse and MASS packages. library(tidyverse) library(MASS) Data Generation We’ll generate data with three possible outcome classes (coded as 0, 1, and 2) with two independent variables/predictors. We’ll use MASS’s mvnorm function to sample from a multivariate normal distribution. Throughout this example we’ll refer to the first variable as X1 and the second variable as X2. For each class we’ll construct a vector of means for X1 and X2 with randomly sampled integers between 1 and 10: X1_means &lt;- sample.int(10, 3, replace = FALSE) X2_means &lt;- sample.int(10, 3, replace = FALSE) pop_mean_c0_X1 &lt;- X1_means[1] pop_mean_c1_X1 &lt;- X1_means[2] pop_mean_c2_X1 &lt;- X1_means[3] pop_mean_c0_X2 &lt;- X2_means[1] pop_mean_c1_X2 &lt;- X2_means[2] pop_mean_c2_X2 &lt;- X2_means[3] mu_c0 &lt;- c(pop_mean_c0_X1, pop_mean_c0_X2) mu_c1 &lt;- c(pop_mean_c1_X1, pop_mean_c1_X2) mu_c2 &lt;- c(pop_mean_c2_X1, pop_mean_c2_X2) Remember that in the univariate case LDA assumes equal variance for each class so in the multivariate case LDA assumes equal covariance for each class, meaning that the population covariance matrices for each class are identical (they also need to be positive semi-definite). For our data to be sampled from distributions with this idealized property, we’ll construct a single covariance matrix (which we’ll call pop_sigma) to use when sampling our data: pop_corr &lt;- runif(1, 0, 1) pop_var &lt;- runif(1, 0, 10) pop_sigma &lt;- matrix(c(pop_var, pop_corr, pop_corr, pop_var), 2, 2) Now that we have our variable- and class-specific means and a common covariance matrix we can use mvnorm to generate our sample data. We’ll create equally-sized train and test sets with 300 observations in each of our three classes and bind our X1 and X2 samples together with our outcome set into a data frame. Using a loop with the get and assign functions, we can variably refer to our class-level means as well as variably generate our class sample data sets. n &lt;- 300 for (i in 0:2) { class &lt;- paste(&quot;c&quot;, i, sep = &quot;&quot;) mu = paste(&quot;mu&quot;, class, sep = &quot;_&quot;) temp_train_data &lt;- mvrnorm(n = n, mu = get(mu), Sigma = pop_sigma) temp_test_data &lt;- mvrnorm(n = n, mu = get(mu), Sigma = pop_sigma) assign(paste(class, &quot;train&quot;, sep = &quot;_&quot;), temp_train_data) assign(paste(class, &quot;test&quot;, sep = &quot;_&quot;), temp_test_data) } train_sample_df &lt;- bind_rows( tibble(y = 0, X1 = c0_train[,1], X2 = c0_train[,2]), tibble(y = 1, X1 = c1_train[,1], X2 = c1_train[,2]), tibble(y = 2, X1 = c2_train[,1], X2 = c2_train[,2]) ) test_sample_df &lt;- bind_rows( tibble(y = 0, X1 = c0_test[,1], X2 = c0_test[,2]), tibble(y = 1, X1 = c1_test[,1], X2 = c1_test[,2]), tibble(y = 2, X1 = c2_test[,1], X2 = c2_test[,2]) ) What do our data look like? We can easily create scatterplots: train_sample_scatter &lt;- ggplot(train_sample_df) + geom_point(aes(x = X1, y = X2, color = as.factor(y))) + scale_color_brewer(palette = &quot;Dark2&quot;, name = &quot;class&quot;) + labs(title = &quot;Bivariate Training Sample of Three Classes: Scatterplot&quot;) test_sample_scatter &lt;- ggplot(test_sample_df) + geom_point(aes(x = X1, y = X2, color = as.factor(y))) + scale_color_brewer(palette = &quot;Dark2&quot;, name = &quot;class&quot;) + labs(title = &quot;Bivariate Test Sample of Three Classes: Scatterplot&quot;) train_sample_scatter Implementation The Bayes Classifier and Decision Boundaries Like with the univariate LDA example we will compare performance against the Bayes classifier. This time, because we have more than one independent variable/predictor, building the classifier and displaying the decision boundaries won’t be as straightforward as in the univariate LDA case (though the fundamental concept is the same). Also like in the univariate example, we will use a discriminant function to determine which class to assign to a given observation. The discriminant functions are exactly the same and are derived in the same way, except that since we have more than one independent variable we will use a vector of variables and instead of a scalar variance (\\(\\sigma\\)) we will use the common covariance matrix we constructed above (\\(\\Sigma\\)). We can easily build our class-wise discriminant functions. Since we also know the true parameters of the distributions we are sampling from we can create the optimal decision boundaries. First, we’ll construct vectors of our population means by class as well as the class-wise prior probabilities. In this example we know that the class-wise prior probabilities are equal, but we’ll calculate them each separately for thoroughness. pop_c0_mean_vec &lt;- c(pop_mean_c0_X1, pop_mean_c0_X2) pop_c1_mean_vec &lt;- c(pop_mean_c1_X1, pop_mean_c1_X2) pop_c2_mean_vec &lt;- c(pop_mean_c2_X1, pop_mean_c2_X2) pop_c0_prior &lt;- n / (n * 3) pop_c1_prior &lt;- n / (n * 3) pop_c2_prior &lt;- n / (n * 3) Then we’ll build our three discriminant functions and apply them inside the decision rule for classification described above. d0_bayes &lt;- function(x_vec){ t(x_vec) %*% solve(pop_sigma) %*% pop_c0_mean_vec - .5 * t(pop_c0_mean_vec) %*% solve(pop_sigma) %*% pop_c0_mean_vec + log(pop_c0_prior) } d1_bayes &lt;- function(x_vec){ t(x_vec) %*% solve(pop_sigma) %*% pop_c1_mean_vec - .5 * t(pop_c1_mean_vec) %*% solve(pop_sigma) %*% pop_c1_mean_vec + log(pop_c1_prior) } d2_bayes &lt;- function(x_vec){ t(x_vec) %*% solve(pop_sigma) %*% pop_c2_mean_vec - .5 * t(pop_c2_mean_vec) %*% solve(pop_sigma) %*% pop_c2_mean_vec + log(pop_c2_prior) } bayes_classifier &lt;- function(x_vec){ score_c0 &lt;- d0_bayes(x_vec) score_c1 &lt;- d1_bayes(x_vec) score_c2 &lt;- d2_bayes(x_vec) if (score_c0 &gt; score_c1 &amp; score_c0 &gt; score_c2) { 0 } else if (score_c1 &gt; score_c0 &amp; score_c1 &gt; score_c2) { 1 } else { 2 } } train_sample_df$bayes_predicted_y &lt;- apply(train_sample_df[, c(&quot;X1&quot;, &quot;X2&quot;)], 1, bayes_classifier) Can we plot the decision boundaries? Absolutely, but it’s a little more involved than in the univariate case. How do we draw these boundary lines? One obvious thing must be true: the decision boundary between two classes must pass through the midpoint between the centers of the two classes. What determines the direction of the line crossing through this point? The boundary line is orthogonal1 to \\(\\Sigma^{-1} (\\mu_k - \\mu_l)\\). To get a vector orthogonal to \\(\\Sigma^{-1} (\\mu_k - \\mu_l)\\) we can use MASS’s Null function. # Calculate midpoints between class-level population mean-vectors pop_c0_c1_midpoint &lt;- (pop_c0_mean_vec + pop_c1_mean_vec) / 2 pop_c0_c2_midpoint &lt;- (pop_c0_mean_vec + pop_c2_mean_vec) / 2 pop_c1_c2_midpoint &lt;- (pop_c1_mean_vec + pop_c2_mean_vec) / 2 # Generate orthogonal vectors pop_c0_c1_ortho &lt;- Null(solve(pop_sigma) %*% (pop_c0_mean_vec - pop_c1_mean_vec)) pop_c0_c2_ortho &lt;- Null(solve(pop_sigma) %*% (pop_c0_mean_vec - pop_c2_mean_vec)) pop_c1_c2_ortho &lt;- Null(solve(pop_sigma) %*% (pop_c1_mean_vec - pop_c2_mean_vec)) Then we can use these midpoints and orthogonal vectors to overlay the decision boundaries over the scatterplot we created above: train_sample_scatter &lt;- train_sample_scatter + geom_segment(aes(x = pop_c0_c1_midpoint[1] - 5 * pop_c0_c1_ortho[1], xend = pop_c0_c1_midpoint[1] + 5 * pop_c0_c1_ortho[1], y = pop_c0_c1_midpoint[2] - 5 * pop_c0_c1_ortho[2], yend = pop_c0_c1_midpoint[2] + 5 * pop_c0_c1_ortho[2]), linetype = &quot;solid&quot;) + geom_segment(aes(x = pop_c0_c2_midpoint[1] - 5 * pop_c0_c2_ortho[1], xend = pop_c0_c2_midpoint[1] + 5 * pop_c0_c2_ortho[1], y = pop_c0_c2_midpoint[2] - 5 * pop_c0_c2_ortho[2], yend = pop_c0_c2_midpoint[2] + 5 * pop_c0_c2_ortho[2]), linetype = &quot;solid&quot;) + geom_segment(aes(x = pop_c1_c2_midpoint[1] - 5 * pop_c1_c2_ortho[1], xend = pop_c1_c2_midpoint[1] + 5 * pop_c1_c2_ortho[1], y = pop_c1_c2_midpoint[2] - 5 * pop_c1_c2_ortho[2], yend = pop_c1_c2_midpoint[2] + 5 * pop_c1_c2_ortho[2]), linetype = &quot;solid&quot;) train_sample_scatter Obviously the segments represented above are just that: segments. The true lines extend infinitely in both directions. The LDA Classifier and Decision Boundaries Like before, first we’ll construct vectors of our sample means by class as well as the class-wise prior probabilities. Also like before, in this example we know that the class-wise prior probabilities are equal, but we’ll calculate estimates of each separately for thoroughness. Depending on your sample, your class samples obviously may not be same size. sample_c0_mean_vec &lt;- c(mean(train_sample_df$X1[train_sample_df$y == 0]), mean(train_sample_df$X2[train_sample_df$y == 0])) sample_c1_mean_vec &lt;- c(mean(train_sample_df$X1[train_sample_df$y == 1]), mean(train_sample_df$X2[train_sample_df$y == 1])) sample_c2_mean_vec &lt;- c(mean(train_sample_df$X1[train_sample_df$y == 2]), mean(train_sample_df$X2[train_sample_df$y == 2])) sample_c0_prior &lt;- nrow(train_sample_df[train_sample_df$y == 0,]) / nrow(train_sample_df) sample_c1_prior &lt;- nrow(train_sample_df[train_sample_df$y == 1,]) / nrow(train_sample_df) sample_c2_prior &lt;- nrow(train_sample_df[train_sample_df$y == 2,]) / nrow(train_sample_df) To build \\(\\hat{\\Sigma}\\) we will first de-mean the X1 and X2 vectors class-wise. We will do this explicitly and add2 these de-meaned versions of the X1 and X2 variables to the data tibble rather than de-mean the variables within a loop for conceptual simplicity. sample_mean_c0_X1 &lt;- mean(train_sample_df$X1[train_sample_df$y == 0]) sample_mean_c0_X2 &lt;- mean(train_sample_df$X2[train_sample_df$y == 0]) sample_mean_c1_X1 &lt;- mean(train_sample_df$X1[train_sample_df$y == 1]) sample_mean_c1_X2 &lt;- mean(train_sample_df$X2[train_sample_df$y == 1]) sample_mean_c2_X1 &lt;- mean(train_sample_df$X1[train_sample_df$y == 2]) sample_mean_c2_X2 &lt;- mean(train_sample_df$X2[train_sample_df$y == 2]) train_sample_df &lt;- train_sample_df %&gt;% mutate(centered_X1_sample = if_else(y == 0, X1 - sample_mean_c0_X1, if_else(y == 1, X1 - sample_mean_c1_X1, X1 - sample_mean_c2_X1)), centered_X2_sample = if_else(y == 0, X2 - sample_mean_c0_X2, if_else(y == 1, X2 - sample_mean_c1_X2, X2 - sample_mean_c2_X2))) Then we can build our estimated covariance matrix with a simple for loop and make the correction afterward: # Zero-matrix of correct dimensions to build over train_sigma_LDA &lt;- matrix(c(0, 0, 0, 0), 2, 2) for (i in 1:nrow(train_sample_df)) { temp_cov &lt;- c(train_sample_df$centered_X1_sample[i], train_sample_df$centered_X2_sample[i]) train_sigma_LDA &lt;- train_sigma_LDA + temp_cov %*% t(temp_cov) } # Correct the covariance matrix train_sigma_LDA &lt;- train_sigma_LDA / (n * 3 - 3) How close is our \\(\\hat{\\Sigma}\\) to \\(\\Sigma\\)? pop_sigma ## [,1] [,2] ## [1,] 3.6228982 0.8000072 ## [2,] 0.8000072 3.6228982 train_sigma_LDA ## [,1] [,2] ## [1,] 3.5945229 0.9970572 ## [2,] 0.9970572 3.5915687 Then, exactly like we did with the Bayes classifier, we build the LDA classifier by creating our discriminant functions and apply them via a decision rule function to our training sample: d0_LDA &lt;- function(x_vec){ t(x_vec) %*% solve(train_sigma_LDA) %*% sample_c0_mean_vec - .5 * t(sample_c0_mean_vec) %*% solve(train_sigma_LDA) %*% sample_c0_mean_vec + log(sample_c0_prior) } d1_LDA &lt;- function(x_vec){ t(x_vec) %*% solve(train_sigma_LDA) %*% sample_c1_mean_vec - .5 * t(sample_c1_mean_vec) %*% solve(train_sigma_LDA) %*% sample_c1_mean_vec + log(sample_c1_prior) } d2_LDA &lt;- function(x_vec){ t(x_vec) %*% solve(train_sigma_LDA) %*% sample_c2_mean_vec - .5 * t(sample_c2_mean_vec) %*% solve(train_sigma_LDA) %*% sample_c2_mean_vec + log(sample_c2_prior) } LDA_classifier &lt;- function(x_vec){ score_c0 &lt;- d0_LDA(x_vec) score_c1 &lt;- d1_LDA(x_vec) score_c2 &lt;- d2_LDA(x_vec) if (score_c0 &gt; score_c1 &amp; score_c0 &gt; score_c2) { 0 } else if (score_c1 &gt; score_c0 &amp; score_c1 &gt; score_c2) { 1 } else { 2 } } train_sample_df$LDA_predicted_y &lt;- apply(train_sample_df[, c(&quot;X1&quot;, &quot;X2&quot;)], 1, LDA_classifier) We can then overlay these lines on our existing scatterplot just like we did with the Bayes decision boundaries. Again, we will calculate pairwise class midpoints with the sample data and use Null to generate vectors orthogonal to \\(\\hat{\\Sigma} (\\hat{\\mu}_k - \\hat{\\mu}_l)\\). This time we’ll plot the LDA decision boundaries as dashed lines. # Calculate midpoints between class-level sample mean-vectors sample_c0_c1_midpoint &lt;- (sample_c0_mean_vec + sample_c1_mean_vec) / 2 sample_c0_c2_midpoint &lt;- (sample_c0_mean_vec + sample_c2_mean_vec) / 2 sample_c1_c2_midpoint &lt;- (sample_c1_mean_vec + sample_c2_mean_vec) / 2 # Generate the sample orthogonal vectors for plotting sample_c0_c1_ortho &lt;- Null(solve(train_sigma_LDA) %*% (sample_c0_mean_vec - sample_c1_mean_vec)) sample_c0_c2_ortho &lt;- Null(solve(train_sigma_LDA) %*% (sample_c0_mean_vec - sample_c2_mean_vec)) sample_c1_c2_ortho &lt;- Null(solve(train_sigma_LDA) %*% (sample_c1_mean_vec - sample_c2_mean_vec)) # Overlay scatterplot with LDA boundaries train_sample_scatter &lt;- train_sample_scatter + geom_segment(aes(x = sample_c0_c1_midpoint[1] - 5 * sample_c0_c1_ortho[1], xend = sample_c0_c1_midpoint[1] + 5 * sample_c0_c1_ortho[1], y = sample_c0_c1_midpoint[2] - 5 * sample_c0_c1_ortho[2], yend = sample_c0_c1_midpoint[2] + 5 * sample_c0_c1_ortho[2]), linetype = &quot;dashed&quot;) + geom_segment(aes(x = sample_c0_c2_midpoint[1] - 5 * sample_c0_c2_ortho[1], xend = sample_c0_c2_midpoint[1] + 5 * sample_c0_c2_ortho[1], y = sample_c0_c2_midpoint[2] - 5 * sample_c0_c2_ortho[2], yend = sample_c0_c2_midpoint[2] + 5 * sample_c0_c2_ortho[2]), linetype = &quot;dashed&quot;) + geom_segment(aes(x = sample_c1_c2_midpoint[1] - 5 * sample_c1_c2_ortho[1], xend = sample_c1_c2_midpoint[1] + 5 * sample_c1_c2_ortho[1], y = sample_c1_c2_midpoint[2] - 5 * sample_c1_c2_ortho[2], yend = sample_c1_c2_midpoint[2] + 5 * sample_c1_c2_ortho[2]), linetype = &quot;dashed&quot;) + labs(caption = &quot;Bayes classifier boundaries are solid; LDA boundaries are dashed&quot;) train_sample_scatter Testing Now that we have functions built for the Bayes classifier and LDA classifier, we can easily apply them to the test data just as we did with the training data. test_sample_df$bayes_predicted_y &lt;- apply(test_sample_df[, c(&quot;X1&quot;, &quot;X2&quot;)], 1, bayes_classifier) test_sample_df$LDA_predicted_y &lt;- apply(test_sample_df[, c(&quot;X1&quot;, &quot;X2&quot;)], 1, LDA_classifier) What does the misclassification rate for LDA look like? LDA_misclass_rate &lt;- nrow(test_sample_df[test_sample_df$y != test_sample_df$LDA_predicted_y,]) / nrow(test_sample_df) LDA_misclass_rate * 100 ## [1] 6.444444 How about the misclassification rate for our (optimal) Bayes classifier? bayes_misclass_rate &lt;- nrow(test_sample_df[test_sample_df$y != test_sample_df$bayes_predicted_y,]) / nrow(test_sample_df) bayes_misclass_rate * 100 ## [1] 6 It’s of course possible for LDA to “outperform” the optimal Bayes classifier depending on the train and test samples, but the Bayes classifier here represents the “true” optimal classifier in a sense since its classifying based on population parameters. See this excellent CV answer for explanations why.↩︎ See mutate.↩︎ "],["quadratic-discriminant-analysis.html", "Quadratic Discriminant Analysis", " Quadratic Discriminant Analysis This example walks through using quadratic discriminant analysis to classify observations in a three-class multivariate setting with (idealized) generated data. Covering the multivariate LDA example first is recommended. Unlike linear discriminant analysis, QDA allows for non-linear decision boundaries, and QDA does not assume that the class distribution covariance matrices are identical. Theory Deriving the pairwise discriminant functions in the QDA case where we don’t assume that the class covariance matrices are identical is the exact same process as in the multivariate LDA case: we simply end up with slightly uglier discriminant functions since we don’t assume identical covariance matrices. In fact, the derivation is even easier than in multivariate LDA and is really just an exercise in remembering log rules. Bayes’ Theorem Just like in the multivariate LDA example, we start by taking Bayes’ theorem for continuous variables and plugging in the multivariate normal PDF and then taking the log ratio of conditional probabilities. \\[\\begin{align} \\log\\frac{\\Pr(Y = k|X = x)}{\\Pr(Y = l|X = x)} &amp;= \\log \\frac{\\pi_k f_k(x)}{\\pi_l f_l(x)} \\\\ &amp;= \\log{\\frac{\\pi_k}{\\pi_l}} + \\log{\\frac{f_k(x)}{f_l(x)}} \\\\ &amp;= \\log{\\frac{\\pi_k}{\\pi_l}} + \\log{\\frac{\\frac{1}{(2\\color{red}{\\pi})^{\\frac{p}{2}} \\det(\\Sigma_k)^\\frac{1}{2}} e^{\\frac{-1}{2} (x - \\mu_k)^T \\Sigma_k^{-1} (x - \\mu_k)}}{\\frac{1}{(2\\color{red}{\\pi})^{\\frac{p}{2}} \\det(\\Sigma_l)^\\frac{1}{2}} e^{\\frac{-1}{2} (x - \\mu_l)^T \\Sigma_l^{-1} (x - \\mu_l)}}} \\end{align}\\] Just as in the univariate and multivariate LDA examples, \\(\\color{red}{\\pi}\\) in the PDF is the literal value pi, not a prior probability. \\(p\\) is the number of variables. Remembering our log rules, we can easily break this down: \\[\\scriptsize \\begin{align} \\log\\frac{\\Pr(Y = k|X = x)}{\\Pr(Y = l|X = x)} &amp;= \\log{\\frac{\\pi_k}{\\pi_l}} + \\log{\\frac{\\frac{1}{(2\\color{red}{\\pi})^{\\frac{p}{2}} \\det(\\Sigma_k)^\\frac{1}{2}} e^{\\frac{-1}{2} (x - \\mu_k)^T \\Sigma_k^{-1} (x - \\mu_k)}}{\\frac{1}{(2\\color{red}{\\pi})^{\\frac{p}{2}} \\det(\\Sigma_l)^\\frac{1}{2}} e^{\\frac{-1}{2} (x - \\mu_l)^T \\Sigma_l^{-1} (x - \\mu_l)}}} \\\\ &amp;= \\log{\\frac{\\pi_k}{\\pi_l}} + \\log{\\frac{\\frac{1}{(2\\color{red}{\\pi})^{\\frac{p}{2}} \\det(\\Sigma_k)^\\frac{1}{2}}}{\\frac{1}{(2\\color{red}{\\pi})^{\\frac{p}{2}} \\det(\\Sigma_l)^\\frac{1}{2}}}} + \\log{\\frac{e^{\\frac{-1}{2} (x - \\mu_k)^T \\Sigma_k^{-1} (x - \\mu_k)}}{e^{\\frac{-1}{2} (x - \\mu_l)^T \\Sigma_l^{-1} (x - \\mu_l)}}} \\\\ &amp;= \\log{\\frac{\\pi_k}{\\pi_l}} + \\log{1} - \\log{2\\color{red}{\\pi}^\\frac{p}{2} \\det(\\Sigma_k)^\\frac{1}{2}} - \\log{1} + \\log{2\\color{red}{\\pi}^\\frac{p}{2} \\det(\\Sigma_l)^\\frac{1}{2}} + \\log{e^{\\frac{-1}{2} (x - \\mu_k)^T \\Sigma_l^{-1} (x - \\mu_k)}} - \\log{e^{\\frac{-1}{2} (x - \\mu_l)^T \\Sigma_l^{-1} (x - \\mu_l)}} \\\\ &amp;= \\log{\\frac{\\pi_k}{\\pi_l}} - \\log{2\\color{red}{\\pi}^\\frac{p}{2}} + \\log{\\det(\\Sigma_k)^\\frac{1}{2}} + \\log{2\\color{red}{\\pi}^\\frac{p}{2}} + \\log{\\det(\\Sigma_l)^\\frac{1}{2}} - \\frac{1}{2} (x - \\mu_k)^T \\Sigma_l^{-1} (x - \\mu_k) + \\frac{1}{2} (x - \\mu_l)^T \\Sigma_l^{-1} (x - \\mu_l) \\\\ &amp;= \\log{\\pi_k} - \\log{\\pi_l} - \\frac{1}{2}\\log{\\det(\\Sigma_k)} + \\frac{1}{2}\\log{\\det(\\Sigma_l)} - \\frac{1}{2} (x - \\mu_k)^T \\Sigma_l^{-1} (x - \\mu_k) + \\frac{1}{2} (x - \\mu_l)^T \\Sigma_l^{-1} (x - \\mu_l) \\\\ \\end{align}\\] Discriminant Functions Then, like in the multivariate LDA case, all that’s left to do is to evaluate this expression at the decision boundary (where \\(\\frac{\\Pr(Y = k|X = x)}{\\Pr(Y = l|X = x)} = 1\\) and so \\(\\log\\frac{\\Pr(Y = k|X = x)}{\\Pr(Y = l|X = x)} = 0\\)) and then reorganize the expression to find our discriminant functions. \\[\\scriptsize \\begin{align} 0 &amp;= \\log{\\pi_k} - \\log{\\pi_l} - \\frac{1}{2}\\log{\\det(\\Sigma_k)} + \\frac{1}{2}\\log{\\det(\\Sigma_l)} - \\frac{1}{2} (x - \\mu_k)^T \\Sigma_l^{-1} (x - \\mu_k) + \\frac{1}{2} (x - \\mu_l)^T \\Sigma_l^{-1} (x - \\mu_l) \\\\ \\underbrace{\\log{\\pi_l} - \\frac{1}{2}\\log{\\det(\\Sigma_l)} - \\frac{1}{2} (x - \\mu_l)^T \\Sigma_l^{-1} (x - \\mu_l)}_{\\delta_l(x)} &amp;= \\underbrace{\\log{\\pi_k} - \\frac{1}{2}\\log{\\det(\\Sigma_k)} - \\frac{1}{2} (x - \\mu_k)^T \\Sigma_k^{-1} (x - \\mu_k)}_{\\delta_k(x)} \\\\ \\end{align}\\] Just like in the univariate and multivariate LDA cases, for a given observation the Bayes classifier assigns that observation to a class based on which class’s discriminant function evaluates largest. "],["data-generation-3.html", "Data Generation", " Data Generation This example uses the tidyverse and MASS packages. library(tidyverse) library(MASS) We’ll generate data with three possible outcome classes (coded as 0, 1, and 2) with two independent variables/predictors. We’ll use MASS’s mvnorm function to sample from a multivariate normal distribution. Throughout this example I’ll refer to the first variable as X1 and the second variable as X2. For each class we’ll construct a vector of means for X1 and X2, randomly sampled integers between 1 and 10: X1_means &lt;- sample.int(10, 3, replace = FALSE) X2_means &lt;- sample.int(10, 3, replace = FALSE) pop_mean_c0_X1 &lt;- X1_means[1] pop_mean_c1_X1 &lt;- X1_means[2] pop_mean_c2_X1 &lt;- X1_means[3] pop_mean_c0_X2 &lt;- X2_means[1] pop_mean_c1_X2 &lt;- X2_means[2] pop_mean_c2_X2 &lt;- X2_means[3] mu_c0 &lt;- c(pop_mean_c0_X1, pop_mean_c0_X2) mu_c1 &lt;- c(pop_mean_c1_X1, pop_mean_c1_X2) mu_c2 &lt;- c(pop_mean_c2_X1, pop_mean_c2_X2) As noted above, QDA does not assume that the class covariance matrices are identical so instead of creating one randomly generated covariance matrix we will create one for each outcome class. For this example, we’ll randomly choose class-level correlations and class-level variances (between 1 and 10). pop_c0_corr &lt;- runif(1, 0, 1) pop_c1_corr &lt;- runif(1, 0, 1) pop_c2_corr &lt;- runif(1, 0, 1) pop_var_c0 &lt;- runif(1, 1, 10) pop_var_c1 &lt;- runif(1, 1, 10) pop_var_c2 &lt;- runif(1, 1, 10) Just like in the multivariate LDA example we’ll generate 300 sample observations for each class, once for a training set and once for a test set. We could use the same process as in the multivariate LDA example, but we can be more efficient. Using a loop with the get and assign functions, we can variably refer to our class-level correlations and variances as well as variably generate our class covariance matrices and class sample data sets. n &lt;- 300 for (i in 0:2) { class &lt;- paste(&quot;c&quot;, i, sep = &quot;&quot;) class_var &lt;- paste(&quot;pop_var&quot;, class, sep = &quot;_&quot;) class_corr &lt;- paste(&quot;pop&quot;, class, &quot;corr&quot;, sep = &quot;_&quot;) mu = paste(&quot;mu&quot;, class, sep = &quot;_&quot;) temp_sigma &lt;- matrix(c(get(class_var), get(class_corr), get(class_corr), get(class_var)), 2, 2) temp_train_data &lt;- mvrnorm(n = n, mu = get(mu), Sigma = temp_sigma) temp_test_data &lt;- mvrnorm(n = n, mu = get(mu), Sigma = temp_sigma) assign(paste(class, &quot;sigma&quot;, sep = &quot;_&quot;), temp_sigma) assign(paste(class, &quot;train&quot;, sep = &quot;_&quot;), temp_train_data) assign(paste(class, &quot;test&quot;, sep = &quot;_&quot;), temp_test_data) } Now all that’s left is to bind the class training and test data together. train_sample_df &lt;- bind_rows( tibble(y = 0, X1 = c0_train[,1], X2 = c0_train[,2]), tibble(y = 1, X1 = c1_train[,1], X2 = c1_train[,2]), tibble(y = 2, X1 = c2_train[,1], X2 = c2_train[,2]) ) test_sample_df &lt;- bind_rows( tibble(y = 0, X1 = c0_test[,1], X2 = c0_test[,2]), tibble(y = 1, X1 = c1_test[,1], X2 = c1_test[,2]), tibble(y = 2, X1 = c2_test[,1], X2 = c2_test[,2]) ) Generally, our data here will look a lot like the data in the multivariate LDA example: three clouds of data points, one for each class, except that in this case the clouds may not be similarly-shaped since each comes from distributions with distinct covariance matrices. We can see this with a simple scatterplot of the training data. train_sample_scatter &lt;- ggplot(train_sample_df) + geom_point(aes(x = X1, y = X2, color = as.factor(y))) + scale_color_brewer(palette = &quot;Dark2&quot;, name = &quot;class&quot;) + labs(title = &quot;Bivariate Training Sample of Three Classes: Scatterplot&quot;) train_sample_scatter "],["implementation-3.html", "Implementation", " Implementation The Bayes Classifier and Decision Boundaries Creating the discriminant functions, classifying with the Bayes classifier, and classifying observations in our training set using the population parameters we know is straightforward in. This will mirror what we did in the multivariate LDA example but with our new discriminant functions. First, we’ll construct vectors of our population means by class as well as the class-wise prior probabilities. In this example we know that the class-wise prior probabilities are equal, but we’ll calculate them each separately for thoroughness. pop_c0_mean_vec &lt;- c(pop_mean_c0_X1, pop_mean_c0_X2) pop_c1_mean_vec &lt;- c(pop_mean_c1_X1, pop_mean_c1_X2) pop_c2_mean_vec &lt;- c(pop_mean_c2_X1, pop_mean_c2_X2) c0_prior &lt;- n / (n * 3) c1_prior &lt;- n / (n * 3) c2_prior &lt;- n / (n * 3) Then we’ll build our three discriminant functions and apply them inside the decision rule for classification described above. d0_bayes &lt;- function(x_vec){ -.5 * log(norm(c0_sigma, type = &quot;2&quot;)) - .5 * t(x_vec - mu_c0) %*% solve(c0_sigma) %*% (x_vec - mu_c0) + log(c0_prior) } d1_bayes &lt;- function(x_vec){ -.5 * log(norm(c1_sigma, type = &quot;2&quot;)) - .5 * t(x_vec - mu_c1) %*% solve(c1_sigma) %*% (x_vec - mu_c1) + log(c1_prior) } d2_bayes &lt;- function(x_vec){ -.5 * log(norm(c2_sigma, type = &quot;2&quot;)) - .5 * t(x_vec - mu_c2) %*% solve(c2_sigma) %*% (x_vec - mu_c2) + log(c2_prior) } bayes_classifier &lt;- function(x_vec){ score_c0 &lt;- d0_bayes(x_vec) score_c1 &lt;- d1_bayes(x_vec) score_c2 &lt;- d2_bayes(x_vec) if (score_c0 &gt; score_c1 &amp; score_c0 &gt; score_c2) { 0 } else if (score_c1 &gt; score_c0 &amp; score_c1 &gt; score_c2) { 1 } else { 2 } } train_sample_df$bayes_predicted_y &lt;- apply(train_sample_df[, c(&quot;X1&quot;, &quot;X2&quot;)], 1, bayes_classifier) The QDA Classifier and Decision Boundaries Just like in the multivariate LDA case, the only difference between the Bayes classifier above and the QDA classifier is that we almost surely can’t know the population covariance matrices and means. To move forward, we need to estimate them. As in the LDA examples, we use the empirical class means as our mean estimates. Unlike the multivariate LDA case where we assume identical variance matrices between classes, we do not need to estimate our covariance matrix in a weighted fashion across classes: we can simply use R’s `cov’ function. First we’ll estimate our means and covariance matrices by class. sample_c0_mean_vec &lt;- c(mean(train_sample_df$X1[train_sample_df$y == 0]), mean(train_sample_df$X2[train_sample_df$y == 0])) sample_c1_mean_vec &lt;- c(mean(train_sample_df$X1[train_sample_df$y == 1]), mean(train_sample_df$X2[train_sample_df$y == 1])) sample_c2_mean_vec &lt;- c(mean(train_sample_df$X1[train_sample_df$y == 2]), mean(train_sample_df$X2[train_sample_df$y == 2])) sample_cov_mat_c0 &lt;- cov(train_sample_df %&gt;% filter(y == 0) %&gt;% dplyr::select(X1, X2)) sample_cov_mat_c1 &lt;- cov(train_sample_df %&gt;% filter(y == 1) %&gt;% dplyr::select(X1, X2)) sample_cov_mat_c2 &lt;- cov(train_sample_df %&gt;% filter(y == 2) %&gt;% dplyr::select(X1, X2)) The only things left to calculate are the classwise prior probabilities. Again, we know that these are equal because of our generated samples so we don’t really need to calculate them separately, but we will anyway for thoroughness. c0_prior &lt;- nrow(train_sample_df[train_sample_df$y == 0,]) / nrow(train_sample_df) c1_prior &lt;- nrow(train_sample_df[train_sample_df$y == 1,]) / nrow(train_sample_df) c2_prior &lt;- nrow(train_sample_df[train_sample_df$y == 2,]) / nrow(train_sample_df) Then, just like with the Bayes classifier, we create our discriminant functions and apply them to the data via a classifier function. d0_QDA &lt;- function(x_vec){ -.5 * log(det(sample_cov_mat_c0)) - .5 * t(x_vec - sample_c0_mean_vec) %*% solve(sample_cov_mat_c0) %*% (x_vec - sample_c0_mean_vec) + log(c0_prior) } d1_QDA &lt;- function(x_vec){ -.5 * log(det(sample_cov_mat_c1)) - .5 * t(x_vec - sample_c1_mean_vec) %*% solve(sample_cov_mat_c1) %*% (x_vec - sample_c1_mean_vec) + log(c1_prior) } d2_QDA &lt;- function(x_vec){ -.5 * log(det(sample_cov_mat_c2)) - .5 * t(x_vec - sample_c2_mean_vec) %*% solve(sample_cov_mat_c2) %*% (x_vec - sample_c2_mean_vec) + log(c2_prior) } QDA_classifier &lt;- function(x_vec){ score_c0 &lt;- d0_QDA(x_vec) score_c1 &lt;- d1_QDA(x_vec) score_c2 &lt;- d2_QDA(x_vec) if (score_c0 &gt; score_c1 &amp; score_c0 &gt; score_c2) { 0 } else if (score_c1 &gt; score_c0 &amp; score_c1 &gt; score_c2) { 1 } else { 2 } } train_sample_df$QDA_predicted_y &lt;- apply(train_sample_df[, c(&quot;X1&quot;, &quot;X2&quot;)], 1, QDA_classifier) Visualizing the Decision Boundaries The nice non-linear decision boundaries shown in Elements of Statistical Learning were created through an extensive contouring process, which is way outside the scope of this working example.3 While we won’t plot the decision boundaries directly, we can pretty easily plot the decision areas using an approach similar to that we use for plotting a maximum likelihood surface in the logistic regression example. This is a hacky approach in this example in a way that it’s not in the logistic regression example, but if it works it should do the trick well enough. The basic idea is that we should be able to apply the QDA classifier to a set of Cartesian coordinates for our X1 and X2 variables and then show the approximate areas in which the QDA classifier would choose one class or another. This obviously gives us the approximate decision boundaries between classes as well. First, we can create our “surface” of coordinates by creating a tibble of every combination of X1 and X2 values within some space. We’ll restrict our space to the smallest and largest train sample values for X1 and X2. We obviously can’t classify every point in this space, so we’ll use seq to create a tibble of coordinates from approximately the smallest and largest values of our two variables in some increment (in this case we’ll use .05). X1_vec &lt;- seq(floor(min(train_sample_df$X1)), ceiling(max(train_sample_df$X1)), .05) X2_vec &lt;- seq(floor(min(train_sample_df$X2)), ceiling(max(train_sample_df$X2)), .05) surface &lt;- expand_grid(X1_vec, X2_vec) colnames(surface) &lt;- c(&quot;X1&quot;, &quot;X2&quot;) We’ve already created our LDA classifier, so we can simply apply it to our surface tibble of coordinates. surface$class &lt;- apply(surface[, c(&quot;X1&quot;, &quot;X2&quot;)], 1, QDA_classifier) Rather than geom_contour_filled like in the logistic example, we use geom_tile because our decision areas aren’t parts of a 3D surface and the classifications aren’t part of a continuous scale. ggplot2 layers plots based on what order they are expressed in the ggplot expression, so instead of adding the tile layer to our existing scatterplot (which would layer the tiles on top of the scatter points) we will rewrite the geom_point expression that provides the scatterplot after the geom_tile expression (which layers the scatterplot on top of the background of the classification areas). QDA_decision_areas &lt;- ggplot() + geom_tile(data = surface, aes(x = X1, y = X2, fill = as.factor(class)), alpha = .25) + scale_fill_brewer(palette = &quot;Dark2&quot;, name = &quot;QDA Classification Area&quot;) + geom_point(data = train_sample_df, aes(x = X1, y = X2, color = as.factor(y))) + scale_color_brewer(palette = &quot;Dark2&quot;, name = &quot;class&quot;) + labs(title = &quot;QDA: Train Scatterplot with Classification Areas&quot;) QDA_decision_areas This is clearly a longer way of saying “I don’t know how to do it and don’t want to learn it for this.”↩︎ "],["testing-3.html", "Testing", " Testing Using the Bayes and QDA classifiers we’ve built, we can easily apply both to the test data just like we did with the training data. test_sample_df$bayes_predicted_y &lt;- apply(test_sample_df[, c(&quot;X1&quot;, &quot;X2&quot;)], 1, bayes_classifier) test_sample_df$QDA_predicted_y &lt;- apply(test_sample_df[, c(&quot;X1&quot;, &quot;X2&quot;)], 1, QDA_classifier) What does the misclassification rate for QDA look like? QDA_misclass_rate &lt;- nrow(test_sample_df[test_sample_df$y != test_sample_df$QDA_predicted_y,]) / nrow(test_sample_df) QDA_misclass_rate * 100 ## [1] 19.22222 And what does the misclassification rate for our (optimal) Bayes classifier look like? bayes_misclass_rate &lt;- nrow(test_sample_df[test_sample_df$y != test_sample_df$bayes_predicted_y,]) / nrow(test_sample_df) bayes_misclass_rate * 100 ## [1] 19.33333 Visualization We can easily visualize how our trained QDA decision decision areas map to our test data. Using the tile-and-scatterplot we created above, all we need to do is simply change our train data to our test data for the scatterplot layer. QDA_decision_areas_test &lt;- ggplot() + geom_tile(data = surface, aes(x = X1, y = X2, fill = as.factor(class)), alpha = .25) + scale_fill_brewer(palette = &quot;Dark2&quot;, name = &quot;QDA Classification Area&quot;) + geom_point(data = test_sample_df, aes(x = X1, y = X2, color = as.factor(y))) + scale_color_brewer(palette = &quot;Dark2&quot;, name = &quot;class&quot;) + labs(title = &quot;QDA: Test Scatterplot with Classification Areas&quot;) QDA_decision_areas_test "],["ridge-regression.html", "Ridge Regression", " Ridge Regression This example walks through fitting a ridge regression model to idealized data. It assumes some prior knowledge of ordinary least squares and linear algebra. Ridge regression is, functionally, nearly the same as OLS except that we include a constraint on the \\(\\hat{\\beta}\\) vector in fitting the model. Constraining a model in this way is broadly called regularization and makes the model less complex by either eliminating or penalizing low-contribution variables. In ridge regression, the particular form of regularization/constraint we use acts as a sum-of-squares (or a L2) penalty and shrinks rather than eliminates variables. Ridge regression may be preferable to OLS if there are a lot of independent variables/predictors to choose from and some of those predictors are highly correlated with one another. Ridge regression may have higher prediction accuracy than OLS due to OLS overfitting the training sample. It can be easily shown that OLS estimates, under certain assumptions, are unbiased and have the lowest possible variance of unbiased estimators. However, we can oftentimes get an even lower variance of the fitted values by allowing our estimates to be biased. This is usually not desirable if the goal of the model is inference, but we likely don’t care as much if the goal is prediction. This can help improve model accuracy. Ridge regression also performs a kind of variable selection by more severely penalizing estimates of variables that contribute less to the overall variance of the fitted values. So long as the penalty term in ridge regression is greater than zero it will shrink all regression coefficients toward zero but it will shrink those with larger projected sample variance less. Ridge regression cannot set coefficients to exactly zero, so it doesn’t actually perform full variable selection. "],["theory-3.html", "Theory", " Theory Just like in OLS, the intent is to minimize residual sum of squares. \\[RSS = \\sum_{i=1}^N (y_i - \\hat{y}_i)^2\\] Minimizing RSS: OLS Before jumping into ridge regression, let’s review deriving the OLS solution via minimizing RSS. Where \\(X\\) is our design matrix: \\[\\vec{\\beta} = \\{ \\beta_0, \\beta_1, \\ldots, \\beta_k \\}\\] \\[\\begin{align*} RSS(\\vec{\\beta})_{OLS} &amp;= (\\vec{y} - X \\vec{\\beta})^T(\\vec{y} - X \\vec{\\beta}) \\\\ &amp;= \\vec{y}^T \\vec{y} - \\underbrace{\\vec{y}^T X \\vec{\\beta}}_{\\vec{\\beta}^T X^T \\vec{y}} - \\vec{\\beta}^T X^T \\vec{y} + \\vec{\\beta}^T X^T X \\vec{\\beta} \\\\ &amp;= \\vec{y}^T \\vec{y} - 2 \\vec{\\beta}^T X^T \\vec{y} + \\vec{\\beta}^T X^T X \\vec{\\beta} \\end{align*}\\] Then minimizing4 \\(RSS(\\vec{\\beta})\\) with respect to \\(\\vec{\\beta}\\) \\[\\begin{align*} \\frac{\\partial RSS(\\vec{\\beta})_{OLS}}{\\partial \\vec{\\beta}} = -2 X^T \\vec{y} + 2 X^T X \\vec{\\beta} &amp;= 0 \\\\ -2 (X^T \\vec{y} - X^T X \\vec{\\beta}) &amp;= 0 \\\\ X^T \\vec{y} - X^T X \\vec{\\beta} &amp;= 0 \\\\ X^T X \\vec{\\beta} &amp;= X^T \\vec{y} \\\\ \\hat{\\vec{\\beta}}^{OLS} &amp;= (X^T X)^{-1} X^T \\vec{y} \\end{align*}\\] we get the normal equation along the way and arrive at the usual solution. Minimizing RSS: Ridge The ridge minimization of RSS works the same way, except we include the penalty term (\\(\\lambda\\)) on the squared values of our estimates. However, the penalty does not apply to the intercept term, so we will use a matrix of just our variables without a leading one-vector for the intercept term (which I will call the input matrix). It’s easy to conceptualize how the ridge regression solutions for the non-intercept terms would depend on the intercept if we also penalized it. If the column vectors of \\(X\\) are mean-centered then \\(\\beta_0 = \\bar{y}\\). Then we minimize RSS with the penalty term as if there were no intercept. \\[\\vec{\\beta} = \\{ \\beta_1, \\beta_2 \\ldots, \\beta_k \\}\\] \\[\\begin{align*} RSS(\\vec{\\beta})_{Ridge} &amp;= (\\vec{y} - X \\vec{\\beta})^T(\\vec{y} - X \\vec{\\beta}) + \\lambda \\vec{\\beta}^T \\vec{\\beta} \\\\ &amp;= \\vec{y}^T \\vec{y} - \\underbrace{\\vec{y}^T X \\vec{\\beta}}_{\\vec{\\beta}^T X^T \\vec{y}} - \\vec{\\beta}^T X^T \\vec{y} + \\vec{\\beta}^T X^T X \\vec{\\beta} + \\lambda \\vec{\\beta}^T \\vec{\\beta} \\\\ &amp;= \\vec{y}^T \\vec{y} - 2 \\vec{\\beta}^T X^T \\vec{y} + \\vec{\\beta}^T X^T X \\vec{\\beta} + \\lambda \\vec{\\beta}^T \\vec{\\beta} \\end{align*}\\] Then minimize with respect to \\(\\vec{\\beta}\\): \\[\\begin{align*} \\frac{\\partial RSS(\\vec{\\beta})_{Ridge}}{\\partial \\vec{\\beta}} = -2 X^T \\vec{y} + 2 X^T X \\vec{\\beta} + 2 \\lambda \\vec{\\beta} &amp;= 0 \\\\ -2 (X^T \\vec{y} - X^T X \\vec{\\beta} - \\lambda \\vec{\\beta}) &amp;= 0 \\\\ X^T \\vec{y} - X^T X \\vec{\\beta} - \\lambda \\vec{\\beta} &amp;= 0 \\\\ X^T \\vec{y} &amp;= X^T X \\beta + \\lambda \\vec{\\beta} \\\\ X^T \\vec{y} &amp;= \\vec{\\beta} (X^T X + \\lambda I) \\\\ \\hat{\\vec{\\beta}}^{Ridge} &amp;= (X^T X + \\lambda I)^{-1} X^T \\vec{y} \\end{align*}\\] Notice that for \\(\\lambda = 0\\) our minimization is the same as with OLS, so a ridge regression fit with \\(\\lambda = 0\\) is just an OLS fit. In calculating MSE we will need to add \\(\\beta_0 = \\bar{y}\\) in so long as we’re concerned about being off by a constant. Important Features This section is not strictly necessary and may be skipped, but know that the ridge regression problem will never not have a solution because of rank-deficiency or linearly dependent variables and ridge regression performs a type of variable selection by shrinking variables that contribute less to the variance of fitted values more.5 Solvability Because we add a positive constant, \\(\\lambda I\\), to the diagonal of \\(X^T X\\) prior to inverting that matrix the resulting adjusted matrix will always be nonsingular. So there will always be a solution, even if \\(X^T X\\) is rank-deficient. This is not the case with OLS: ridge regression can provide solutions to least squares-like problems where OLS cannot.6 This may be useful if two or more variables are linearly dependent or are very highly correlated or if there are more variables than observations. (Sort Of) Variable Selection It’s also important to note that ridge regression shrinks all estimates toward zero, but it does not shrink them equally.7 Ridge regression shrinks the variables that contribute more variation to the fitted (or predicted) values less than those variables that don’t: in this way, it performs a kind of variable selection. To demonstrate this, we can take the singular value decomposition of the mean-centered input matrix.8 \\[X = U D V^T \\] Like with all SVDs, the \\(D\\) matrix is diagonal and contains the singular values (which are associated with principal components) of \\(X\\) along its diagonal in descending order. We can take this decomposition of \\(X\\) and plug it into our hat matrix formula \\[\\begin{align} X \\hat{\\beta}^{Ridge} &amp;= X (X^T X + \\lambda I)^{-1} X^T \\vec{y} \\\\ &amp;= U D V^T (V D^T U^T U D V^T + \\lambda I)^{-1} V D^T U^T \\vec{y} \\\\ &amp;= U D V^T (V D^T D V^T + \\lambda I)^{-1} V D^T U^T \\vec{y} &amp;&amp; U^T U = I \\\\ &amp;= UD (D^T D + \\lambda I)^{-1}) D^T U^T \\vec{y} &amp;&amp; V^T V^{-1} (V^T)^{-1} V = I \\\\ &amp;= UD (D^2 + \\lambda I)^{-1} D U^T \\vec{y} &amp;&amp; D^T D = D^2 \\text{, } D^T = D \\end{align}\\] where \\(U^T U = I\\) because \\(U\\) is symmetric, \\(V^T V^{-1} (V^T)^{-1} V = I\\) because \\(V\\) is unitary, and \\(D^T = D\\) because \\(D\\) is diagonal and therefore also symmetric. If we focus on the above expression in terms of individual vectors of \\(U\\) and individual diagonal elements of \\(D\\) it’s easy to see that \\[X \\hat{\\beta}^{Ridge} = \\sum_{k = 1}^K \\vec{u}_k \\frac{\\sigma_k^2}{\\sigma_k^2 + \\lambda} \\vec{u}_k^T \\vec{y}\\] where \\(K\\) is the number of variables, \\(u_k\\) is the \\(k\\)th column of \\(U\\) (or the \\(k\\)th left singular vector), and \\(\\sigma_k\\) is the \\(k\\)th singular value. A hat matrix is simply a projection of \\(X\\) onto the column space of \\(\\vec{y}\\), so it is clear to see here that the fraction is the penalty in that projection. It’s also clear to see that, because the singular values are decreasing along the diagonal of \\(D\\) (i.e., \\(\\sigma_1 &gt; \\sigma_2, \\ldots, &gt; \\sigma_K\\)), this penalty grows larger for basis vectors associated with smaller singular values: i.e., ridge regression more heavily shrinks the estimated “effect” of variables with smaller sample variance. Recall that \\(\\frac{\\partial a^T b a}{\\partial a} = 2 a x\\) and \\(\\frac{\\partial a^T b}{\\partial a} = a\\).↩︎ It can be easy to think that this means the “less important” variables which don’t “explain” our outcome are being shrunk more, but the reason why these variables are shrunk actually has nothing to do with the outcome. Remember that we’re going for prediction here, not inference. So while it might not be true that the variables with higher variance “explain” our outcome, we don’t really care since the goal isn’t causally explaining the outcome. Elements of Statistical Learning explains this well: “The implicit assumption is that the response will tend to vary most in the directions of high variance of the inputs. This is often a reasonable assumption, since predictors are often chosen for study because they vary with the response variable, but need not hold in general.”↩︎ But, unlike OLS, the estimates will not be BLUE.↩︎ It would be pretty useless if it did shrink all variables equally!↩︎ Remember, this is just our matrix of variable values, not including the one-vector that would be in the design matrix for the intercept.↩︎ "],["case-1-linearly-dependent-variables.html", "Case 1: Linearly Dependent Variables", " Case 1: Linearly Dependent Variables First we’ll explore a simple case demonstrating how ridge regression can solve problems with linearly dependent variables when OLS can’t. This is just a particular case of rank-deficiency, the other one that can cause issues with OLS being having more variables (columns) than observations (rows) in your design matrix. Ridge regression can handle both cases. Data Generation For the cases in this example, load the tidyverse, MASS, latex2exp, ggcorrplot, and RColorBrewer packages. library(tidyverse) library(MASS) library(latex2exp) library(ggcorrplot) library(RColorBrewer) We’ll create one normally-distributed variable and then a second variable that is simply a multiple of the first to make the two variables linearly dependent. Let’s generate one variable with 1,000 observations, a mean between -10 and 10, and a standard deviation between 1 and 10. X1 &lt;- rnorm(1000, mean = runif(1, -10, 10), sd = runif(1, 1, 10)) We’ll make X1 linearly dependent with a new variable, X2, by making X2 a variable that is simply X1 divided by two. X2 &lt;- X1 / 2 Next we’ll write a standardization function9 and standardize both of our variables. standardize &lt;- function(vec){ sd &lt;- sqrt(sum((vec - mean(vec))^2) / length(vec)) (vec - mean(vec)) / sd } X1_std &lt;- standardize(X1) X2_std &lt;- standardize(X2) Lastly we’ll create a normally-distributed outcome variable. y &lt;- rnorm(1000, mean = runif(1, -10, 10), sd = runif(1, 1, 10)) The Problem: OLS Let’s run a simple linear regression and see what happens: summary(lm(y ~ X1_std + X2_std)) ## ## Call: ## lm(formula = y ~ X1_std + X2_std) ## ## Residuals: ## Min 1Q Median 3Q Max ## -26.8522 -4.9137 -0.2605 4.9228 23.3034 ## ## Coefficients: (1 not defined because of singularities) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.3193 0.2367 9.797 &lt;2e-16 *** ## X1_std -0.1831 0.2367 -0.773 0.44 ## X2_std NA NA NA NA ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.486 on 998 degrees of freedom ## Multiple R-squared: 0.0005989, Adjusted R-squared: -0.0004025 ## F-statistic: 0.598 on 1 and 998 DF, p-value: 0.4395 As expected, R notes that our matrix is singular and returns NA for the X2 estimate. But why is this? The objective of OLS is to minimize RSS, but clearly that’s not happening here. So let’s start there: we can evaluate the RSS function from the theory section over combinations of \\(\\beta_1\\) and \\(\\beta_2\\) values to take a look at the RSS surface.10. Let’s build our RSS function. RSS_OLS &lt;- function(b_vec){ t(y) %*% y - 2 * t(b_vec) %*% t(design_matrix) %*% y + t(b_vec) %*% t(design_matrix) %*% design_matrix %*% b_vec } Then we can create a dataset of \\(\\beta_1\\) and \\(\\beta_2\\) combinations and apply our function to it in order to evaluate the log likelihood functions for those \\(\\beta\\) values. b1_vec &lt;- seq(-10, 10, .25) b2_vec &lt;- seq(-10, 10, .25) surface &lt;- bind_cols(mean(y), expand_grid(b1_vec, b2_vec)) design_matrix &lt;- model.matrix(y ~ X1_std + X2_std) surface$rss_ols &lt;- apply(surface, MARGIN = 1, RSS_OLS) So what does our log-likelihood surface look like in this case? rss_ols_surface_plot &lt;- ggplot(surface, aes(x = b1_vec, y = b2_vec, fill = rss_ols)) + geom_raster(interpolate = TRUE) + # scale_fill_gradient(name = &quot;RSS&quot;, high = &#39;red&#39;, low = &#39;blue&#39;) + scale_fill_distiller(palette = &quot;RdBu&quot;, name = &quot;RSS: OLS&quot;) + labs(title = &quot;Surface Plot of RSS Function: OLS&quot;, subtitle = &quot;Evaluated Across Range of Beta Coefficients&quot;, x = TeX(&quot;$\\\\beta_1$&quot;), y = TeX(&quot;$\\\\beta_2$&quot;)) There’s a long valley! Clearly there’s not a unique minimum here: there are many combinations of \\(\\beta_1\\) and \\(\\beta_2\\) that could minimize RSS, so there is no unique solution with OLS! The valley in the RSS function corresponds to a ridge in the maximum likelihood surface. A Solution: Ridge Regression So how does ridge regression fix this optimization problem? By penalizing (or regularizing) our function! Ideally we would choose our penalty term, \\(\\lambda\\), optimally by cross-validating but we’ll arbitrarily set \\(\\lambda = 1,000\\) here to illustrate how ridge regression changes the RSS surface. Following the steps for evaluating the OLS RSS, we’ll create our ridge RSS function. RSS_ridge &lt;- function(b_vec){ lambda &lt;- 1000 t(y) %*% y - 2 * t(b_vec) %*% t(input_matrix) %*% y + t(b_vec) %*% t(input_matrix) %*% input_matrix %*% b_vec + lambda * t(b_vec) %*% b_vec } As noted in the theory section, we do not penalize the intercept. So we will create an input matrix that only has our X1 and X2 variable values. input_matrix &lt;- cbind(X1_std, X2_std) surface$rss_ridge &lt;- apply(surface[2:3], MARGIN = 1, RSS_ridge) So what does our log-likelihood surface look like now? rss_ridge_surface_plot &lt;- ggplot(surface, aes(x = b1_vec, y = b2_vec, fill = rss_ridge)) + geom_raster(interpolate = TRUE) + scale_fill_distiller(palette = &quot;RdBu&quot;, name = &quot;RSS: Ridge&quot;) + labs(title = &quot;Surface Plot of RSS Function: Ridge&quot;, subtitle = &quot;Evaluated Across Range of Beta Coefficients&quot;, x = TeX(&quot;$\\\\beta_1$&quot;), y = TeX(&quot;$\\\\beta_2$&quot;)) This looks much more like a surface with a unique minimum! Adding the L2 penalty “lifts” up the ends of the long valley in the RSS surface to create a bowl, for which we can easily identify and solve for a unique minimum. This also makes it easy to see how this penalization pushes or shrinks our coefficients toward zero. With OLS, we allow the least-squares solving process infinite space within which to find a unique solution, but this fails when we have a canyon in the RSS surface without a unique minimum. By applying the L2 constraint in the ridge regression definition of the problem we are restricting the solution space and forcing the solving process to find the best \\(\\beta_1\\) and \\(\\beta_2\\) that minimize RSS within that space. Residuals are a function of our \\(\\beta\\)s, so this allows for residuals to be higher for some observations than they would be under OLS, which is evidenced by the “lifting” of the RSS surface above. Choosing how much much space to allow (i.e., which value to use for \\(\\lambda\\)) is another optimization problem that is context-dependent and usually addressed with cross-validation. Solving for \\(\\hat{\\beta}^{Ridge}\\) We can easily implement the \\(\\hat{\\beta}^{Ridge}\\) solution derived in the theory section. Let’s write a function, ridge, to solve for the ridge regression coefficients and use it to solve our ridge regression problem. input_matrix &lt;- cbind(X1_std, X2_std) lambda &lt;- 1000 ridge &lt;- function(y_vec, X_matrix, lambda){ # Arguments: # y_vec - a continuous outcome vector # X_matrix - a standardized input vector # lambda - a &gt; 0 penalty term penalty &lt;- lambda * diag(1, ncol(X_matrix)) solve(t(X_matrix) %*% X_matrix + penalty) %*% t(X_matrix) %*% y_vec } b_vec_ridge &lt;- ridge(y_vec = y, X_matrix = input_matrix, lambda = lambda) ## [,1] ## X1_std -0.06102517 ## X2_std -0.06102517 In this case, because our standardized variables are identical, our ridge regression coefficients are identical. Why not use scale? Both scale and sd use Bessel’s correction to calculate the sample standard deviation. We instead use the population standard deviation. glmnet, the primary ridge regression implementation in R, uses the population standard deviation when standardizing variables.↩︎ This is almost identical to what we do in the univariate binary logistic case with the maximum likelihood function.↩︎ "],["case-2-correlated-independent-variables.html", "Case 2: Correlated Independent Variables", " Case 2: Correlated Independent Variables Perfect collinearity or multicollinearity like in the last case is rarely found in real data. What’s much more common is having variables that are strongly correlated with one another, but not perfectly. When this is the case, OLS can fail to find a solution due to numerical rounding (if variables are almost perfectly related) or find an unstable solution that can vary wildly between similar data and make it difficult to discern which variables contribute most to prediction of the outcome (usually by imprecisely estimating coefficients). In the same way as in the previous case, ridge regression can alleviate both of these problems, but to a different degree. We won’t perform cross validation to determine the optimal penalty parameter but will instead evaluate models over ranges of penalty values to show how ridge regression behaves over the range and compare to OLS. Data For this case, we’ll use the famous Boston housing data set. The data come pre-loaded in the MASS package, so there’s no need to download and read in data. The data set contains variables relating to home values in various towns and suburbs in Boston, such as crim (crime rate), nox (an air pollution measure), and rm (average number of rooms per home). We’ll build a model to predict medv, the median value of a home in a town. Some of the variable names are not incredibly self-explanatory, so feel free to check the documentation for more details. This case uses many of the same packages and functions that were loaded and written in the first case, so review that case if not already completed. We’ll use model.matrix to create our input matrix without the vector for the intercept and separate medv into its own vector. X &lt;- model.matrix(medv ~ ., data = Boston)[,-1] y &lt;- Boston$medv Then we can split our data into training sets and testing sets. We’ll use sample to randomly pick 60% of the rows in the data to use as our training set while the other 40% will go into our testing set. Then we’ll apply our standardize function we wrote to the input data for each set. train &lt;- sample(1:nrow(X), nrow(X) * .6) test &lt;- (-train) X_train &lt;- apply(X[train,], 2, standardize) y_train &lt;- y[train] X_test &lt;- apply(X[test,], 2, standardize) y_test &lt;- y[test] Exploratory Analysis We can easily visualize the correlations between the independent variables using the main function of the ggcorrplot package. First we create a correlation matrix of the independent variables using cor,11 R’s built-in correlation function, and feed it into ggcorrplot for a quick and simple correlation heat map. X_corr &lt;- cor(X_train) ggcorrplot(X_corr, type = &quot;lower&quot;, colors = rev(brewer.pal(3, name = &quot;RdBu&quot;))) Several variables are highly correlated with one another. indus (a measure of town industrial concentration), nox (air pollution), and age (average home age) are all highly negatively correlated with dis (distance to employment centers). It’s probably not a surprise that some towns in more industrial areas would have more exposure to air pollution and that development might be more common (and so home ages would be lower) in high-employment suburbs far away from job centers. But how much, together, are these variables contributing to prediction? In other words, do we need all of these variables that are highly related to one another or will one or two be enough to capture the “action” contained in all of them? To explore this further, we can quickly perform a principal component analysis12 of our input matrix. prcomp is R’s built-in PCA function. We’ll look only at the first five principal components. boston_pcomps &lt;- prcomp(X_train)$rotation[,1:5] ## PC1 PC2 PC3 PC4 PC5 ## crim -0.264780006 -0.37282951 -0.16137850 0.14940584 -0.09959834 ## zn 0.258614828 -0.34251580 -0.23554642 0.27582209 -0.13375550 ## indus -0.345089443 0.13653508 0.01805417 0.01606727 0.06103215 ## chas 0.007525757 0.37446396 -0.32623810 0.72461728 0.43784178 ## nox -0.336301311 0.20732636 -0.21875410 -0.03748248 -0.16122577 ## rm 0.182576134 0.02412359 -0.60878205 -0.43797652 0.15803329 ## age -0.306075721 0.32632017 -0.04290426 -0.13304103 -0.17632615 ## dis 0.323360209 -0.32134530 0.17335956 0.20247976 0.02936345 ## rad -0.313515436 -0.33509540 -0.21820409 -0.04899921 0.28889519 ## tax -0.336258639 -0.26954740 -0.15223175 -0.03086535 0.22602840 ## ptratio -0.218911877 -0.19989065 0.41446534 -0.11632606 0.57560981 ## black 0.212536538 0.32601358 0.22275140 -0.15243863 0.36679772 ## lstat -0.307788532 0.01778561 0.26299642 0.29021112 -0.30564916 The rotation matrix from prcomp contains the loadings, which are the correlations between the input variables and the directions represented by the principal components when the input variables are standardized.13 To find how much each variable “contributes” to the direction of each principal component we simply take the column-wise absolute values as a percentage of the totals. We’ll do this using a combination of sweep to divide within columns and colSums to get column totals. boston_pcomps &lt;- as.data.frame(abs(boston_pcomps)) boston_pcomps &lt;- sweep(abs(boston_pcomps), 2, colSums(abs(boston_pcomps)), &quot;/&quot;) * 100 ## PC1 PC2 PC3 PC4 PC5 ## crim 7.7572242 11.4583066 5.2812870 5.7118162 3.2977871 ## zn 7.5766039 10.5266641 7.7085129 10.5447357 4.4287604 ## indus 10.1100390 4.1961828 0.5908425 0.6142551 2.0208271 ## chas 0.2204811 11.5085387 10.6764968 27.7022684 14.4973198 ## nox 9.8525743 6.3718374 7.1589659 1.4329631 5.3383246 ## rm 5.3489085 0.7413992 19.9230549 16.7439329 5.2326189 ## age 8.9670593 10.0289178 1.4040887 5.0861862 5.8383114 ## dis 9.4734406 9.8760234 5.6733802 7.7408430 0.9722493 ## rad 9.1850196 10.2986104 7.1409662 1.8732499 9.5655695 ## tax 9.8513241 8.2840996 4.9819498 1.1799888 7.4839957 ## ptratio 6.4134318 6.1433129 13.5638292 4.4471693 19.0589386 ## black 6.2266544 10.0194954 7.2897820 5.8277604 12.1449895 ## lstat 9.0172392 0.5466117 8.6068440 11.0948311 10.1203079 Scanning across the principal components (columns) it’s relatively easy to see which variables contribute the most to the first five principal components. This is analogous to the discussion in the theory section about variables being associated with singular values: we should expect that the variables that contribute the most to the these first few principal components will be among the slowest to shrink to zero as \\(\\lambda\\) increases. Implementation We’ll solve for our ridge regression estimates over a wide range of the penalty term, \\(\\lambda\\), to explore how ridge regression shrinks our variables - paying special attention to our three correlated variables - and how \\(\\lambda\\) plays a role in the bias-variance tradeoff. For this example we’ll evaluate our models based on minimizing mean square error. We’ll create two outcome data sets, betas and performance, that show us how our estimated coefficients and how our mean square error behave across the range of \\(\\lambda\\)s. betas &lt;- NULL performance &lt;- NULL for (lambda in seq(0, 1000, 1)) { b_vec &lt;- ridge(y_train, X_train, lambda) b_MSE &lt;- mean((X_test %*% b_vec + mean(y_test) - y_test)^2) betas &lt;- bind_rows(betas, tibble(&quot;lambda&quot; = lambda, &quot;Variable&quot; = colnames(X_train), &quot;Coefficient&quot; = b_vec)) performance &lt;- bind_rows(performance, tibble(&quot;lambda&quot; = lambda, &quot;Test_MSE&quot; = b_MSE)) } Let’s visualize how the coefficients of our variables change as \\(\\lambda \\rightarrow \\infty\\). palette &lt;- colorRampPalette(brewer.pal(12, &quot;Paired&quot;))(length(colnames(X_train))) coef_plot &lt;- ggplot(betas, aes(x = lambda, y = Coefficient, color = Variable)) + geom_line(size = 1.5) + scale_color_manual(values = palette) + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;) + labs(title = &quot;Ridge Regression Coefficient Shrinkage&quot;, x = TeX(&quot;$\\\\lambda$&quot;)) As expected, the estimates for the variables with higher overall principal component contributions maintain their magnitude the most as \\(\\lambda \\rightarrow \\infty\\) while estimates for the variables that don’t contribute much to the principal components (and therefore don’t contribute much to the variance of the projected data) shrink more quickly toward zero. Next, we’ll visualize the test set MSE for \\(\\lambda \\rightarrow \\infty\\). Can ridge regression outperform OLS? performance_plot &lt;- ggplot(performance %&gt;% filter(lambda &lt;= 200), aes(x = lambda, y = Test_MSE)) + geom_line() + geom_hline(yintercept = performance$Test_MSE[performance$lambda == 0], linetype=&quot;dashed&quot;, color = &quot;red&quot;) + labs(title = &quot;Ridge Regression Test MSE Performance&quot;, x = TeX(&quot;$\\\\lambda$&quot;), y = &quot;Test MSE&quot;) The dashed red line represents the test MSE for OLS (i.e., where \\(\\lambda = 0\\)). With some penalty, ridge performs better in prediction than OLS in this case with this train/test split. Shrinking some of our coefficients close to zero (i.e., using a penalty, like \\(\\lambda\\) in ridge regression) can minimize the variance of our fitted/predicted values, which can sometimes translate to better predictions. While estimating model variance via bootstrapping is outside the scope of this example, generally OLS - which minimizes bias but at the expense of allowing more than the minimum variance - doesn’t reach as low a MSE as a regularized regression like ridge regression - which can find a balance between bias and variance that minimizes MSE. However, this can change depending on the particular data in the train/test split. cor calculates Pearson’s correlation by default.↩︎ If you’re unfamiliar with PCA, see this excellent intuitive explanation.↩︎ If this is unfamiliar, for now just think of this as a measure of how much a direction of a principal component is associated with the existing directions of the variables in the hyperplanar space occupied by the data.↩︎ "],["standardization.html", "Standardization", " Standardization Why did we standardize our independent variables in these cases? Because the ridge regression solutions will change depending on the scales of the variables. In reality, this may be because of differences in units between variables (e.g., a dataset with temperatures, distances, monetary values measured in whole amounts or 1000s, etc.). Standardizing puts our variables on the same scale. OLS does not have this problem: it will find the same solution for the same data, scaled or not. To illustrate the point, let’s generate samples of two independently normally distributed variables as well as versions of those variables scaled or multiplied by 1000. y &lt;- rnorm(1000, mean = 0, sd = 1) X1 &lt;- rnorm(1000, mean = runif(1, -10, 10), sd = runif(1, 1, 5)) X2 &lt;- rnorm(1000, mean = runif(1, -10, 10), sd = runif(1, 5, 10)) X1_scaled &lt;- X1 * 1000 X2_scaled &lt;- X2 * 1000 Fitting a simple linear model, we can easily see that the model produces a scaled version of the non-scaled fit using the scaled data. lm(y ~ X1 + X2)$coefficients ## (Intercept) X1 X2 ## 0.002182465 -0.002519184 -0.003036797 lm(y ~ X1_scaled + X2_scaled)$coefficients ## (Intercept) X1_scaled X2_scaled ## 2.182465e-03 -2.519184e-06 -3.036797e-06 But we won’t get identical solutions if we fit both versions of the data using ridge regression for any \\(\\lambda &gt; 0\\). The estimated effects of the independent variables on the outcome fundamentally change, not simply by the magnitude of the change in scale. ridge(y = y, X = cbind(X1, X2), lambda = 1) ## [,1] ## X1 -0.002169880 ## X2 -0.002941879 ridge(y = y, X = cbind(X1_scaled, X2_scaled), lambda = 1) ## [,1] ## X1_scaled -2.170306e-06 ## X2_scaled -2.941882e-06 In other words, OLS solutions are equivariant to scaling but ridge solutions aren’t. Standardizing the data removes scaling for comparability. Of course, we lose some interpretability in the need to standardize our data and it means that we must input standardized test data into our ridge model for prediction (rather than raw data). glmnet, the most prominent implementation of ridge regression in R, implements ridge regression in a different way than presented here: it standardizes the data within the function (so that data don’t need to be standardized prior to the call) and then produces de-standardized regression coefficients that can be used with raw, non-standardized test data for prediction. See the glmnet vignette for details. tidymodels and parsnip use glmnet to implement regularized regression. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
