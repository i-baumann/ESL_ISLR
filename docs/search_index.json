[["logistic-regression.html", "Logistic Regression", " Logistic Regression This example walks through fitting a univariate logistic regression model to (idealized) generated data. It should be easy to see how this generalizes to multivariate cases. It assumes some prior knowledge of logistic regression and log-odds (or see Elements of Statistical Learning) and will be light on theory. We’ll skip doing a train/test split and comparison in this example and focus solely on implementing logistic regression from scratch. "],["data-generation.html", "Data Generation", " Data Generation This example uses the tidyverse and latex2exp packages. library(tidyverse) library(latex2exp) We’ll generate data with 1,000 observations from a simple case: a binary (0/1) balanced outcome… y &lt;- append(rep(0, each = 500), rep(1, each = 500)) …and a Gaussian independent variable. To ensure some overlap between the \\(y = 0\\) and \\(y = 1\\) observations we’ll draw from two normal distributions with offset means. To include an intercept term we’ll augment our independent variable vectors with a one-vector. means &lt;- sample.int(6, 2, replace = FALSE) x &lt;- append(rnorm(500, mean = means[1], sd = 1), rnorm(500, mean = means[2], sd = 1)) x &lt;- cbind(1, x) colnames(x) &lt;- c(&quot;Int&quot;, &quot;X1&quot;) "],["the-log-likelihood-surface.html", "The Log-Likelihood Surface", " The Log-Likelihood Surface Like linear regression, logistic regression is simply an optimization problem. In the logistic case, the function we maximize is the log-likelihood function \\[\\ell (\\vec{\\beta}) = \\sum_{i = 1}^N \\left[ \\vec{y}_i \\vec{\\beta}^T \\vec{x}_i - \\log(1 + e^{\\vec{\\beta}^T \\vec{x}_i}) \\right]\\] This function is strictly concave, so it will have one maximum and our \\(\\beta\\)s (\\(\\beta_0\\) for the intercept and \\(\\beta_1\\) for our single independent variable) will (hopefully!) maximize this function. In this case we can easily visualize a log-likelihood surface by evaluating the function over some set of values for our two \\(\\beta\\)s. First we’ll create our set of \\(\\beta\\) values and then use those vectors to create a tibble of coordinates for our plot as well as put our \\(y\\) vector and design matrix together to form a data matrix for easier evaluation. b0_vec &lt;- seq(-20, 20, .5) b1_vec &lt;- seq(-20, 20, .5) surface &lt;- expand_grid(b0_vec, b1_vec) data_matrix &lt;- cbind(y, x) Since we’re evaluating the log-likelihood function over the data for all combinations of \\(\\beta\\) values in our set, we’ll write two functions. The first handles the interior of the sum of the log-likelihood log_like_interior &lt;- function(data, b_0, b_1){ b_vec &lt;- c(b_0, b_1) data[1] %*% t(b_vec) %*% data[2:3] - log(1 + exp(t(b_vec) %*% data[2:3])) } and the second handles the summation, applying over the data matrix. log_like_sum &lt;- function(b_vec){ sum(apply(data_matrix, 1, log_like_interior, b_0 = b_vec[1], b_1 = b_vec[2])) } Then we can apply the summation function over the \\(\\beta\\)-value coordinated in our surface tibble and add the appropriate log-likelihood to that tibble. surface$l &lt;- apply(surface, 1, log_like_sum) Now that we have our \\(\\beta\\) coordinates and their corresponding log-likelihoods for our data, we can plot the log-likelihood surface. likelihood_surface_plot &lt;- ggplot(surface, aes(x = b0_vec, y = b1_vec, z = l, fill = l)) + geom_raster(interpolate = TRUE) + geom_contour(bins = 50, color = &quot;grey&quot;, size = 0.5) + scale_fill_distiller(palette = &quot;YlOrRd&quot;, name = &quot;Log-Likelihood&quot;, direction = 1) + labs(title = &quot;Surface Plot of Log-Likelihood Function&quot;, subtitle = &quot;Evaluated Across Range of Beta Coefficients&quot;, x = TeX(&quot;$\\\\beta_0$&quot;), y = TeX(&quot;$\\\\beta_1$&quot;)) # + # theme(legend.position = &quot;none&quot;) Our \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) coordinates estimated by our logistic regression should be in the bright yellow area, representing our highest-valued bin of log-likelihoods, at the maximum of the log-likelihood surface. "],["implementation.html", "Implementation", " Implementation To fit the logistic regression model to the data and find the optimal \\(\\hat{\\beta}\\) vector we set the derivatives of the log-likelihood function (or score equations ) with respect to \\(\\vec{\\beta}\\) to zero: \\[\\frac{\\partial \\ell(\\vec{\\beta})}{\\partial \\vec{\\beta}} = \\sum_{i = 1}^N x_i (y_i - \\Pr (Y = 1 | X = x_i; \\vec{\\beta})) = 0\\] Method 1: Newton-Raphson/IRLS The method Hastie et al. use in Elements of Statistical Learning is the Newton-Raphson method (which is re-written to be a iteratively re-weighted least squares problem), which iteratively updates \\(\\vec{\\hat{\\beta}}\\) to find the maximum likelihood. We will use a stopping criteria that checks how the score equations for \\(\\vec{\\hat{\\beta}}_0\\) and \\(\\vec{\\hat{\\beta}}_1\\) evaluate and stops iterating when they are within a tolerable distance from zero. This method uses a diagonal matrix \\(W\\) which contains the weights derived from \\(\\Pr (Y = 1 | X = x_i; \\vec{\\hat{\\beta}})\\) where \\(\\vec{\\hat{\\beta}}\\) contains the estimated coefficients from the previous iteration. These weights are then used to solve a weighted least squares problem, which produces a vector \\(z\\) of “adjusted” responses that are used to update the coefficients. In each iteration \\(i\\) we will solve \\[\\vec{\\hat{\\beta}} \\vphantom{1}^{i} = (X^T W X)^{-1} X^T W \\vec{z}\\] where \\[z = X \\vec{\\hat{\\beta}} \\vphantom{1}^{i - 1} + W^{-1} (\\vec{y} - \\vec{p})\\] and \\(\\vec{p}\\) are the fitted probabilities given \\(\\vec{\\hat{\\beta}} \\vphantom{1}^{i - 1}\\). beta_vec_irls &lt;- c(0,0) max_check &lt;- FALSE irls_iterations &lt;- 0 while (max_check != TRUE) { p &lt;- exp(x %*% beta_vec_irls) / (1 + exp(x %*% beta_vec_irls)) W &lt;- diag(as.vector(p * (1 - p))) z &lt;- x %*% beta_vec_irls + solve(W) %*% (y - p) beta_vec_irls &lt;- solve(t(x) %*% W %*% x) %*% t(x) %*% W %*% z # Check that we are actually maximizing the likelihood function by checking # if the score functions are evaluating to zero score &lt;- t(x) %*% (y - p) score &lt;- c(score[1], score[2]) max_check &lt;- all.equal(c(0,0), score, tolerance = .Machine$double.eps ^ 0.5) irls_iterations &lt;- irls_iterations + 1 } Our \\(\\vec{\\hat{\\beta}}\\) for this example from IRLS is: ## [,1] ## Int 2.589805 ## X1 -1.038948 Method 2: Modified IRLS Because \\(W\\) is a \\(N \\times N\\) matrix, IRLS process may not always be effcient. We can side-step the large \\(W\\) matrix and matrix operations involving it by instead directly multiplying the rows of the \\(X\\) matrix by the predicted probability of \\(Y = 1\\) for the observation corresponding to that row (given the “old” \\(\\beta\\) from the previous iteration); see pseudo-code. This new matrix will be \\(\\tilde{X}\\). Note that, in R, %*% is a dot product operator. But if two matrices have identical dimensions then the traditional multiplication operator * will perform an element-for-element scalar multiplication. So we will use our vector of probabilities to create an appropriately dimensioned matrix using matrix and ncol to create \\(\\tilde{X}\\). beta_vec_mod &lt;- c(0,0) max_check &lt;- FALSE mod_iterations &lt;- 0 while (max_check != TRUE) { p &lt;- exp(x %*% beta_vec_mod) / (1 + exp(x %*% beta_vec_mod)) p_mat &lt;- matrix(p * (1 - p), length(p * (1 - p)), ncol(x)) # Use `sweep` to multiply each row of x by scalar vector x_tilde &lt;- x * p_mat beta_vec_mod &lt;- beta_vec_mod + solve(t(x) %*% x_tilde) %*% t(x) %*% (y - p) # Check that we are actually maximizing the likelihood function by checking # if the score functions are evaluating to zero score &lt;- t(x) %*% (y - p) score &lt;- c(score[1], score[2]) max_check &lt;- all.equal(c(0,0), score, tolerance = .Machine$double.eps ^ 0.5) mod_iterations &lt;- mod_iterations + 1 } This will sometimes converge a little more quickly. The number of iterations for this particular example were ## [1] &quot;IRLS iterations: 6&quot; ## [1] &quot;Modified IRLS iterations: 6&quot; Our \\(\\vec{\\hat{\\beta}}\\) for this example from the modified algorithm is: ## [,1] ## Int 2.589805 ## X1 -1.038948 "],["testing.html", "Testing", " Testing Comparing to glm We can easily check the coefficients we’ve calculated against those produced by R’s glm. By default glm uses IRLS to fit models. glm_model &lt;- glm(y ~ X1, data = as.data.frame(data_matrix), family = &quot;binomial&quot;) beta_vec_irls ## [,1] ## Int 2.589805 ## X1 -1.038948 beta_vec_mod ## [,1] ## Int 2.589805 ## X1 -1.038948 glm_model$coefficients ## (Intercept) X1 ## 2.589805 -1.038948 The Log-Likelihood Surface We can also use the log-likelihood surface plot to (approximately) verify that the coefficients we estimated are correct and that the logistic regression estimates do maximize the log-likelihood function by plotting the point for our estimates on the surface plot: likelihood_surface_plot &lt;- likelihood_surface_plot + geom_point(aes(x = beta_vec_mod[1], y = beta_vec_mod[2])) Plotting the Logistic Fit To get the classic logistic regression plot we can calculate \\[\\Pr (Y = 1 | X = x_i; \\vec{\\beta}) = \\frac{e^{\\vec{x} \\hat{\\vec{\\beta^T}}}}{1 + e^{\\vec{x} \\hat{\\vec{\\beta^T}}}}\\] for all \\(x_i\\) and plot those probabilities as a line against the data points on the \\((x, y)\\) plane. We’ll also color-code the points by how our model fit classifies the points, using the decision rule that an observation is classified to \\(\\hat{Y} = 0\\) if \\(\\Pr (Y = 1 | X = x_i; \\vec{\\beta}) &lt; 0.5\\) and \\(\\hat{Y} = 1\\) otherwise. pred_prob &lt;- exp(data_matrix[, 2:3] %*% beta_vec_mod) / (1 + exp(data_matrix[, 2:3] %*% beta_vec_mod)) data_tibble &lt;- as_tibble(data_matrix) data_tibble$pred_prob &lt;- pred_prob[,1] data_tibble$pred &lt;- if_else(data_tibble$pred_prob &lt; .5, 0, 1) logistic_plot_preds &lt;- ggplot(data_tibble) + geom_point(aes(X1, y, color = as.factor(pred))) + geom_line(aes(X1, pred_prob)) + scale_color_manual(name = &quot;Predicted y&quot;, values = c(&quot;red&quot;, &quot;blue&quot;)) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
