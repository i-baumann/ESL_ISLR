<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Multivariate Linear Discriminant Analysis</title>

<script src="site_libs/header-attrs-2.9/header-attrs.js"></script>
<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<script src="site_libs/clipboard-2.0.6/clipboard.min.js"></script>
<link href="site_libs/xaringanExtra-clipboard-0.2.6/xaringanExtra-clipboard.css" rel="stylesheet" />
<script src="site_libs/xaringanExtra-clipboard-0.2.6/xaringanExtra-clipboard.js"></script>
<script>window.xaringanExtraClipboard(null, {"button":"<i class=\"fa fa-clipboard\"><\/i>","success":"<i class=\"fa fa-check\" style=\"color: #90BE6D\"><\/i>","error":"Press Ctrl+C to Copy"})</script>
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>








<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.tab('show');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">ESL/ISLR Examples</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/my-cabbages/ESL_ISLR">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Multivariate Linear Discriminant Analysis</h1>

</div>


<p>This example walks through using linear discriminant analysis to classify observations in a three-class multivariate setting with (idealized) generated data. I’d suggest reading the <a href="univariate_LDA.html">univariate LDA</a> example first.</p>
<p>Though not shown here, this example uses the <a href="https://cran.r-project.org/web/packages/tidyverse/index.html" target="_blank"><code>tidyverse</code></a> and <a href="https://cran.r-project.org/web/packages/MASS/index.html" target="_blank"><code>MASS</code></a> packages.</p>
<div id="data-generation" class="section level2">
<h2>Data Generation</h2>
<p>We’ll generate data with three possible outcome classes (coded as 0, 1, and 2) with two independent variables/predictors.</p>
<p>We’ll use <code>MASS</code>’s <a href="https://www.rdocumentation.org/packages/rockchalk/versions/1.8.144/topics/mvrnorm" target="_blank"><code>mvnorm</code></a> function to sample from a multivariate normal distribution. Throughout this example I’ll refer to the first variable as X1 and the second variable as X2. For each class we’ll construct a vector of means for X1 and X2, randomly sampled integers between 1 and 10:</p>
<pre class="r"><code>X1_means &lt;- sample.int(10, 3, replace = FALSE)
X2_means &lt;- sample.int(10, 3, replace = FALSE)

pop_mean_c0_X1 &lt;- X1_means[1]
pop_mean_c1_X1 &lt;- X1_means[2]
pop_mean_c2_X1 &lt;- X1_means[3]

pop_mean_c0_X2 &lt;- X2_means[1]
pop_mean_c1_X2 &lt;- X2_means[2]
pop_mean_c2_X2 &lt;- X2_means[3]

mu_c0 &lt;- c(pop_mean_c0_X1, pop_mean_c0_X2)
mu_c1 &lt;- c(pop_mean_c1_X1, pop_mean_c1_X2)
mu_c2 &lt;- c(pop_mean_c2_X1, pop_mean_c2_X2)</code></pre>
<p>Remember that in the univariate case LDA assumes equal variance for each class so in the multivariate case <strong>LDA assumes equal covariance for each class</strong>, meaning that the population covariance matrices for each class are identical (they also need to be <a href="https://en.wikipedia.org/wiki/Definite_matrix" target="_blank">positive semi-definite</a>). For our data to be sampled from distributions with this idealized property, we’ll construct a single covariance matrix (which we’ll call <code>pop_sigma</code>) to use when sampling our data:</p>
<pre class="r"><code>pop_corr &lt;- runif(1, 0, 1)
pop_var &lt;- runif(1, 0, 10)
pop_sigma &lt;- matrix(c(pop_var, pop_corr,
                      pop_corr, pop_var), 2, 2)</code></pre>
<p>In this particular example our common covariance matrix looks like</p>
<pre><code>##            [,1]       [,2]
## [1,] 2.37429323 0.03586445
## [2,] 0.03586445 2.37429323</code></pre>
<p>Now that we have our variable- and class-specific means and a common covariance matrix we can use <code>mvnorm</code> to generate our sample data. We’ll create equally-sized train and test sets with 300 observations in each of our three classes and bind our X1 and X2 samples together with our outcome set into a data frame.</p>
<pre class="r"><code>n &lt;- 300

c0_train &lt;- mvrnorm(n = n,
              mu = mu_c0,
              Sigma = pop_sigma)

c1_train &lt;- mvrnorm(n = n,
              mu = mu_c1,
              Sigma = pop_sigma)

c2_train &lt;- mvrnorm(n = n,
              mu = mu_c2,
              Sigma = pop_sigma)

c0_test &lt;- mvrnorm(n = n,
              mu = mu_c0,
              Sigma = pop_sigma)

c1_test &lt;- mvrnorm(n = n,
              mu = mu_c1,
              Sigma = pop_sigma)

c2_test &lt;- mvrnorm(n = n,
              mu = mu_c2,
              Sigma = pop_sigma)

train_sample_df &lt;- bind_rows(
  tibble(y = 0,
         X1 = c0_train[,1],
         X2 = c0_train[,2]),
  tibble(y = 1,
         X1 = c1_train[,1],
         X2 = c1_train[,2]),
  tibble(y = 2,
         X1 = c2_train[,1],
         X2 = c2_train[,2])
)

test_sample_df &lt;- bind_rows(
  tibble(y = 0,
         X1 = c0_test[,1],
         X2 = c0_test[,2]),
  tibble(y = 1,
         X1 = c1_test[,1],
         X2 = c1_test[,2]),
  tibble(y = 2,
         X1 = c2_test[,1],
         X2 = c2_test[,2])
)</code></pre>
<div id="initial-visualization" class="section level3">
<h3>Initial Visualization</h3>
<p>What do our data look like? We can easily create scatterplots:</p>
<pre class="r"><code>train_sample_scatter &lt;- ggplot(train_sample_df) + 
  geom_point(aes(x = X1, y = X2, color = as.factor(y))) +
  scale_color_manual(values = c(&quot;red&quot;, &quot;blue&quot;, &quot;green&quot;),
                     name = &quot;Class&quot;) +
  labs(title = &quot;Bivariate Training Sample of Three Classes: Scatterplot&quot;)

test_sample_scatter &lt;- ggplot(test_sample_df) + 
  geom_point(aes(x = X1, y = X2, color = as.factor(y))) +
  scale_color_manual(values = c(&quot;red&quot;, &quot;blue&quot;, &quot;green&quot;),
                     name = &quot;Class&quot;) +
  labs(title = &quot;Bivariate Test Sample of Three Classes: Scatterplot&quot;)</code></pre>
<p>Looking only at the training data for now:</p>
<p><img src="multivariate_LDA_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
</div>
</div>
<div id="the-bayes-classifier-and-decision-boundaries" class="section level2">
<h2>The Bayes Classifier and Decision Boundaries</h2>
<p>Like with the <a href="univariate_LDA.html" target="&quot;_blank">univariate LDA example</a> we will compare performance against the Bayes classifier. This time, because we have more than one independent variable/predictor, building the classifier and displaying the decision boundaries won’t be as straightforward as in the univariate LDA case (though the fundamental concept is the same).</p>
<p>Also like in the univariate example, we will use a discriminant function to determine which class to assign to a given observation. The discriminant functions are exactly the same and are derived in the same way, except that since we have more than one independent variable we will use a vector of variables and instead of a scalar variance (<span class="math inline">\(\sigma\)</span>) we will use the common covariance matrix we constructed above (<span class="math inline">\(\Sigma\)</span>).</p>
<div id="theory" class="section level3">
<h3>Theory</h3>
<p>This section will assume that you have read and understood the theory section of the <a href="univariate_LDA.html" target="&quot;_blank">univariate LDA example</a>. The theory in the multivariate case is essentially the same, but we will cover it here for thoroughness.</p>
<p>Like in the univariate case, we will take Bayes’ theorem,</p>
<center>
<span class="math display">\[\Pr(Y = k|X = x) = \frac{\pi_k f_k(x)}{\sum^K_{j = 1} \pi_j f_j(x)}\]</span>
</center>
<p>where <span class="math inline">\(k\)</span> represents one class and <span class="math inline">\(j\)</span> all others, <span class="math inline">\(\pi_k\)</span> represents the prior probability of class <span class="math inline">\(k\)</span>, <span class="math inline">\(p\)</span> is the number of variables, and we use a probability density function for <span class="math inline">\(f(\cdot)\)</span>. In this case we use the <a href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution" target="_blank">multivariate normal PDF</a></p>
<center>
<span class="math display">\[\frac{1}{{2\color{red}{\pi}}^{\frac{p}{2}} \det(\Sigma)^\frac{1}{2}} e^{\frac{-1}{2} (x - \mu_k)^T \Sigma^{-1} (x - \mu_k)}\]</span>
</center>
<p>where, in this idealized example, <span class="math inline">\(\Sigma_k, \Sigma_l, \ldots, \Sigma_K = \Sigma\)</span> since we are assuming (and in fact generating data from a distribution in which) all classes have the same covariance matrix. As noted in the univariate example, <span class="math inline">\(\color{red}{\pi}\)</span> in the PDF is the literal value pi, not a prior probability.</p>
<p>Whereas in the univariate case we only had two outcomes/classes, in this example we have three. Instead of pairwise solving for two discriminant functions we need to pairwise solve for three. The process is of course the same for each pair, so we will generically work through solving for the discriminant functions for a single pair of classes.</p>
<p>Just as in the univariate case, in order to arrive at our discriminant functions we take the log of the ratio of conditional probabilities and the summation terms in the ratio cancel, leaving us with</p>
<center>
<span class="math display">\[\begin{align}
\log\frac{\Pr(Y = k|X = x)}{\Pr(Y = l|X = x)} &amp;= \log \frac{\pi_k f_k(x)}{\pi_l f_l(x)} \\
&amp;= \log{\frac{\pi_k}{\pi_l}} + \log{\frac{f_k(x)}{f_l(x)}}
\end{align}\]</span>
</center>
<p>Plugging in the multivariate normal PDF for <span class="math inline">\(f(\cdot)\)</span>, like in the univariate case, allows for a lot of simplification but with some twists:</p>
<center>
<span class="math display">\[\begin{align}
\log\frac{\Pr(Y = k|X = x)}{\Pr(Y = l|X = x)} &amp;= \log{\frac{\pi_k}{\pi_l}} + \log \frac{\frac{1}{{2\color{red}{\pi}}^{\frac{p}{2}} \det(\Sigma)^\frac{1}{2}} e^{\frac{-1}{2} (x - \mu_k)^T \Sigma^{-1} (x - \mu_k)}}{\frac{1}{{2\color{red}{\pi}}^{\frac{p}{2}} \det(\Sigma)^\frac{1}{2}} e^{\frac{-1}{2} (x - \mu_l)^T \Sigma^{-1} (x - \mu_l)}} \\
&amp;= \log{\frac{\pi_k}{\pi_l}} + \log \left( \frac{1}{{2\color{red}{\pi}}^{\frac{p}{2}} \det(\Sigma)^\frac{1}{2}} \bigg/ \frac{1}{{2\color{red}{\pi}}^{\frac{p}{2}} \det(\Sigma)^\frac{1}{2}}  \right) + \log{\frac{e^{\frac{-1}{2} (x - \mu_k)^T \Sigma^{-1} (x - \mu_k)}}{e^{\frac{-1}{2} (x - \mu_l)^T \Sigma^{-1} (x - \mu_l)}}} \\
&amp;= \log{\frac{\pi_k}{\pi_l}} - \frac{1}{2} (x - \mu_k)^T \Sigma^{-1} (x - \mu_k) + \frac{1}{2} (x - \mu_l)^T \Sigma^{-1} (x - \mu_l) \\
\end{align}\]</span>
</center>
<p>Because <span class="math inline">\(\Sigma\)</span> represents the covariance matrix for all classes, a convenient cancellation occurs after we expand the right two terms in the expression above:</p>
<center>
<span class="math display">\[\begin{align}
\log\frac{\Pr(Y = k|X = x)}{\Pr(Y = l|X = x)} &amp;= \log{\frac{\pi_k}{\pi_l}} - \frac{1}{2} (x - \mu_k)^T \Sigma^{-1} (x - \mu_k) + \frac{1}{2} (x - \mu_l)^T \Sigma^{-1} (x - \mu_l) \\
&amp;= \log{\frac{\pi_k}{\pi_l}} \color{red}{- \frac{1}{2}x^T \Sigma^{-1}x} + \frac{1}{2}x^T \Sigma^{-1} \mu_k + \frac{1}{2}\mu_k^T \Sigma^{-1} x - \frac{1}{2} \mu_k^T \Sigma^{-1} \mu_k \\ &amp;\qquad \qquad \color{red}{+ \frac{1}{2}x^T \Sigma^{-1}x} - \frac{1}{2}x^T \Sigma^{-1} \mu_l - \frac{1}{2}\mu_l^T \Sigma^{-1} x + \frac{1}{2} \mu_l^T \Sigma^{-1} \mu_l \\
&amp;= \log{\frac{\pi_k}{\pi_l}} \color{purple}{+ \frac{1}{2}x^T \Sigma^{-1} \mu_k + \frac{1}{2}\mu_k^T \Sigma^{-1} x} \color{green}{- \frac{1}{2} \mu_k^T \Sigma^{-1} \mu_k} \\ &amp;\qquad ~~~~~~~ \color{purple}{- \frac{1}{2}x^T \Sigma^{-1} \mu_l - \frac{1}{2}\mu_l^T \Sigma^{-1} x} \color{green}{+ \frac{1}{2} \mu_l^T \Sigma^{-1} \mu_l} \\
\end{align}\]</span>
</center>
<p>This is where the fun starts. Because <span class="math inline">\(\Sigma\)</span> is symmetric (and so <span class="math inline">\(\Sigma^{-1}\)</span> is symmetric) <span class="math inline">\(a^T \Sigma^{-1} b = b^T \Sigma^{-1} a\)</span>. So the <span style="color:purple">purple</span> terms can be rewritten and consolidated:</p>
<center>
<span class="math display">\[\begin{align}
\log\frac{\Pr(Y = k|X = x)}{\Pr(Y = l|X = x)} &amp;= \log{\frac{\pi_k}{\pi_l}} \color{purple}{+ x^T \Sigma^{-1} \mu_k} \color{purple}{- x^T \Sigma^{-1} \mu_l} \color{green}{- \frac{1}{2} \mu_k^T \Sigma^{-1} \mu_k} \color{green}{+ \frac{1}{2} \mu_l^T \Sigma^{-1} \mu_l} \\
&amp;= \log{\frac{\pi_k}{\pi_l}} \color{purple}{+ x^T \Sigma^{-1} (\mu_k - \mu_l)} \color{green}{- \frac{1}{2} \mu_k^T \Sigma^{-1} \mu_k} \color{green}{+ \frac{1}{2} \mu_l^T \Sigma^{-1} \mu_l} \\
\end{align}\]</span>
</center>
<p>The <span style="color:green">green</span> terms can’t be easily simplified unless we creatively add zero to the equation. Using the above property that <span class="math inline">\(a^T \Sigma^{-1} b = b^T \Sigma^{-1} a\)</span> we can do exactly that. Let’s add <span class="math inline">\(0 = \frac{1}{2} \mu_k^T \Sigma^{-1} \mu_l - \frac{1}{2} \mu_l^T \Sigma^{-1} \mu_k\)</span> and factor and consolidate:</p>
<center>
<span class="math display">\[\begin{align}
\log\frac{\Pr(Y = k|X = x)}{\Pr(Y = l|X = x)} &amp;= \log{\frac{\pi_k}{\pi_l}} \color{purple}{+ x^T \Sigma^{-1} (\mu_k - \mu_l)} \color{green}{- \frac{1}{2} \mu_k^T \Sigma^{-1} \mu_k} \color{green}{+ \frac{1}{2} \mu_l^T \Sigma^{-1} \mu_l + \frac{1}{2} \mu_k^T \Sigma^{-1} \mu_l - \frac{1}{2} \mu_l^T \Sigma^{-1} \mu_k} \\
&amp;= \log{\frac{\pi_k}{\pi_l}} \color{purple}{+ x^T \Sigma^{-1} (\mu_k - \mu_l)} \color{green}{- \frac{1}{2} \mu_k^T \Sigma^{-1} (\mu_k - \mu_l) - \frac{1}{2} \mu_l^T \Sigma^{-1} (\mu_k - \mu_l)} \\
&amp;= \log{\frac{\pi_k}{\pi_l}} \color{purple}{+ x^T \Sigma^{-1} (\mu_k - \mu_l)} \color{green}{- \frac{1}{2} (\mu_k + \mu_l)^T \Sigma^{-1} (\mu_k - \mu_l)} \\
\end{align}\]</span>
</center>
<p>This gets us to the nice, neat expression in <a href="https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12_toc.pdf#%5B%7B%22num%22%3A195%2C%22gen%22%3A0%7D%2C%7B%22name%22%3A%22Fit%22%7D%5D" target="_blank">Elements of Statistical Learning</a>, but for arriving at our discriminant functions it’s easier to stay with the fully-expanded-without-consolidating expression. From this point on we will work from the following expression we intermediately arrived at above:</p>
<center>
<span class="math display">\[\log\frac{\Pr(Y = k|X = x)}{\Pr(Y = l|X = x)} = \log{\frac{\pi_k}{\pi_l}} + x^T \Sigma^{-1} \mu_k - x^T \Sigma^{-1} \mu_l - \frac{1}{2} \mu_k^T \Sigma^{-1} \mu_k + \frac{1}{2} \mu_l^T \Sigma^{-1} \mu_l\]</span>
</center>
<p>Just like in the univariate case, all we need to do to get the discriminant functions is evaluate this ratio on the pairwise decision boundary where our probability ratio is 1 and so the log of the ratio is 0. Doing this, we can simply rearrange the expression to get our discriminant functions for classes <span class="math inline">\(k\)</span> and <span class="math inline">\(l\)</span></p>
<center>
<span class="math display">\[\begin{align}
0 &amp;= \log{\pi_k} - \log{\pi_l} + x^T \Sigma^{-1} \mu_k - x^T \Sigma^{-1} \mu_l - \frac{1}{2} \mu_k^T \Sigma^{-1} \mu_k + \frac{1}{2} \mu_l^T \Sigma^{-1} \mu_l \\
\underbrace{\log{\pi_l} + x^T \Sigma^{-1} \mu_l - \frac{1}{2} \mu_l^T \Sigma^{-1} \mu_l}_{\delta_l (x)} &amp;= \underbrace{\log{\pi_k} + x^T \Sigma^{-1} \mu_k - \frac{1}{2} \mu_k^T \Sigma^{-1} \mu_k}_{\delta_k (x)} \\
\end{align}\]</span>
</center>
<p>Again, just like in the univariate case, we evaluate the discriminant functions for each class for some observation’s vector of values for X1 and X2, then assign the class to that observation for which the discriminant function evaluates largest.</p>
<p>Wait: isn’t this section supposed to be about the <em>Bayes</em> decision boundary? Why does this seem like almost the same process for building the <em>LDA</em> decision boundaries in the univariate example? Because <strong>LDA is essentially just using the Bayes classifier while making some parametric assumptions about the data and assuming that the class variances (in the univariate case) or covariance matrices (in the multivariate case) are equal</strong>.</p>
</div>
<div id="implementation" class="section level3">
<h3>Implementation</h3>
<p>We can easily build our class-wise discriminant functions. Since we also know the true parameters of the distributions we are sampling from we can create the <em>optimal</em> decision boundaries.</p>
<p>First, we’ll construct vectors of our population means by class as well as the class-wise prior probabilities. In this example we know that the class-wise prior probabilities are equal, but we’ll calculate them each separately for thoroughness.</p>
<pre class="r"><code>pop_c0_mean_vec &lt;- c(pop_mean_c0_X1, pop_mean_c0_X2)
pop_c1_mean_vec &lt;- c(pop_mean_c1_X1, pop_mean_c1_X2)
pop_c2_mean_vec &lt;- c(pop_mean_c2_X1, pop_mean_c2_X2)

pop_c0_prior &lt;- n / (n * 3)
pop_c1_prior &lt;- n / (n * 3)
pop_c2_prior &lt;- n / (n * 3)</code></pre>
<p>Then we’ll build our three discriminant functions and <a href="https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/apply" target="_blank"><code>apply</code></a> them inside the decision rule for classification described above.</p>
<pre class="r"><code>d0_bayes &lt;- function(x_vec){
  t(x_vec) %*% solve(pop_sigma) %*% pop_c0_mean_vec -
    .5 * t(pop_c0_mean_vec) %*% solve(pop_sigma) %*% pop_c0_mean_vec +
    log(pop_c0_prior)
}

d1_bayes &lt;- function(x_vec){
  t(x_vec) %*% solve(pop_sigma) %*% pop_c1_mean_vec -
    .5 * t(pop_c1_mean_vec) %*% solve(pop_sigma) %*% pop_c1_mean_vec +
    log(pop_c1_prior)
}

d2_bayes &lt;- function(x_vec){
  t(x_vec) %*% solve(pop_sigma) %*% pop_c2_mean_vec -
    .5 * t(pop_c2_mean_vec) %*% solve(pop_sigma) %*% pop_c2_mean_vec +
    log(pop_c2_prior)
}

bayes_classifier &lt;- function(x_vec){
  score_c0 &lt;- d0_bayes(x_vec)
  score_c1 &lt;- d1_bayes(x_vec)
  score_c2 &lt;- d2_bayes(x_vec)
  
  if (score_c0 &gt; score_c1 &amp; score_c0 &gt; score_c2) {
    0
  } else if (score_c1 &gt; score_c0 &amp; score_c1 &gt; score_c2) {
    1
  } else {
    2
  }
}

train_sample_df$bayes_predicted_y &lt;- apply(train_sample_df[, c(&quot;X1&quot;, &quot;X2&quot;)], 
                                           1, bayes_classifier)</code></pre>
</div>
<div id="visualization" class="section level3">
<h3>Visualization</h3>
<p>Can we plot the decision boundaries? Absolutely, but it’s a little more involved than in the univariate case. How <em>do</em> we draw these boundary lines?</p>
<p>One obvious thing must be true: the decision boundary between two classes must pass through the midpoint between the centers of the two classes.</p>
<p>What determines the direction of the line crossing through this point? The boundary line is orthogonal<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> to <span class="math inline">\(\Sigma^{-1} (\mu_k - \mu_l)\)</span>. To get a vector orthogonal to <span class="math inline">\(\Sigma^{-1} (\mu_k - \mu_l)\)</span> we can use <code>MASS</code>’s <a href="https://www.rdocumentation.org/packages/MASS/versions/7.3-54/topics/Null" target="_blank"><code>Null</code></a> function.</p>
<pre class="r"><code># Calculate midpoints between class-level population mean-vectors
pop_c0_c1_midpoint &lt;- (pop_c0_mean_vec + pop_c1_mean_vec) / 2
pop_c0_c2_midpoint &lt;- (pop_c0_mean_vec + pop_c2_mean_vec) / 2
pop_c1_c2_midpoint &lt;- (pop_c1_mean_vec + pop_c2_mean_vec) / 2

# Generate orthogonal vectors
pop_c0_c1_ortho &lt;- Null(solve(pop_sigma) %*% 
                      (pop_c0_mean_vec - pop_c1_mean_vec))
pop_c0_c2_ortho &lt;- Null(solve(pop_sigma) %*% 
                      (pop_c0_mean_vec - pop_c2_mean_vec))
pop_c1_c2_ortho &lt;- Null(solve(pop_sigma) %*% 
                      (pop_c1_mean_vec - pop_c2_mean_vec))</code></pre>
<p>Then we can use these midpoints and orthogonal vectors to overlay the decision boundaries over the scatterplot we created above:</p>
<pre class="r"><code>train_sample_scatter &lt;- train_sample_scatter +
  geom_segment(aes(x = pop_c0_c1_midpoint[1] - 5 * pop_c0_c1_ortho[1],
                   xend = pop_c0_c1_midpoint[1] + 5 * pop_c0_c1_ortho[1],
                   y = pop_c0_c1_midpoint[2] - 5 * pop_c0_c1_ortho[2],
                   yend = pop_c0_c1_midpoint[2] + 5 * pop_c0_c1_ortho[2]),
               linetype = &quot;solid&quot;) +
  geom_segment(aes(x = pop_c0_c2_midpoint[1] - 5 * pop_c0_c2_ortho[1],
                   xend = pop_c0_c2_midpoint[1] + 5 * pop_c0_c2_ortho[1],
                   y = pop_c0_c2_midpoint[2] - 5 * pop_c0_c2_ortho[2],
                   yend = pop_c0_c2_midpoint[2] + 5 * pop_c0_c2_ortho[2]),
               linetype = &quot;solid&quot;) +
  geom_segment(aes(x = pop_c1_c2_midpoint[1] - 5 * pop_c1_c2_ortho[1],
                   xend = pop_c1_c2_midpoint[1] + 5 * pop_c1_c2_ortho[1],
                   y = pop_c1_c2_midpoint[2] - 5 * pop_c1_c2_ortho[2],
                   yend = pop_c1_c2_midpoint[2] + 5 * pop_c1_c2_ortho[2]),
               linetype = &quot;solid&quot;)</code></pre>
<p><img src="multivariate_LDA_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>Obviously the segments represented above are just that: segments. The true lines extend infinitely in both directions.</p>
</div>
</div>
<div id="the-lda-classifier-and-decision-boundaries" class="section level2">
<h2>The LDA Classifier and Decision Boundaries</h2>
<div id="theory-1" class="section level3">
<h3>Theory</h3>
<p>The theory behind the LDA classifier and its discriminant functions is exactly the same as the the theory behind the Bayes classifier (so if you skipped that section, go back), with one major exception in this example: in reality, we typically don’t know the covariance matrix, prior probabilities, and class-wise population means for our variables. Instead, we need to estimate them from the sample data.</p>
<p>Estimating the class-wise population means and prior probabilities is easy</p>
<center>
<span class="math display">\[\hat{\pi}_k = \frac{n_k}{n}\]</span>
</center>
<center>
<span class="math display">\[\hat{\mu}_k = \frac{\sum_{i = 1; y = k}^{n_k} x_i}{n_k}\]</span>
</center>
<p>where <span class="math inline">\(n\)</span> is the number of observations in the overall sample and so <span class="math inline">\(n_k\)</span> is the number of observations for class <span class="math inline">\(k\)</span> in the sample.</p>
<p>Estimating the common covariance matrix is less straightforward. We estimate this matrix by creating a weighted average of covariance matrices within each class with a <a href="https://en.wikipedia.org/wiki/Bessel%27s_correction" target="_blank">Bessel-like correction</a>. This is a lot like <a href="https://en.wikipedia.org/wiki/Pooled_variance" target="_blank">pooled variance</a>.</p>
<center>
<span class="math display">\[\hat{\Sigma} = \frac{\sum_{k = 1}^K \sum_{i = 1; y = k}^{n_k} (x_i - \hat{\mu}_k)(x_i - \hat{\mu}_k)^T}{n - K}\]</span>
</center>
<p><span class="math inline">\(K\)</span> here is the total number of classes.</p>
<p>Because we are still assuming that all classes have the same covariance matrix (in this case, the estimated matrix <span class="math inline">\(\hat{\Sigma}\)</span>) none of the derivations in the Bayes classifier theory section are different: we are simply substituting the “true” means and covariance matrix in the Bayes classifier with our estimates since we usually cannot know the “true” parameters.</p>
</div>
<div id="implementation-1" class="section level3">
<h3>Implementation</h3>
<p>Like before, first we’ll construct vectors of our <em>sample</em> means by class as well as the class-wise prior probabilities. Also like before, in this example we know that the class-wise prior probabilities are equal, but we’ll calculate estimates of each separately for thoroughness. Depending on your sample, your class samples obviously may not be same size.</p>
<pre class="r"><code>sample_c0_mean_vec &lt;- c(mean(train_sample_df$X1[train_sample_df$y == 0]), 
                        mean(train_sample_df$X2[train_sample_df$y == 0]))
sample_c1_mean_vec &lt;- c(mean(train_sample_df$X1[train_sample_df$y == 1]), 
                        mean(train_sample_df$X2[train_sample_df$y == 1]))
sample_c2_mean_vec &lt;- c(mean(train_sample_df$X1[train_sample_df$y == 2]), 
                        mean(train_sample_df$X2[train_sample_df$y == 2]))

sample_c0_prior &lt;- nrow(train_sample_df[train_sample_df$y == 0,]) /
  nrow(train_sample_df)
sample_c1_prior &lt;- nrow(train_sample_df[train_sample_df$y == 1,]) /
  nrow(train_sample_df)
sample_c2_prior &lt;- nrow(train_sample_df[train_sample_df$y == 2,]) /
  nrow(train_sample_df)</code></pre>
<p>To build <span class="math inline">\(\hat{\Sigma}\)</span> we will first de-mean the X1 and X2 vectors class-wise. We will do this explicitly and add<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> these de-meaned versions of the X1 and X2 variables to the data tibble rather than de-mean the variables within a loop for conceptual simplicity.</p>
<pre class="r"><code>sample_mean_c0_X1 &lt;- mean(train_sample_df$X1[train_sample_df$y == 0])
sample_mean_c0_X2 &lt;- mean(train_sample_df$X2[train_sample_df$y == 0])
sample_mean_c1_X1 &lt;- mean(train_sample_df$X1[train_sample_df$y == 1])
sample_mean_c1_X2 &lt;- mean(train_sample_df$X2[train_sample_df$y == 1])
sample_mean_c2_X1 &lt;- mean(train_sample_df$X1[train_sample_df$y == 2])
sample_mean_c2_X2 &lt;- mean(train_sample_df$X2[train_sample_df$y == 2])

train_sample_df &lt;- train_sample_df %&gt;%
  mutate(centered_X1_sample = if_else(y == 0, X1 - sample_mean_c0_X1,
                                   if_else(y == 1, X1 - sample_mean_c1_X1,
                                           X1 - sample_mean_c2_X1)),
         centered_X2_sample = if_else(y == 0, X2 - sample_mean_c0_X2,
                                   if_else(y == 1, X2 - sample_mean_c1_X2,
                                           X2 - sample_mean_c2_X2)))</code></pre>
<p>Then we can build our estimated covariance matrix with a simple for loop and make the correction afterward:</p>
<pre class="r"><code># Zero-matrix of correct dimensions to build over
train_sigma_LDA &lt;- matrix(c(0, 0, 
                            0, 0), 2, 2)

for (i in 1:nrow(train_sample_df)) {
  
  temp_cov &lt;- c(train_sample_df$centered_X1_sample[i], train_sample_df$centered_X2_sample[i])
  
  train_sigma_LDA &lt;- train_sigma_LDA + temp_cov %*% t(temp_cov)
  
}

# Correct the covariance matrix
train_sigma_LDA &lt;- train_sigma_LDA / (n * 3 - 3)</code></pre>
<p>How close is our <span class="math inline">\(\hat{\Sigma}\)</span> to <span class="math inline">\(\Sigma\)</span>?</p>
<pre><code>## [1] &quot;Sigma&quot;</code></pre>
<pre><code>##            [,1]       [,2]
## [1,] 2.37429323 0.03586445
## [2,] 0.03586445 2.37429323</code></pre>
<pre><code>## [1] &quot;Estimated sigma&quot;</code></pre>
<pre><code>##              [,1]         [,2]
## [1,]  2.448339786 -0.003762778
## [2,] -0.003762778  2.323354480</code></pre>
<p>Then, exactly like we did with the Bayes classifier, we build the LDA classifier by creating our discriminant functions and <code>apply</code> them via a decision rule function to our training sample:</p>
<pre class="r"><code>d0_LDA &lt;- function(x_vec){
  t(x_vec) %*% solve(train_sigma_LDA) %*% sample_c0_mean_vec -
    .5 * t(sample_c0_mean_vec) %*% solve(train_sigma_LDA) %*% 
    sample_c0_mean_vec + log(sample_c0_prior)
}

d1_LDA &lt;- function(x_vec){
  t(x_vec) %*% solve(train_sigma_LDA) %*% sample_c1_mean_vec -
    .5 * t(sample_c1_mean_vec) %*% solve(train_sigma_LDA) %*% 
    sample_c1_mean_vec + log(sample_c1_prior)
}

d2_LDA &lt;- function(x_vec){
  t(x_vec) %*% solve(train_sigma_LDA) %*% sample_c2_mean_vec -
    .5 * t(sample_c2_mean_vec) %*% solve(train_sigma_LDA) %*% 
    sample_c2_mean_vec + log(sample_c2_prior)
}

LDA_classifier &lt;- function(x_vec){
  score_c0 &lt;- d0_LDA(x_vec)
  score_c1 &lt;- d1_LDA(x_vec)
  score_c2 &lt;- d2_LDA(x_vec)
  
  if (score_c0 &gt; score_c1 &amp; score_c0 &gt; score_c2) {
    0
  } else if (score_c1 &gt; score_c0 &amp; score_c1 &gt; score_c2) {
    1
  } else {
    2
  }
}

train_sample_df$LDA_predicted_y &lt;- apply(train_sample_df[, c(&quot;X1&quot;, &quot;X2&quot;)], 
                                           1, LDA_classifier)</code></pre>
</div>
<div id="visualization-1" class="section level3">
<h3>Visualization</h3>
<p>We can then overlay these lines on our existing scatterplot just like we did with the Bayes decision boundaries. Again, we will calculate pairwise class midpoints with the <em>sample</em> data and use <code>Null</code> to generate vectors orthogonal to <span class="math inline">\(\hat{\Sigma} (\hat{\mu}_k - \hat{\mu}_l)\)</span>. This time we’ll plot the LDA decision boundaries as dashed lines.</p>
<pre class="r"><code># Calculate midpoints between class-level sample mean-vectors
sample_c0_c1_midpoint &lt;- (sample_c0_mean_vec + sample_c1_mean_vec) / 2
sample_c0_c2_midpoint &lt;- (sample_c0_mean_vec + sample_c2_mean_vec) / 2
sample_c1_c2_midpoint &lt;- (sample_c1_mean_vec + sample_c2_mean_vec) / 2

# Generate the sample orthogonal vectors for plotting
sample_c0_c1_ortho &lt;- Null(solve(train_sigma_LDA) %*% 
                             (sample_c0_mean_vec - sample_c1_mean_vec))
sample_c0_c2_ortho &lt;- Null(solve(train_sigma_LDA) %*% 
                             (sample_c0_mean_vec - sample_c2_mean_vec))
sample_c1_c2_ortho &lt;- Null(solve(train_sigma_LDA) %*% 
                             (sample_c1_mean_vec - sample_c2_mean_vec))

# Overlay scatterplot with LDA boundaries
train_sample_scatter &lt;- train_sample_scatter +
  geom_segment(aes(x = sample_c0_c1_midpoint[1] - 5 * sample_c0_c1_ortho[1],
                   xend = sample_c0_c1_midpoint[1] + 5 * sample_c0_c1_ortho[1],
                   y = sample_c0_c1_midpoint[2] - 5 * sample_c0_c1_ortho[2],
                   yend = sample_c0_c1_midpoint[2] + 5 * sample_c0_c1_ortho[2]),
               linetype = &quot;dashed&quot;) +
  geom_segment(aes(x = sample_c0_c2_midpoint[1] - 5 * sample_c0_c2_ortho[1],
                   xend = sample_c0_c2_midpoint[1] + 5 * sample_c0_c2_ortho[1],
                   y = sample_c0_c2_midpoint[2] - 5 * sample_c0_c2_ortho[2],
                   yend = sample_c0_c2_midpoint[2] + 5 * sample_c0_c2_ortho[2]),
               linetype = &quot;dashed&quot;) +
  geom_segment(aes(x = sample_c1_c2_midpoint[1] - 5 * sample_c1_c2_ortho[1],
                   xend = sample_c1_c2_midpoint[1] + 5 * sample_c1_c2_ortho[1],
                   y = sample_c1_c2_midpoint[2] - 5 * sample_c1_c2_ortho[2],
                   yend = sample_c1_c2_midpoint[2] + 5 * sample_c1_c2_ortho[2]),
               linetype = &quot;dashed&quot;) +
  labs(caption = &quot;Bayes classifier boundaries are solid; LDA boundaries are dashed&quot;)</code></pre>
<p><img src="multivariate_LDA_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
</div>
</div>
<div id="testing" class="section level2">
<h2>Testing</h2>
<p>Now that we have functions built for the Bayes classifier and LDA classifier, we can easily <code>apply</code> them to the test data just as we did with the training data.</p>
<pre class="r"><code>test_sample_df$bayes_predicted_y &lt;- apply(test_sample_df[, c(&quot;X1&quot;, &quot;X2&quot;)], 
                                           1, bayes_classifier)

test_sample_df$LDA_predicted_y &lt;- apply(test_sample_df[, c(&quot;X1&quot;, &quot;X2&quot;)], 
                                         1, LDA_classifier)</code></pre>
<p>What does the misclassification rate for LDA look like?</p>
<pre class="r"><code>LDA_misclass_rate &lt;- nrow(test_sample_df[test_sample_df$y != 
                                           test_sample_df$LDA_predicted_y,]) /
  nrow(test_sample_df)

LDA_misclass_rate * 100</code></pre>
<pre><code>## [1] 6.888889</code></pre>
<p>How about the misclassification rate for our (optimal) Bayes classifier?</p>
<pre class="r"><code>bayes_misclass_rate &lt;- nrow(test_sample_df[test_sample_df$y != 
                                           test_sample_df$bayes_predicted_y,]) /
  nrow(test_sample_df)

bayes_misclass_rate * 100</code></pre>
<pre><code>## [1] 6.666667</code></pre>
<p>In this case, the LDA classifier performs well: it’s very close to the Bayes classifier. As noted in the univariate case, it’s of course possible for LDA to “outperform” the optimal Bayes classifier depending on the train and test samples, but the Bayes classifier here represents the “true” optimal classifier in a sense since its classifying based on population parameters.</p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>See <a href="https://stats.stackexchange.com/questions/92157/compute-and-graph-the-lda-decision-boundary/103552#103552" target="_blank">this excellent CV answer</a> for explanations why.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>See <a href="https://dplyr.tidyverse.org/reference/mutate.html" target="_blank"><code>mutate</code></a>.<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
</ol>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
