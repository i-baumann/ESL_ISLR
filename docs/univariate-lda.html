<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Univariate LDA | ESL &amp; ISLR Working Examples</title>
  <meta name="description" content="Univariate LDA | ESL &amp; ISLR Working Examples" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Univariate LDA | ESL &amp; ISLR Working Examples" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://my-cabbages.github.io/ESL_ISLR/" />
  <meta property="og:image" content="https://my-cabbages.github.io/ESL_ISLR//./_bookdown_files/multivariate_QDA_files/figure-html" />
  
  <meta name="github-repo" content="my-cabbages/ESL_ISLR/" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Univariate LDA | ESL &amp; ISLR Working Examples" />
  
  
  <meta name="twitter:image" content="https://my-cabbages.github.io/ESL_ISLR//./_bookdown_files/multivariate_QDA_files/figure-html" />

<meta name="author" content="Isaac Baumann" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="linear-discriminant-analysis.html"/>
<link rel="next" href="multivariate-lda.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/clipboard-2.0.6/clipboard.min.js"></script>
<link href="libs/xaringanExtra-clipboard-0.2.6/xaringanExtra-clipboard.css" rel="stylesheet" />
<script src="libs/xaringanExtra-clipboard-0.2.6/xaringanExtra-clipboard.js"></script>
<script>window.xaringanExtraClipboard(null, {"button":"<i class=\"fa fa-clipboard\"><\/i>","success":"<i class=\"fa fa-check\" style=\"color: #90BE6D\"><\/i>","error":"Press Ctrl+C to Copy"})</script>
<link href="libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  background-color: #f8f8f8; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">ESL & ISLR Working Examples</a></li>

<li class="divider"></li>
<li><a href="index.html#section"></a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a>
<ul>
<li class="chapter" data-level="" data-path="packages.html"><a href="packages.html"><i class="fa fa-check"></i>Packages</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i>Logistic Regression</a>
<ul>
<li class="chapter" data-level="" data-path="data-generation.html"><a href="data-generation.html"><i class="fa fa-check"></i>Data Generation</a></li>
<li class="chapter" data-level="" data-path="the-log-likelihood-surface.html"><a href="the-log-likelihood-surface.html"><i class="fa fa-check"></i>The Log-Likelihood Surface</a></li>
<li class="chapter" data-level="" data-path="implementation.html"><a href="implementation.html"><i class="fa fa-check"></i>Implementation</a>
<ul>
<li class="chapter" data-level="" data-path="implementation.html"><a href="implementation.html#method-1-newton-raphsonirls"><i class="fa fa-check"></i>Method 1: Newton-Raphson/IRLS</a></li>
<li class="chapter" data-level="" data-path="implementation.html"><a href="implementation.html#method-2-modified-irls"><i class="fa fa-check"></i>Method 2: Modified IRLS</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="testing.html"><a href="testing.html"><i class="fa fa-check"></i>Testing</a>
<ul>
<li><a href="testing.html#comparing-to-glm">Comparing to <code>glm</code></a></li>
<li class="chapter" data-level="" data-path="testing.html"><a href="testing.html#the-log-likelihood-surface-1"><i class="fa fa-check"></i>The Log-Likelihood Surface</a></li>
<li class="chapter" data-level="" data-path="testing.html"><a href="testing.html#plotting-the-logistic-fit"><i class="fa fa-check"></i>Plotting the Logistic Fit</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="linear-discriminant-analysis.html"><a href="linear-discriminant-analysis.html"><i class="fa fa-check"></i>Linear Discriminant Analysis</a>
<ul>
<li class="chapter" data-level="" data-path="univariate-lda.html"><a href="univariate-lda.html"><i class="fa fa-check"></i>Univariate LDA</a>
<ul>
<li class="chapter" data-level="" data-path="univariate-lda.html"><a href="univariate-lda.html#theory"><i class="fa fa-check"></i>Theory</a></li>
<li class="chapter" data-level="" data-path="univariate-lda.html"><a href="univariate-lda.html#setup"><i class="fa fa-check"></i>Setup</a></li>
<li class="chapter" data-level="" data-path="univariate-lda.html"><a href="univariate-lda.html#data-generation-1"><i class="fa fa-check"></i>Data Generation</a></li>
<li class="chapter" data-level="" data-path="univariate-lda.html"><a href="univariate-lda.html#implementation-1"><i class="fa fa-check"></i>Implementation</a></li>
<li class="chapter" data-level="" data-path="univariate-lda.html"><a href="univariate-lda.html#testing-1"><i class="fa fa-check"></i>Testing</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="multivariate-lda.html"><a href="multivariate-lda.html"><i class="fa fa-check"></i>Multivariate LDA</a>
<ul>
<li class="chapter" data-level="" data-path="multivariate-lda.html"><a href="multivariate-lda.html#theory-1"><i class="fa fa-check"></i>Theory</a></li>
<li class="chapter" data-level="" data-path="multivariate-lda.html"><a href="multivariate-lda.html#setup-1"><i class="fa fa-check"></i>Setup</a></li>
<li class="chapter" data-level="" data-path="multivariate-lda.html"><a href="multivariate-lda.html#data-generation-2"><i class="fa fa-check"></i>Data Generation</a></li>
<li class="chapter" data-level="" data-path="multivariate-lda.html"><a href="multivariate-lda.html#implementation-2"><i class="fa fa-check"></i>Implementation</a></li>
<li class="chapter" data-level="" data-path="multivariate-lda.html"><a href="multivariate-lda.html#testing-2"><i class="fa fa-check"></i>Testing</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="quadratic-discriminant-analysis.html"><a href="quadratic-discriminant-analysis.html"><i class="fa fa-check"></i>Quadratic Discriminant Analysis</a>
<ul>
<li class="chapter" data-level="" data-path="quadratic-discriminant-analysis.html"><a href="quadratic-discriminant-analysis.html#theory-2"><i class="fa fa-check"></i>Theory</a></li>
<li class="chapter" data-level="" data-path="data-generation-3.html"><a href="data-generation-3.html"><i class="fa fa-check"></i>Data Generation</a></li>
<li class="chapter" data-level="" data-path="implementation-3.html"><a href="implementation-3.html"><i class="fa fa-check"></i>Implementation</a>
<ul>
<li class="chapter" data-level="" data-path="implementation-3.html"><a href="implementation-3.html#the-bayes-classifier-and-decision-boundaries-1"><i class="fa fa-check"></i>The Bayes Classifier and Decision Boundaries</a></li>
<li class="chapter" data-level="" data-path="implementation-3.html"><a href="implementation-3.html#the-qda-classifier-and-decision-boundaries"><i class="fa fa-check"></i>The QDA Classifier and Decision Boundaries</a></li>
<li class="chapter" data-level="" data-path="implementation-3.html"><a href="implementation-3.html#visualizing-the-decision-boundaries"><i class="fa fa-check"></i>Visualizing the Decision Boundaries</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="testing-3.html"><a href="testing-3.html"><i class="fa fa-check"></i>Testing</a>
<ul>
<li class="chapter" data-level="" data-path="testing-3.html"><a href="testing-3.html#visualization"><i class="fa fa-check"></i>Visualization</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="ridge-regression.html"><a href="ridge-regression.html"><i class="fa fa-check"></i>Ridge Regression</a>
<ul>
<li class="chapter" data-level="" data-path="theory-3.html"><a href="theory-3.html"><i class="fa fa-check"></i>Theory</a>
<ul>
<li class="chapter" data-level="" data-path="theory-3.html"><a href="theory-3.html#minimizing-rss-ols"><i class="fa fa-check"></i>Minimizing RSS: OLS</a></li>
<li class="chapter" data-level="" data-path="theory-3.html"><a href="theory-3.html#minimizing-rss-ridge"><i class="fa fa-check"></i>Minimizing RSS: Ridge</a></li>
<li class="chapter" data-level="" data-path="theory-3.html"><a href="theory-3.html#important-features"><i class="fa fa-check"></i>Important Features</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="case-1-linearly-dependent-variables.html"><a href="case-1-linearly-dependent-variables.html"><i class="fa fa-check"></i>Case 1: Linearly Dependent Variables</a>
<ul>
<li class="chapter" data-level="" data-path="case-1-linearly-dependent-variables.html"><a href="case-1-linearly-dependent-variables.html#data-generation-4"><i class="fa fa-check"></i>Data Generation</a></li>
<li class="chapter" data-level="" data-path="case-1-linearly-dependent-variables.html"><a href="case-1-linearly-dependent-variables.html#the-problem-ols"><i class="fa fa-check"></i>The Problem: OLS</a></li>
<li class="chapter" data-level="" data-path="case-1-linearly-dependent-variables.html"><a href="case-1-linearly-dependent-variables.html#a-solution-ridge-regression"><i class="fa fa-check"></i>A Solution: Ridge Regression</a></li>
<li><a href="case-1-linearly-dependent-variables.html#solving-for-hatbetaridge">Solving for <span class="math inline">\(\hat{\beta}^{Ridge}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="" data-path="case-2-correlated-independent-variables.html"><a href="case-2-correlated-independent-variables.html"><i class="fa fa-check"></i>Case 2: Correlated Independent Variables</a>
<ul>
<li class="chapter" data-level="" data-path="case-2-correlated-independent-variables.html"><a href="case-2-correlated-independent-variables.html#data"><i class="fa fa-check"></i>Data</a></li>
<li class="chapter" data-level="" data-path="case-2-correlated-independent-variables.html"><a href="case-2-correlated-independent-variables.html#exploratory-analysis"><i class="fa fa-check"></i>Exploratory Analysis</a></li>
<li class="chapter" data-level="" data-path="case-2-correlated-independent-variables.html"><a href="case-2-correlated-independent-variables.html#implementation-4"><i class="fa fa-check"></i>Implementation</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="standardization.html"><a href="standardization.html"><i class="fa fa-check"></i>Standardization</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">ESL &amp; ISLR Working Examples</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="univariate-lda" class="section level2">
<h2>Univariate LDA</h2>
<p>This example walks through using linear discriminant analysis to classify observations in a two-class univariate setting with (idealized) generated data.</p>
<div id="theory" class="section level3">
<h3>Theory</h3>
<p>The LDA classifier uses estimates of mean and variance for each class as well as discriminant functions to determine the probability that an observation is of a particular class. The LDA classifier then assigns to each observation that class for which the estimated probability of membership is highest.</p>
<p><strong>LDA assumes equal variance between classes</strong>. The theory discussion and example reflect this.</p>
<div id="bayes-theorem" class="section level4">
<h4>Bayes’ Theorem</h4>
<p>The general LDA model explored here and in <a href="https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12_toc.pdf#%5B%7B%22num%22%3A195%2C%22gen%22%3A0%7D%2C%7B%22name%22%3A%22Fit%22%7D%5D" target="_blank">Elements of Statistical Learning</a> is based on <a href="https://en.wikipedia.org/wiki/Bayes%27_theorem#For_continuous_random_variables" target="_blank">Bayes’ theorem for continuous variables</a>. Since we have normal data, we use the normal probability density function (for each class, respectively) as our probability function. Taking Bayes’ theorem</p>
<center>
<span class="math display">\[\Pr(Y = k|X = x) = \frac{\pi_k f_k(x)}{\sum^K_{j = 1} \pi_j f_j(x)}\]</span>
</center>
<p>where <span class="math inline">\(k\)</span> represents one class and <span class="math inline">\(j\)</span> all others, <span class="math inline">\(\pi_k\)</span> represents the prior probability of class <span class="math inline">\(k\)</span>, and <span class="math inline">\(f(\cdot)\)</span> is some probability function, we substitute the normal PDF for <span class="math inline">\(f(\cdot)\)</span> and rewrite</p>
<center>
<span class="math display">\[\Pr(Y = k|X = x) = \frac{\pi_k \frac{1}{\sqrt{2 \color{red}{\pi} \sigma_k}} e^{ \left( \frac{-1}{2\sigma^2_k}(x - \mu_k)^2 \right)}}{\sum^K_{j = 1} \pi_j \frac{1}{\sqrt{2 \color{red}{\pi} \sigma_j}} e^{ \left( \frac{-1}{2\sigma^2_j}(x - \mu_j)^2 \right)}}\]</span>
</center>
<p>where <span class="math inline">\(\color{red}{\pi}\)</span> is literally the value pi (as used in the normal PDF), not a prior probability.</p>
<p>In this example, we only have two classes and we will set <span class="math inline">\(\sigma_k = \sigma_l\)</span>.</p>
<p>A <em>Bayes decision boundary</em> is a boundary between classes along which the classifier determines the probability of class membership to be equal, i.e. <span class="math inline">\(\Pr(Y = k|X = x) = \Pr(Y = l|X = x)\)</span>.</p>
</div>
<div id="discriminant-functions" class="section level4">
<h4>Discriminant Functions</h4>
<p>So where does the “discriminant” in LDA come from? How does LDA divide between classes? In the two-class case like this example, we can use the log of the ratio of the probabilities to get our <em>discriminant functions</em> for each class. Taking the ratio of the probabilities…</p>
<center>
<span class="math display">\[\begin{align}
\frac{\Pr(Y = k|X = x)}{\Pr(Y = l|X = x)} &amp;= \frac{\pi_k f_k(x)}{\sum^K_{j = 1} \pi_j f_j(x)} \frac{\sum^K_{j = 1} \pi_j f_j(x)}{\pi_l f_l(x)} \\ 
&amp;= \frac{\pi_k f_k(x)}{\pi_l f_l(x)}
\end{align}\]</span>
</center>
<p>…and taking the log of the ratio…</p>
<center>
<span class="math display">\[\begin{align}
\log\frac{\Pr(Y = k|X = x)}{\Pr(Y = l|X = x)} &amp;= \log \frac{\pi_k f_k(x)}{\pi_l f_l(x)} \\
&amp;= \log{\frac{\pi_k}{\pi_l}} + \log{\frac{f_k(x)}{f_l(x)}}
\end{align}\]</span>
</center>
<p>…we can build a general formula for finding our discriminant functions. Plugging in the normal PDF for <span class="math inline">\(f(\cdot)\)</span> in this case allows for quite a lot of simplification.</p>
<center>
<span class="math display">\[\begin{align}
\log\frac{\Pr(Y = k|X = x)}{\Pr(Y = l|X = x)} &amp;= \log{\frac{\pi_k}{\pi_l}} + \log \frac{\frac{1}{\sqrt{2 \color{red}{\pi} \sigma}} e^{ \left( \frac{-1}{2\sigma^2}(x - \mu_k)^2 \right)}}{\frac{1}{\sqrt{2 \color{red}{\pi} \sigma}} e^{ \left( \frac{-1}{2\sigma^2}(x - \mu_l)^2 \right)}} \\
&amp;= \log{\frac{\pi_k}{\pi_l}} + \log \left( \frac{1}{\sqrt{2 \color{red}{\pi} \sigma}} \bigg/ \frac{1}{\sqrt{2 \color{red}{\pi} \sigma}} \right) + \log \frac{e^{ \left( \frac{-1}{2\sigma^2}(x - \mu_k)^2 \right)}}{e^{ \left( \frac{-1}{2\sigma^2}(x - \mu_l)^2 \right)}} \\
&amp;= \log{\frac{\pi_k}{\pi_l}} + \log e^{ \left( \frac{-1}{2\sigma^2}(x - \mu_k)^2 \right)} - \log e^{ \left( \frac{-1}{2\sigma^2}(x - \mu_l)^2 \right)} \\
&amp;= \log{\frac{\pi_k}{\pi_l}} - \frac{1}{2\sigma^2}(x - \mu_k)^2 + \frac{1}{2\sigma^2}(x - \mu_l)^2 \\
&amp;= \log{\frac{\pi_k}{\pi_l}} + \frac{-x^2 + 2x \mu_k - \mu_k^2 + x^2 - 2x \mu_l + \mu_l^2}{2 \sigma^2} \\
&amp;= \log{\frac{\pi_k}{\pi_l}} + x \frac{\mu_k - \mu_l}{\sigma^2} + \frac{\mu_l^2 - \mu_k^2}{2 \sigma^2}
\end{align}\]</span>
</center>
<p>Remember the Bayes decision boundary? The LDA decision boundary also exists where the probability of an observation being class <span class="math inline">\(k\)</span> is equal to its probability of being class <span class="math inline">\(l\)</span> in the two-class case, but using the sample data. If we think of our log-ratio in this way (in which case the ratio is 1 and so the log of the ratio is 0) and evaluate on the boundary then we can easily use the above expression to find our discriminant functions.</p>
<center>
<span class="math display">\[\Pr(Y = k|X = x) = \Pr(Y = l|X = x) \rightarrow \frac{\Pr(Y = k|X = x)}{\Pr(Y = l|X = x)} = 1\]</span>
<span class="math display">\[\begin{align}
\log \left( 1 \right) &amp;= \log{\frac{\pi_k}{\pi_l}} + x \frac{\mu_k - \mu_l}{\sigma^2} + \frac{\mu_l^2 - \mu_k^2}{2 \sigma^2} \\
0 &amp;= \log{\pi_k} - \log{\pi_l} + \frac{x \mu_k}{\sigma^2} - \frac{x \mu_l}{\sigma^2} + \frac{\mu_l^2}{2 \sigma^2} - \frac{\mu_k^2}{2 \sigma^2} \\
\underbrace{\log \pi_l + \frac{x \mu_l}{\sigma^2} - \frac{\mu_l^2}{2 \sigma^2}}_{\delta_l(x)} &amp;= \underbrace{\log \pi_k + \frac{x \mu_k}{\sigma^2} - \frac{\mu_k^2}{2 \sigma^2}}_{\delta_k (x)}
\end{align}\]</span>
</center>
<p><span class="math inline">\(\delta_k(x)\)</span> and <span class="math inline">\(\delta_l(x)\)</span> are our discriminant functions which we use to classify observations. We use a simple decision rule for classification: if for observation <span class="math inline">\(x_i\)</span> the <span class="math inline">\(k\)</span> discriminant function evaluates greater than the <span class="math inline">\(l\)</span> discriminant function then we assign <span class="math inline">\(x_i\)</span> to class <span class="math inline">\(k\)</span>.</p>
</div>
</div>
<div id="setup" class="section level3">
<h3>Setup</h3>
<p>This example uses the <a href="https://cran.r-project.org/web/packages/tidyverse/index.html" target="_blank"><code>tidyverse</code></a> and <a href="https://patchwork.data-imaginist.com/" target="_blank"><code>patchwork</code></a> packages.</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="univariate-lda.html#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb28-2"><a href="univariate-lda.html#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(patchwork)</span></code></pre></div>
</div>
<div id="data-generation-1" class="section level3">
<h3>Data Generation</h3>
<p>We’ll generate train and test data for two classes (coded 0/1): 1,000 normally-distributed observations for each class with differing means for each class but equal variances.</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="univariate-lda.html#cb29-1" aria-hidden="true" tabindex="-1"></a>means <span class="ot">&lt;-</span> <span class="fu">sample.int</span>(<span class="dv">6</span>, <span class="dv">2</span>, <span class="at">replace =</span> <span class="cn">FALSE</span>)</span>
<span id="cb29-2"><a href="univariate-lda.html#cb29-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-3"><a href="univariate-lda.html#cb29-3" aria-hidden="true" tabindex="-1"></a>population0_mean <span class="ot">&lt;-</span> <span class="fu">min</span>(means)</span>
<span id="cb29-4"><a href="univariate-lda.html#cb29-4" aria-hidden="true" tabindex="-1"></a>population0_sd <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb29-5"><a href="univariate-lda.html#cb29-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-6"><a href="univariate-lda.html#cb29-6" aria-hidden="true" tabindex="-1"></a>population1_mean <span class="ot">&lt;-</span> <span class="fu">max</span>(means)</span>
<span id="cb29-7"><a href="univariate-lda.html#cb29-7" aria-hidden="true" tabindex="-1"></a>population1_sd <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb29-8"><a href="univariate-lda.html#cb29-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-9"><a href="univariate-lda.html#cb29-9" aria-hidden="true" tabindex="-1"></a>c0_train <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">y =</span> <span class="dv">0</span>,</span>
<span id="cb29-10"><a href="univariate-lda.html#cb29-10" aria-hidden="true" tabindex="-1"></a>                   <span class="at">x =</span> <span class="fu">rnorm</span>(<span class="dv">1000</span>, </span>
<span id="cb29-11"><a href="univariate-lda.html#cb29-11" aria-hidden="true" tabindex="-1"></a>                             <span class="at">mean =</span> population0_mean, </span>
<span id="cb29-12"><a href="univariate-lda.html#cb29-12" aria-hidden="true" tabindex="-1"></a>                             <span class="at">sd =</span> population0_sd))</span>
<span id="cb29-13"><a href="univariate-lda.html#cb29-13" aria-hidden="true" tabindex="-1"></a>c1_train <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">y =</span> <span class="dv">1</span>,</span>
<span id="cb29-14"><a href="univariate-lda.html#cb29-14" aria-hidden="true" tabindex="-1"></a>                   <span class="at">x =</span> <span class="fu">rnorm</span>(<span class="dv">1000</span>,</span>
<span id="cb29-15"><a href="univariate-lda.html#cb29-15" aria-hidden="true" tabindex="-1"></a>                             <span class="at">mean =</span> population1_mean, </span>
<span id="cb29-16"><a href="univariate-lda.html#cb29-16" aria-hidden="true" tabindex="-1"></a>                             <span class="at">sd =</span> population1_sd))</span>
<span id="cb29-17"><a href="univariate-lda.html#cb29-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-18"><a href="univariate-lda.html#cb29-18" aria-hidden="true" tabindex="-1"></a>c0_test <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">y =</span> <span class="dv">0</span>,</span>
<span id="cb29-19"><a href="univariate-lda.html#cb29-19" aria-hidden="true" tabindex="-1"></a>                  <span class="at">x =</span> <span class="fu">rnorm</span>(<span class="dv">1000</span>, </span>
<span id="cb29-20"><a href="univariate-lda.html#cb29-20" aria-hidden="true" tabindex="-1"></a>                             <span class="at">mean =</span> population0_mean, </span>
<span id="cb29-21"><a href="univariate-lda.html#cb29-21" aria-hidden="true" tabindex="-1"></a>                             <span class="at">sd =</span> population0_sd))</span>
<span id="cb29-22"><a href="univariate-lda.html#cb29-22" aria-hidden="true" tabindex="-1"></a>c1_test <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">y =</span> <span class="dv">1</span>,</span>
<span id="cb29-23"><a href="univariate-lda.html#cb29-23" aria-hidden="true" tabindex="-1"></a>                  <span class="at">x =</span> <span class="fu">rnorm</span>(<span class="dv">1000</span>, </span>
<span id="cb29-24"><a href="univariate-lda.html#cb29-24" aria-hidden="true" tabindex="-1"></a>                             <span class="at">mean =</span> population1_mean, </span>
<span id="cb29-25"><a href="univariate-lda.html#cb29-25" aria-hidden="true" tabindex="-1"></a>                             <span class="at">sd =</span> population1_sd))</span>
<span id="cb29-26"><a href="univariate-lda.html#cb29-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-27"><a href="univariate-lda.html#cb29-27" aria-hidden="true" tabindex="-1"></a>train_sample_df <span class="ot">&lt;-</span> <span class="fu">bind_rows</span>(c0_train, c1_train)</span>
<span id="cb29-28"><a href="univariate-lda.html#cb29-28" aria-hidden="true" tabindex="-1"></a>test_sample_df <span class="ot">&lt;-</span> <span class="fu">bind_rows</span>(c0_test, c1_test)</span></code></pre></div>
<p>We can easily visualize the distributions of our class-wise populations:</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="univariate-lda.html#cb30-1" aria-hidden="true" tabindex="-1"></a>population_density <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(<span class="fu">data.frame</span>(<span class="at">x =</span> <span class="fu">c</span>(population0_mean <span class="sc">-</span> <span class="dv">3</span> <span class="sc">*</span> population0_sd,</span>
<span id="cb30-2"><a href="univariate-lda.html#cb30-2" aria-hidden="true" tabindex="-1"></a>                                              population1_mean <span class="sc">+</span> <span class="dv">3</span> <span class="sc">*</span> population1_sd)), </span>
<span id="cb30-3"><a href="univariate-lda.html#cb30-3" aria-hidden="true" tabindex="-1"></a>                             <span class="fu">aes</span>(x)) <span class="sc">+</span> </span>
<span id="cb30-4"><a href="univariate-lda.html#cb30-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stat_function</span>(<span class="at">fun =</span> dnorm, </span>
<span id="cb30-5"><a href="univariate-lda.html#cb30-5" aria-hidden="true" tabindex="-1"></a>                <span class="fu">aes</span>(<span class="at">color =</span> <span class="st">&quot;Class: 0&quot;</span>), <span class="at">size =</span> <span class="fl">1.25</span>,</span>
<span id="cb30-6"><a href="univariate-lda.html#cb30-6" aria-hidden="true" tabindex="-1"></a>                <span class="at">args =</span> <span class="fu">list</span>(<span class="at">mean =</span> population0_mean, <span class="at">sd =</span> population0_sd)) <span class="sc">+</span></span>
<span id="cb30-7"><a href="univariate-lda.html#cb30-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stat_function</span>(<span class="at">fun =</span> dnorm, </span>
<span id="cb30-8"><a href="univariate-lda.html#cb30-8" aria-hidden="true" tabindex="-1"></a>                <span class="fu">aes</span>(<span class="at">color =</span> <span class="st">&quot;Class: 1&quot;</span>), <span class="at">size =</span> <span class="fl">1.25</span>,</span>
<span id="cb30-9"><a href="univariate-lda.html#cb30-9" aria-hidden="true" tabindex="-1"></a>                <span class="at">args =</span> <span class="fu">list</span>(<span class="at">mean =</span> population1_mean, <span class="at">sd =</span> population1_sd)) <span class="sc">+</span></span>
<span id="cb30-10"><a href="univariate-lda.html#cb30-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_brewer</span>(<span class="at">palette =</span> <span class="st">&quot;Dark2&quot;</span>) <span class="sc">+</span></span>
<span id="cb30-11"><a href="univariate-lda.html#cb30-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Population Densities of Two Classes&quot;</span>) <span class="sc">+</span></span>
<span id="cb30-12"><a href="univariate-lda.html#cb30-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.title =</span> <span class="fu">element_blank</span>())</span>
<span id="cb30-13"><a href="univariate-lda.html#cb30-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-14"><a href="univariate-lda.html#cb30-14" aria-hidden="true" tabindex="-1"></a>population_density</span></code></pre></div>
<p><img src="LDA_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>Let’s also quickly visualize the train and test data:</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="univariate-lda.html#cb31-1" aria-hidden="true" tabindex="-1"></a>train_sample_histogram <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(train_sample_df) <span class="sc">+</span></span>
<span id="cb31-2"><a href="univariate-lda.html#cb31-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">fill =</span> <span class="fu">as.factor</span>(y)), </span>
<span id="cb31-3"><a href="univariate-lda.html#cb31-3" aria-hidden="true" tabindex="-1"></a>                 <span class="at">alpha =</span> .<span class="dv">7</span>,</span>
<span id="cb31-4"><a href="univariate-lda.html#cb31-4" aria-hidden="true" tabindex="-1"></a>                 <span class="at">position =</span> <span class="st">&quot;identity&quot;</span>) <span class="sc">+</span></span>
<span id="cb31-5"><a href="univariate-lda.html#cb31-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_brewer</span>(<span class="at">palette =</span> <span class="st">&quot;Dark2&quot;</span>,</span>
<span id="cb31-6"><a href="univariate-lda.html#cb31-6" aria-hidden="true" tabindex="-1"></a>                     <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">&quot;Class: 0&quot;</span>, <span class="st">&quot;Class: 1&quot;</span>)) <span class="sc">+</span></span>
<span id="cb31-7"><a href="univariate-lda.html#cb31-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Training Sample&quot;</span>) <span class="sc">+</span></span>
<span id="cb31-8"><a href="univariate-lda.html#cb31-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.title =</span> <span class="fu">element_blank</span>())</span>
<span id="cb31-9"><a href="univariate-lda.html#cb31-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-10"><a href="univariate-lda.html#cb31-10" aria-hidden="true" tabindex="-1"></a>test_sample_histogram <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(test_sample_df) <span class="sc">+</span></span>
<span id="cb31-11"><a href="univariate-lda.html#cb31-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">fill =</span> <span class="fu">as.factor</span>(y)), </span>
<span id="cb31-12"><a href="univariate-lda.html#cb31-12" aria-hidden="true" tabindex="-1"></a>                 <span class="at">alpha =</span> .<span class="dv">7</span>,</span>
<span id="cb31-13"><a href="univariate-lda.html#cb31-13" aria-hidden="true" tabindex="-1"></a>                 <span class="at">position =</span> <span class="st">&quot;identity&quot;</span>) <span class="sc">+</span></span>
<span id="cb31-14"><a href="univariate-lda.html#cb31-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_brewer</span>(<span class="at">palette =</span> <span class="st">&quot;Dark2&quot;</span>,</span>
<span id="cb31-15"><a href="univariate-lda.html#cb31-15" aria-hidden="true" tabindex="-1"></a>                     <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">&quot;Class: 0&quot;</span>, <span class="st">&quot;Class: 1&quot;</span>)) <span class="sc">+</span></span>
<span id="cb31-16"><a href="univariate-lda.html#cb31-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Test Sample&quot;</span>) <span class="sc">+</span></span>
<span id="cb31-17"><a href="univariate-lda.html#cb31-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.title =</span> <span class="fu">element_blank</span>())</span>
<span id="cb31-18"><a href="univariate-lda.html#cb31-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-19"><a href="univariate-lda.html#cb31-19" aria-hidden="true" tabindex="-1"></a>train_sample_histogram <span class="sc">+</span> test_sample_histogram</span></code></pre></div>
<p><img src="LDA_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
</div>
<div id="implementation-1" class="section level3">
<h3>Implementation</h3>
<div id="bayes-decision-boundary" class="section level4">
<h4>Bayes Decision Boundary</h4>
<p>We’ll use the <a href="https://en.wikipedia.org/wiki/Bayes_classifier" target="_blank">Bayes classifier</a> as a comparison for our LDA. As covered in the <a href="Theory">theory section</a> the Bayes classifier simply assigns an observation to the class for which an observation has the highest prior probability of belonging. The Bayes decision boundary is the boundary for which the probability of an observation being classified by the Bayes classifier is equal among classes; in this case, we will only have one boundary because we only have two classes.</p>
<p>We’ll compute the optimal Bayes decision boundary from the <em>population</em> data to compare our LDA against. In this case, since we only have two classes and one independent variable/predictor, it’s easy:</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="univariate-lda.html#cb32-1" aria-hidden="true" tabindex="-1"></a>bayes_decision <span class="ot">&lt;-</span> (population0_mean <span class="sc">^</span> <span class="dv">2</span> <span class="sc">-</span> population1_mean <span class="sc">^</span> <span class="dv">2</span>) <span class="sc">/</span></span>
<span id="cb32-2"><a href="univariate-lda.html#cb32-2" aria-hidden="true" tabindex="-1"></a>  (<span class="dv">2</span> <span class="sc">*</span> (population0_mean <span class="sc">-</span> population1_mean))</span></code></pre></div>
<p>We can easily add this optimal boundary to our population density plot and histogram. Unsurprisingly, the decision boundary lies where the two <a href="https://en.wikipedia.org/wiki/Probability_density_function" target="_blank">PDF</a>s meet:</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="univariate-lda.html#cb33-1" aria-hidden="true" tabindex="-1"></a>population_density <span class="sc">+</span> <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> bayes_decision,</span>
<span id="cb33-2"><a href="univariate-lda.html#cb33-2" aria-hidden="true" tabindex="-1"></a>                                <span class="at">color =</span> <span class="st">&quot;black&quot;</span>)</span></code></pre></div>
<p><img src="LDA_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="univariate-lda.html#cb34-1" aria-hidden="true" tabindex="-1"></a>train_sample_histogram <span class="sc">+</span> <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> bayes_decision,</span>
<span id="cb34-2"><a href="univariate-lda.html#cb34-2" aria-hidden="true" tabindex="-1"></a>                                <span class="at">color =</span> <span class="st">&quot;black&quot;</span>)</span></code></pre></div>
<p><img src="LDA_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
</div>
<div id="lda-decision-boundary" class="section level4">
<h4>LDA Decision Boundary</h4>
<p>For our estimates of the mean and variance we use the empirical (i.e., data/sample-derived) class-specific means and standard deviations. Even though the class variances are equivalent in this case we will treat them separately for thoroughness.</p>
<p>We also need to calculate the empirical prior probability that an observation belongs to each class. In this case, both will be 0.5 since we have equal samples but we will again calculate these separately for thoroughness.</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="univariate-lda.html#cb35-1" aria-hidden="true" tabindex="-1"></a>class_0_mean <span class="ot">&lt;-</span> <span class="fu">mean</span>(train_sample_df<span class="sc">$</span>x[train_sample_df<span class="sc">$</span>y <span class="sc">==</span> <span class="dv">0</span>])</span>
<span id="cb35-2"><a href="univariate-lda.html#cb35-2" aria-hidden="true" tabindex="-1"></a>class_0_var <span class="ot">&lt;-</span> <span class="fu">var</span>(train_sample_df<span class="sc">$</span>x[train_sample_df<span class="sc">$</span>y <span class="sc">==</span> <span class="dv">0</span>])</span>
<span id="cb35-3"><a href="univariate-lda.html#cb35-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-4"><a href="univariate-lda.html#cb35-4" aria-hidden="true" tabindex="-1"></a>class_1_mean <span class="ot">&lt;-</span> <span class="fu">mean</span>(train_sample_df<span class="sc">$</span>x[train_sample_df<span class="sc">$</span>y <span class="sc">==</span> <span class="dv">1</span>])</span>
<span id="cb35-5"><a href="univariate-lda.html#cb35-5" aria-hidden="true" tabindex="-1"></a>class_1_var <span class="ot">&lt;-</span> <span class="fu">var</span>(train_sample_df<span class="sc">$</span>x[train_sample_df<span class="sc">$</span>y <span class="sc">==</span> <span class="dv">1</span>])</span>
<span id="cb35-6"><a href="univariate-lda.html#cb35-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-7"><a href="univariate-lda.html#cb35-7" aria-hidden="true" tabindex="-1"></a>class_0_prior <span class="ot">&lt;-</span> <span class="fu">length</span>(train_sample_df<span class="sc">$</span>x[train_sample_df<span class="sc">$</span>y <span class="sc">==</span> <span class="dv">0</span>]) <span class="sc">/</span></span>
<span id="cb35-8"><a href="univariate-lda.html#cb35-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">length</span>(train_sample_df<span class="sc">$</span>x)</span>
<span id="cb35-9"><a href="univariate-lda.html#cb35-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-10"><a href="univariate-lda.html#cb35-10" aria-hidden="true" tabindex="-1"></a>class_1_prior <span class="ot">&lt;-</span> <span class="fu">length</span>(train_sample_df<span class="sc">$</span>x[train_sample_df<span class="sc">$</span>y <span class="sc">==</span> <span class="dv">1</span>]) <span class="sc">/</span></span>
<span id="cb35-11"><a href="univariate-lda.html#cb35-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">length</span>(train_sample_df<span class="sc">$</span>x)</span></code></pre></div>
<p>Building our discriminant functions as specified in the previous section is simple. We’ll write these as functions.</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="univariate-lda.html#cb36-1" aria-hidden="true" tabindex="-1"></a>d0 <span class="ot">&lt;-</span> <span class="cf">function</span>(x){</span>
<span id="cb36-2"><a href="univariate-lda.html#cb36-2" aria-hidden="true" tabindex="-1"></a>  x <span class="sc">*</span> (class_0_mean <span class="sc">/</span> class_0_var) <span class="sc">-</span> (class_0_mean <span class="sc">^</span> <span class="dv">2</span> <span class="sc">/</span> (<span class="dv">2</span> <span class="sc">*</span> class_0_var <span class="sc">^</span> <span class="dv">2</span>)) <span class="sc">+</span></span>
<span id="cb36-3"><a href="univariate-lda.html#cb36-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">log</span>(class_0_prior)</span>
<span id="cb36-4"><a href="univariate-lda.html#cb36-4" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb36-5"><a href="univariate-lda.html#cb36-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-6"><a href="univariate-lda.html#cb36-6" aria-hidden="true" tabindex="-1"></a>d1 <span class="ot">&lt;-</span> <span class="cf">function</span>(x){</span>
<span id="cb36-7"><a href="univariate-lda.html#cb36-7" aria-hidden="true" tabindex="-1"></a>  x <span class="sc">*</span> (class_1_mean <span class="sc">/</span> class_1_var) <span class="sc">-</span> (class_1_mean <span class="sc">^</span> <span class="dv">2</span> <span class="sc">/</span> (<span class="dv">2</span> <span class="sc">*</span> class_1_var <span class="sc">^</span> <span class="dv">2</span>)) <span class="sc">+</span></span>
<span id="cb36-8"><a href="univariate-lda.html#cb36-8" aria-hidden="true" tabindex="-1"></a>    <span class="fu">log</span>(class_1_prior)</span>
<span id="cb36-9"><a href="univariate-lda.html#cb36-9" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>Then we can build our simple decision rule function, assigning an observation to class <span class="math inline">\(k\)</span> if <span class="math inline">\(\delta_k(x) &gt; \delta_l(x)\)</span> and vice versa:</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="univariate-lda.html#cb37-1" aria-hidden="true" tabindex="-1"></a>LDA <span class="ot">&lt;-</span> <span class="cf">function</span>(x){</span>
<span id="cb37-2"><a href="univariate-lda.html#cb37-2" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb37-3"><a href="univariate-lda.html#cb37-3" aria-hidden="true" tabindex="-1"></a>  score_0 <span class="ot">&lt;-</span> <span class="fu">d0</span>(x)</span>
<span id="cb37-4"><a href="univariate-lda.html#cb37-4" aria-hidden="true" tabindex="-1"></a>  score_1 <span class="ot">&lt;-</span> <span class="fu">d1</span>(x)</span>
<span id="cb37-5"><a href="univariate-lda.html#cb37-5" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb37-6"><a href="univariate-lda.html#cb37-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ifelse</span>(score_0 <span class="sc">&gt;</span> score_1, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb37-7"><a href="univariate-lda.html#cb37-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-8"><a href="univariate-lda.html#cb37-8" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>Then we can <a href="https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/apply" target="_blank"><code>apply</code></a> our LDA function over our training data to classify the observations:</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="univariate-lda.html#cb38-1" aria-hidden="true" tabindex="-1"></a>train_sample_df<span class="sc">$</span>predicted_y <span class="ot">&lt;-</span> <span class="fu">apply</span>(train_sample_df[,<span class="dv">2</span>], <span class="at">MARGIN =</span> <span class="dv">1</span>, LDA)</span></code></pre></div>
<p>Because our discriminant functions are linear in <span class="math inline">\(x\)</span>, we can also solve for the value of <span class="math inline">\(x\)</span> that acts as our LDA boundary:</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb39-1"><a href="univariate-lda.html#cb39-1" aria-hidden="true" tabindex="-1"></a>LDA_decision <span class="ot">&lt;-</span> (class_0_mean <span class="sc">^</span> <span class="dv">2</span> <span class="sc">/</span> (<span class="dv">2</span> <span class="sc">*</span> class_0_var) <span class="sc">-</span></span>
<span id="cb39-2"><a href="univariate-lda.html#cb39-2" aria-hidden="true" tabindex="-1"></a>                   class_1_mean <span class="sc">^</span> <span class="dv">2</span> <span class="sc">/</span> (<span class="dv">2</span> <span class="sc">*</span> class_0_var) <span class="sc">+</span></span>
<span id="cb39-3"><a href="univariate-lda.html#cb39-3" aria-hidden="true" tabindex="-1"></a>                   <span class="fu">log</span>(class_1_prior) <span class="sc">-</span> <span class="fu">log</span>(class_0_prior)) <span class="sc">/</span></span>
<span id="cb39-4"><a href="univariate-lda.html#cb39-4" aria-hidden="true" tabindex="-1"></a>                (class_0_mean <span class="sc">/</span> class_0_var <span class="sc">-</span> class_1_mean <span class="sc">/</span> class_1_var)</span></code></pre></div>
<p>And we can easily visualize both our (population-based) Bayes decision boundary and our (sample-based) LDA decision boundary:</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="univariate-lda.html#cb40-1" aria-hidden="true" tabindex="-1"></a>train_sample_histogram <span class="sc">+</span> </span>
<span id="cb40-2"><a href="univariate-lda.html#cb40-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> bayes_decision,</span>
<span id="cb40-3"><a href="univariate-lda.html#cb40-3" aria-hidden="true" tabindex="-1"></a>                                   <span class="at">color =</span> <span class="st">&quot;black&quot;</span>) <span class="sc">+</span></span>
<span id="cb40-4"><a href="univariate-lda.html#cb40-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> LDA_decision,</span>
<span id="cb40-5"><a href="univariate-lda.html#cb40-5" aria-hidden="true" tabindex="-1"></a>             <span class="at">color =</span> <span class="st">&quot;black&quot;</span>,</span>
<span id="cb40-6"><a href="univariate-lda.html#cb40-6" aria-hidden="true" tabindex="-1"></a>             <span class="at">linetype =</span> <span class="st">&quot;longdash&quot;</span>) <span class="sc">+</span></span>
<span id="cb40-7"><a href="univariate-lda.html#cb40-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">subtitle =</span> <span class="st">&quot;Solid line: Bayes (optimal) decision boundary</span><span class="sc">\n</span><span class="st">Dashed line: LDA decision boundary&quot;</span>)</span></code></pre></div>
<p><img src="LDA_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
</div>
</div>
<div id="testing-1" class="section level3">
<h3>Testing</h3>
<p>Using the decision boundary from our LDA model applied to the training data, we can simply and easily classify the test data observations:</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb41-1"><a href="univariate-lda.html#cb41-1" aria-hidden="true" tabindex="-1"></a>test_sample_df<span class="sc">$</span>predicted_y <span class="ot">&lt;-</span> <span class="fu">if_else</span>(test_sample_df<span class="sc">$</span>x <span class="sc">&lt;</span> LDA_decision,</span>
<span id="cb41-2"><a href="univariate-lda.html#cb41-2" aria-hidden="true" tabindex="-1"></a>                                             <span class="dv">0</span>, <span class="dv">1</span>)</span></code></pre></div>
<p>What’s LDA’s misclassification rate for the test data in this case?</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="univariate-lda.html#cb42-1" aria-hidden="true" tabindex="-1"></a>LDA_misclass_rate <span class="ot">&lt;-</span> <span class="fu">nrow</span>(test_sample_df[test_sample_df<span class="sc">$</span>y <span class="sc">!=</span> </span>
<span id="cb42-2"><a href="univariate-lda.html#cb42-2" aria-hidden="true" tabindex="-1"></a>                                           test_sample_df<span class="sc">$</span>predicted_y,]) <span class="sc">/</span></span>
<span id="cb42-3"><a href="univariate-lda.html#cb42-3" aria-hidden="true" tabindex="-1"></a>                     <span class="fu">nrow</span>(test_sample_df)</span>
<span id="cb42-4"><a href="univariate-lda.html#cb42-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-5"><a href="univariate-lda.html#cb42-5" aria-hidden="true" tabindex="-1"></a>LDA_misclass_rate <span class="sc">*</span> <span class="dv">100</span></span></code></pre></div>
<pre><code>## [1] 0.35</code></pre>
<p>It’s certainly not perfect! But how imperfect is it? Since we’re using Bayes’ classifier on the population data as our standard, we can apply its decision rule to the test data and calculate its misclassification rate.</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="univariate-lda.html#cb44-1" aria-hidden="true" tabindex="-1"></a>test_sample_df<span class="sc">$</span>bayes_predicted_y <span class="ot">&lt;-</span> <span class="fu">if_else</span>(test_sample_df<span class="sc">$</span>x <span class="sc">&lt;</span> bayes_decision,</span>
<span id="cb44-2"><a href="univariate-lda.html#cb44-2" aria-hidden="true" tabindex="-1"></a>                                       <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb44-3"><a href="univariate-lda.html#cb44-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-4"><a href="univariate-lda.html#cb44-4" aria-hidden="true" tabindex="-1"></a>bayes_misclass_rate <span class="ot">&lt;-</span> <span class="fu">nrow</span>(test_sample_df[test_sample_df<span class="sc">$</span>y <span class="sc">!=</span> </span>
<span id="cb44-5"><a href="univariate-lda.html#cb44-5" aria-hidden="true" tabindex="-1"></a>                                             test_sample_df<span class="sc">$</span>bayes_predicted_y,]) <span class="sc">/</span></span>
<span id="cb44-6"><a href="univariate-lda.html#cb44-6" aria-hidden="true" tabindex="-1"></a>                       <span class="fu">nrow</span>(test_sample_df)</span>
<span id="cb44-7"><a href="univariate-lda.html#cb44-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-8"><a href="univariate-lda.html#cb44-8" aria-hidden="true" tabindex="-1"></a>bayes_misclass_rate <span class="sc">*</span> <span class="dv">100</span></span></code></pre></div>
<pre><code>## [1] 0.35</code></pre>
<p>Of course, it is possible for LDA to “outperform” the optimal Bayes classifier depending on the train and test samples, but the Bayes classifier here in a sense represents the “true” optimal classifier since it is coming from population data.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="linear-discriminant-analysis.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="multivariate-lda.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"],
"google": false,
"instapper": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
},
"df_print": "kable"
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
